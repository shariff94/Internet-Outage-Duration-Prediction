{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5iVWPp0okGP"
   },
   "source": [
    "# PHD Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myUHn7rNokGR"
   },
   "source": [
    "# `Broadband Outage Detection`\n",
    "\n",
    "### Description:\n",
    "\n",
    "India is seeing an explosion of new competitors in the Broadband space. 'India Broadband' is\n",
    "a company that is now seeing a lot of customer churn due to customer dissatisfaction\n",
    "because of broadband outages.\n",
    "\n",
    "The company has now curated a dataset, where it tracks several variables that it believes\n",
    "impact the `outage_duration`. They have tracked three different outage durations, `0` for\n",
    "no outage, `1` for short outages that last anywhere between a few minutes and a maximum\n",
    "of 2 hours, and `2` for long outages that can last from 2 hours to sometimes even a couple\n",
    "of days.\n",
    "\n",
    "You will now have to use these metrics that the company has tracked to create a machine\n",
    "learning model that will be able to predict the `outage_duration` so that the company can\n",
    "better handle outages and improve customer satisfaction and therefore reduce customer\n",
    "churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7JldeCMokGS"
   },
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEVHKHFZokGT"
   },
   "outputs": [],
   "source": [
    "import numpy as np                   # array/mathematical package\n",
    "import pandas as pd                  # data manipulation/analysis package\n",
    "import matplotlib.pyplot as plt      # library for vizualisations in matlab style\n",
    "import seaborn as sns                # matplot based package for stylised plots\n",
    "sns.set()                            # set seaborn as default aesthetics in plots\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCHNlDs_okGc"
   },
   "source": [
    "## 2 Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPHFLjBSokGd"
   },
   "outputs": [],
   "source": [
    "# broadband data\n",
    "broadband_df = pd.read_csv('data/broadband_data.csv')\n",
    "\n",
    "# outage data\n",
    "outage_df = pd.read_csv('data/outage_data.csv')\n",
    "\n",
    "# report data\n",
    "report_df = pd.read_csv('data/report_data.csv')\n",
    "\n",
    "# server data\n",
    "server_df = pd.read_csv('data/server_data.csv')\n",
    "\n",
    "# train data\n",
    "train_df = pd.read_csv('data/train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJHeRXrkokGh"
   },
   "source": [
    "## 3. Exploring the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acqPeSDiokGh"
   },
   "source": [
    "#### 3.1 Broadband data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "OM8y7hJ6okGi",
    "outputId": "59d343cc-6675-48f4-8dc0-e157416d2de1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>broadband_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8011</td>\n",
       "      <td>broadband_type_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>broadband_type_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6852</td>\n",
       "      <td>broadband_type_8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    broadband_type\n",
       "0  6597  broadband_type_8\n",
       "1  8011  broadband_type_8\n",
       "2  2597  broadband_type_8\n",
       "3  5022  broadband_type_8\n",
       "4  6852  broadband_type_8"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z6x11ibokGp"
   },
   "outputs": [],
   "source": [
    "# creating a dictionary for broadband types with key as the type and name as the value\n",
    "broadband_type_dict = {  'broadband_type_8'  : 'ADSL 1',\n",
    "                         'broadband_type_2'  : 'ADSL 2',\n",
    "                         'broadband_type_6'  : 'ADSL 2+',\n",
    "                         'broadband_type_7'  : 'Cable',\n",
    "                         'broadband_type_4'  : 'Fiber 1',\n",
    "                         'broadband_type_9'  : 'BPL',\n",
    "                         'broadband_type_3'  : 'Fiber 2',\n",
    "                         'broadband_type_10' : 'Fiber High Speed',\n",
    "                         'broadband_type_1'  : 'Fiber Ultra',\n",
    "                         'broadband_type_5'  : 'Fiber Ultra Max'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOmtVVGgokGu"
   },
   "outputs": [],
   "source": [
    "# mapping the values of the dictionary broadband_type_dict based on the broadband_name as the key\n",
    "broadband_df['broadband_name']=broadband_df.broadband_type.map(broadband_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "exX03U_eokGy",
    "outputId": "1a5b5b9d-23d9-46b5-d67d-17c19a6faca1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>broadband_type</th>\n",
       "      <th>broadband_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8011</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6852</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    broadband_type broadband_name\n",
       "0  6597  broadband_type_8         ADSL 1\n",
       "1  8011  broadband_type_8         ADSL 1\n",
       "2  2597  broadband_type_8         ADSL 1\n",
       "3  5022  broadband_type_8         ADSL 1\n",
       "4  6852  broadband_type_8         ADSL 1"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "eGRfUHBookG2",
    "outputId": "d149a379-2d91-42fd-8da4-40a46cad63e8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>broadband_type</th>\n",
       "      <th>broadband_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21071</th>\n",
       "      <td>3761</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>8720</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21073</th>\n",
       "      <td>6488</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21074</th>\n",
       "      <td>878</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21075</th>\n",
       "      <td>4464</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    broadband_type broadband_name\n",
       "21071  3761  broadband_type_8         ADSL 1\n",
       "21072  8720  broadband_type_8         ADSL 1\n",
       "21073  6488  broadband_type_8         ADSL 1\n",
       "21074   878  broadband_type_8         ADSL 1\n",
       "21075  4464  broadband_type_8         ADSL 1"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "og9sNBOIokG7",
    "outputId": "8fecc824-f10c-4aa9-904b-bf88816fe6a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21076, 3)"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data post mapping with the dictionary\n",
    "broadband_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "DtNl-QgLokG_",
    "outputId": "277578ab-503d-4b52-caaf-2cce24f9a538",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>broadband_type</th>\n",
       "      <th>broadband_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21076.000000</td>\n",
       "      <td>21076</td>\n",
       "      <td>21076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10268</td>\n",
       "      <td>10268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9255.869330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5366.730222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4599.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9256.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13907.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18552.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id    broadband_type broadband_name\n",
       "count   21076.000000             21076          21076\n",
       "unique           NaN                10             10\n",
       "top              NaN  broadband_type_8         ADSL 1\n",
       "freq             NaN             10268          10268\n",
       "mean     9255.869330               NaN            NaN\n",
       "std      5366.730222               NaN            NaN\n",
       "min         1.000000               NaN            NaN\n",
       "25%      4599.750000               NaN            NaN\n",
       "50%      9256.500000               NaN            NaN\n",
       "75%     13907.250000               NaN            NaN\n",
       "max     18552.000000               NaN            NaN"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description of the data\n",
    "broadband_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "lxqKiu2aokHD",
    "outputId": "bcee5c0b-5ec0-430f-f60b-d63495e01c69",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21076 entries, 0 to 21075\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              21076 non-null  int64 \n",
      " 1   broadband_type  21076 non-null  object\n",
      " 2   broadband_name  21076 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 494.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# summary of the dataframe\n",
    "broadband_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bg7xD2LjokHJ"
   },
   "source": [
    ">Seems like id is being treated as int, i.e. as numerical but as we know it is a category and so are the other two columns. Hence changing the datatype in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RWE4pJmqokHL",
    "outputId": "5a903084-1498-4bcf-eb0d-183f32ea5fa8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>broadband_type</th>\n",
       "      <th>broadband_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8011</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2597</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6852</td>\n",
       "      <td>broadband_type_8</td>\n",
       "      <td>ADSL 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    broadband_type broadband_name\n",
       "0  6597  broadband_type_8         ADSL 1\n",
       "1  8011  broadband_type_8         ADSL 1\n",
       "2  2597  broadband_type_8         ADSL 1\n",
       "3  5022  broadband_type_8         ADSL 1\n",
       "4  6852  broadband_type_8         ADSL 1"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df = broadband_df.astype('category')\n",
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2wA8KsrookHQ",
    "outputId": "7fd3a362-4007-4cb7-d8cd-42bc557d4b2a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21076 entries, 0 to 21075\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   id              21076 non-null  category\n",
      " 1   broadband_type  21076 non-null  category\n",
      " 2   broadband_name  21076 non-null  category\n",
      "dtypes: category(3)\n",
      "memory usage: 868.2 KB\n"
     ]
    }
   ],
   "source": [
    "broadband_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "C6kMgRrFokHV",
    "outputId": "58076d52-9976-4fac-876b-dd989f9197cd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "broadband_df.broadband_name.value_counts().plot(kind='bar')\n",
    "plt.xlabel('Broadband Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of the Broadband Types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXQM-1XhokHX"
   },
   "source": [
    "> On exploring the data set we find that the broadband company has recorded most of the complaints where the broadband type was Either ADSL 1 or ADSL 2 where the Fiber type of broadbands have registered the least complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZD_OjnRGokHY"
   },
   "source": [
    "`Wrangling of data for model building`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "DAIdZFrbokHZ",
    "outputId": "6e7614af-3f0d-402a-a4b2-8be16233e9a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>broadband_name</th>\n",
       "      <th>ADSL 1</th>\n",
       "      <th>ADSL 2</th>\n",
       "      <th>ADSL 2+</th>\n",
       "      <th>BPL</th>\n",
       "      <th>Cable</th>\n",
       "      <th>Fiber 1</th>\n",
       "      <th>Fiber 2</th>\n",
       "      <th>Fiber High Speed</th>\n",
       "      <th>Fiber Ultra</th>\n",
       "      <th>Fiber Ultra Max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "broadband_name  ADSL 1  ADSL 2  ...  Fiber Ultra  Fiber Ultra Max\n",
       "id                              ...                              \n",
       "1                    1       0  ...            0                0\n",
       "2                    0       1  ...            0                0\n",
       "3                    1       0  ...            0                0\n",
       "4                    0       1  ...            0                0\n",
       "5                    0       1  ...            0                0\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming the categorical columns to numerical as needed for the model building\n",
    "broadband_df = broadband_df[['id','broadband_name']].pivot_table(values='broadband_name',index='id',columns='broadband_name',fill_value=0,aggfunc=len)\n",
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "r2_prBOZokHb",
    "outputId": "ea3a838f-2d05-47ed-b7f1-b9a29838085f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADSL 1</th>\n",
       "      <th>ADSL 2</th>\n",
       "      <th>ADSL 2+</th>\n",
       "      <th>BPL</th>\n",
       "      <th>Cable</th>\n",
       "      <th>Fiber 1</th>\n",
       "      <th>Fiber 2</th>\n",
       "      <th>Fiber High Speed</th>\n",
       "      <th>Fiber Ultra</th>\n",
       "      <th>Fiber Ultra Max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADSL 1  ADSL 2  ADSL 2+  ...  Fiber High Speed  Fiber Ultra  Fiber Ultra Max\n",
       "id                           ...                                                \n",
       "1        1       0        1  ...                 0            0                0\n",
       "2        0       1        0  ...                 0            0                0\n",
       "3        1       0        0  ...                 0            0                0\n",
       "4        0       1        0  ...                 0            0                0\n",
       "5        0       1        0  ...                 0            0                0\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.columns = broadband_df.columns.categories\n",
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sllyw5CeokHf",
    "outputId": "950c17ab-fa4c-4cf0-e89e-620c7aa23797",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ADSL 1</th>\n",
       "      <th>ADSL 2</th>\n",
       "      <th>ADSL 2+</th>\n",
       "      <th>BPL</th>\n",
       "      <th>Cable</th>\n",
       "      <th>Fiber 1</th>\n",
       "      <th>Fiber 2</th>\n",
       "      <th>Fiber High Speed</th>\n",
       "      <th>Fiber Ultra</th>\n",
       "      <th>Fiber Ultra Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  ADSL 1  ADSL 2  ...  Fiber High Speed  Fiber Ultra  Fiber Ultra Max\n",
       "0  1       1       0  ...                 0            0                0\n",
       "1  2       0       1  ...                 0            0                0\n",
       "2  3       1       0  ...                 0            0                0\n",
       "3  4       0       1  ...                 0            0                0\n",
       "4  5       0       1  ...                 0            0                0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.reset_index(inplace=True)\n",
    "broadband_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h8Dn2WmnokHj",
    "outputId": "63d840e9-5b23-485f-b123-74696fa0bdc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for nulls in the data\n",
    "broadband_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "azMuj6iPokHm",
    "outputId": "a0305d75-9d2b-4c23-e3e5-2dfdd98b2a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18552, 11)"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3SwtuDiokHo"
   },
   "source": [
    "#### 3.2 Outage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "zz2J1y4ZokHq",
    "outputId": "e8e1e3f6-436e-4392-c9c8-6a876174e047",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>outage_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6597</td>\n",
       "      <td>outage_type_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8011</td>\n",
       "      <td>outage_type_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2597</td>\n",
       "      <td>outage_type_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>outage_type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6852</td>\n",
       "      <td>outage_type_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    outage_type\n",
       "0  6597  outage_type_2\n",
       "1  8011  outage_type_2\n",
       "2  2597  outage_type_2\n",
       "3  5022  outage_type_1\n",
       "4  6852  outage_type_1"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0CuNPCHAokHv",
    "outputId": "5328d69e-e4d9-4c46-b9f1-ed3fcfe8edb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18552, 2)"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data\n",
    "outage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "c_gd4ZlwokHz",
    "outputId": "47f77d16-637a-4580-c638-77b0456a0cb8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18552 entries, 0 to 18551\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           18552 non-null  int64 \n",
      " 1   outage_type  18552 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 290.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# summary of the dataset\n",
    "outage_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKkkwK5IokH3"
   },
   "source": [
    ">Seems like id is being treated as int, i.e. as numerical but as we know it is a category and so are the other two columns. Hence changing the datatype in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9gPFSD3okH4"
   },
   "outputs": [],
   "source": [
    "# changing the data type of the columns\n",
    "outage_df = outage_df.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "HajBhDrSokIA",
    "outputId": "fa32dfcf-d35f-4539-848b-e4475d297057",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>outage_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18552</td>\n",
       "      <td>18552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18552</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>18552</td>\n",
       "      <td>outage_type_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>8737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    outage_type\n",
       "count   18552          18552\n",
       "unique  18552              5\n",
       "top     18552  outage_type_2\n",
       "freq        1           8737"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description of the data\n",
    "outage_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNFDIkR9okID"
   },
   "source": [
    "Above cell indicates\n",
    "1. All the ids are unique\n",
    "2. Most frequent outage type is `outage_type_2` which has occurred `8737` times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "RX-aQKbiokIE",
    "outputId": "b1c78f61-8d07-4993-e191-2c453ebe7a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outage_type_2    8737\n",
       "outage_type_1    8728\n",
       "outage_type_4    1014\n",
       "outage_type_5      65\n",
       "outage_type_3       8\n",
       "Name: outage_type, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The frequency of each outage type\n",
    "outage_df.outage_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "m90suF2LokII",
    "outputId": "9ab58289-f30e-45ea-dbe1-da5773920202",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the frequency from the above cell\n",
    "plt.figure(figsize=(7,4))\n",
    "outage_df.outage_type.value_counts(ascending=True).plot(kind='barh')\n",
    "plt.ylabel('Outage Type')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Distribution of the Outage Types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBrPii6qokIM"
   },
   "source": [
    "> `outage_type_1` and `outage_type_2` seems to be most frequent whereas `outage_type_5` and `outage_type_3` are the least"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBdCGIMuokIN"
   },
   "source": [
    "`Wrangling the data for model building`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "GCj9dcoXokIN",
    "outputId": "ff483b69-2538-4fae-8882-cbcba1b261f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>outage_type</th>\n",
       "      <th>outage_type_1</th>\n",
       "      <th>outage_type_2</th>\n",
       "      <th>outage_type_3</th>\n",
       "      <th>outage_type_4</th>\n",
       "      <th>outage_type_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "outage_type  outage_type_1  outage_type_2  ...  outage_type_4  outage_type_5\n",
       "id                                         ...                              \n",
       "1                        1              0  ...              0              0\n",
       "2                        0              1  ...              0              0\n",
       "3                        1              0  ...              0              0\n",
       "4                        0              0  ...              1              0\n",
       "5                        0              1  ...              0              0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_df = outage_df.pivot_table(values='outage_type',index='id',columns='outage_type',fill_value=0,aggfunc=len)\n",
    "outage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "Ee_DNwIfokIQ",
    "outputId": "2b4a219f-b62c-49de-eae2-7c838f830cc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outage_type_1</th>\n",
       "      <th>outage_type_2</th>\n",
       "      <th>outage_type_3</th>\n",
       "      <th>outage_type_4</th>\n",
       "      <th>outage_type_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outage_type_1  outage_type_2  outage_type_3  outage_type_4  outage_type_5\n",
       "id                                                                           \n",
       "1               1              0              0              0              0\n",
       "2               0              1              0              0              0\n",
       "3               1              0              0              0              0\n",
       "4               0              0              0              1              0\n",
       "5               0              1              0              0              0"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_df.columns = outage_df.columns.categories\n",
    "outage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TukOGnHMokIW",
    "outputId": "0fc2d2f3-26b7-456b-ad52-8236eb38648c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>outage_type_1</th>\n",
       "      <th>outage_type_2</th>\n",
       "      <th>outage_type_3</th>\n",
       "      <th>outage_type_4</th>\n",
       "      <th>outage_type_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  outage_type_1  outage_type_2  outage_type_3  outage_type_4  outage_type_5\n",
       "0  1              1              0              0              0              0\n",
       "1  2              0              1              0              0              0\n",
       "2  3              1              0              0              0              0\n",
       "3  4              0              0              0              1              0\n",
       "4  5              0              1              0              0              0"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_df.reset_index(inplace=True)\n",
    "outage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yBeT2HQOokIZ",
    "outputId": "082958a4-d444-4f8f-8159-916dc03b4373",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any nulls\n",
    "outage_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tWnQRM9AokId",
    "outputId": "fd49e88f-6751-4182-f1cc-be1db70ca654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18552, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final shape after processing\n",
    "outage_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ykt3fGfokIf"
   },
   "source": [
    "#### 3.3 Report data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FfzIm-4cokIf",
    "outputId": "24abcbd5-63d1-4f52-8316-b92305984446"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58671, 3)"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial shape of the data\n",
    "report_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Zv-mypzGokIh",
    "outputId": "ab42a854-7a53-4e5d-8096-f89e4c618cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58671 entries, 0 to 58670\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               58671 non-null  int64 \n",
      " 1   log_report_type  58671 non-null  object\n",
      " 2   volume           58671 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Summary of the data\n",
    "report_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agqq_f-WokIj"
   },
   "source": [
    "> `id` and `log_report_type` are of type category but it is currently of type int64 and object respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcltFNt0okIk"
   },
   "outputs": [],
   "source": [
    "# Changing the data type of the columns mention above\n",
    "report_df[['id','log_report_type']] = report_df[['id','log_report_type']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "SdrSad-sokIm",
    "outputId": "f5976690-b23e-4354-f4cd-36f0f2233073",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log_report_type</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58671.0</td>\n",
       "      <td>58671</td>\n",
       "      <td>58671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>386</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>15184.0</td>\n",
       "      <td>log_report_type_312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>20.0</td>\n",
       "      <td>5267</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.685296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.314433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1310.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      log_report_type        volume\n",
       "count   58671.0                58671  58671.000000\n",
       "unique  18552.0                  386           NaN\n",
       "top     15184.0  log_report_type_312           NaN\n",
       "freq       20.0                 5267           NaN\n",
       "mean        NaN                  NaN      9.685296\n",
       "std         NaN                  NaN     27.314433\n",
       "min         NaN                  NaN      1.000000\n",
       "25%         NaN                  NaN      1.000000\n",
       "50%         NaN                  NaN      2.000000\n",
       "75%         NaN                  NaN      7.000000\n",
       "max         NaN                  NaN   1310.000000"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2ggQhr9okIp"
   },
   "source": [
    "From the above description we can infer that\n",
    "1. `id` is not unique, which means an `id` has occured multiple times and with possibly different log_report_type\n",
    "3. Total report types are 386\n",
    "2. Most frequent `log_report_type` is `log_report_type_312` which has occured `5267` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAH_b8DZokIq"
   },
   "source": [
    "**Since there are a total of 386 unique reports it will be difficult to visualize all 386 in a single frame, hence lets see what are the top 20 most frequent log_report_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "xhvgZMt4okIq",
    "outputId": "b7cd91c1-7e92-4af0-e588-fb62efa7f6bb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "report_df.groupby('log_report_type')['volume'].sum().nlargest(20).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPdboPAaokIt"
   },
   "source": [
    "**Now lets see what are the ids that registered most complaints, by visualizing the 50 most frequent ids and their frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "Wnznaar_okIt",
    "outputId": "945f70f5-5eb6-4ac3-f7c1-88174c366dcc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "report_df.groupby('id')['volume'].sum().nlargest(50).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zDmArlSokIv"
   },
   "source": [
    "`Wrangling data for model building`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YIXthl9IokIw",
    "outputId": "af2d58c9-0307-41df-ee0c-352aa924e3f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  volume\n",
       "0  1       5\n",
       "1  2       5\n",
       "2  3       2\n",
       "3  4       3\n",
       "4  5      17"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_volume_df = pd.DataFrame(report_df.groupby('id')['volume'].sum()).reset_index()\n",
    "report_volume_df.rename(columns={'volume':'report_volume'})\n",
    "report_volume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "LyhWD4jbokIz",
    "outputId": "588d7894-f328-4b8b-b6c4-50982badcee9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>18548</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18548</th>\n",
       "      <td>18549</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18549</th>\n",
       "      <td>18550</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18550</th>\n",
       "      <td>18551</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18551</th>\n",
       "      <td>18552</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18552 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  volume\n",
       "0          1       5\n",
       "1          2       5\n",
       "2          3       2\n",
       "3          4       3\n",
       "4          5      17\n",
       "...      ...     ...\n",
       "18547  18548      22\n",
       "18548  18549       8\n",
       "18549  18550       6\n",
       "18550  18551       7\n",
       "18551  18552      48\n",
       "\n",
       "[18552 rows x 2 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_volume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "FMClMNOtokI3",
    "outputId": "4bd9fd07-9aaf-4670-d7dd-5379e1df225f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>log_report_type</th>\n",
       "      <th>log_report_type_1</th>\n",
       "      <th>log_report_type_10</th>\n",
       "      <th>log_report_type_100</th>\n",
       "      <th>log_report_type_101</th>\n",
       "      <th>log_report_type_102</th>\n",
       "      <th>log_report_type_103</th>\n",
       "      <th>log_report_type_104</th>\n",
       "      <th>log_report_type_105</th>\n",
       "      <th>log_report_type_106</th>\n",
       "      <th>log_report_type_107</th>\n",
       "      <th>log_report_type_108</th>\n",
       "      <th>log_report_type_109</th>\n",
       "      <th>log_report_type_11</th>\n",
       "      <th>log_report_type_110</th>\n",
       "      <th>log_report_type_111</th>\n",
       "      <th>log_report_type_112</th>\n",
       "      <th>log_report_type_113</th>\n",
       "      <th>log_report_type_114</th>\n",
       "      <th>log_report_type_115</th>\n",
       "      <th>log_report_type_116</th>\n",
       "      <th>log_report_type_117</th>\n",
       "      <th>log_report_type_118</th>\n",
       "      <th>log_report_type_119</th>\n",
       "      <th>log_report_type_12</th>\n",
       "      <th>log_report_type_120</th>\n",
       "      <th>log_report_type_121</th>\n",
       "      <th>log_report_type_122</th>\n",
       "      <th>log_report_type_123</th>\n",
       "      <th>log_report_type_124</th>\n",
       "      <th>log_report_type_125</th>\n",
       "      <th>log_report_type_126</th>\n",
       "      <th>log_report_type_127</th>\n",
       "      <th>log_report_type_128</th>\n",
       "      <th>log_report_type_129</th>\n",
       "      <th>log_report_type_13</th>\n",
       "      <th>log_report_type_130</th>\n",
       "      <th>log_report_type_131</th>\n",
       "      <th>log_report_type_132</th>\n",
       "      <th>log_report_type_133</th>\n",
       "      <th>log_report_type_134</th>\n",
       "      <th>...</th>\n",
       "      <th>log_report_type_63</th>\n",
       "      <th>log_report_type_64</th>\n",
       "      <th>log_report_type_65</th>\n",
       "      <th>log_report_type_66</th>\n",
       "      <th>log_report_type_67</th>\n",
       "      <th>log_report_type_68</th>\n",
       "      <th>log_report_type_69</th>\n",
       "      <th>log_report_type_7</th>\n",
       "      <th>log_report_type_70</th>\n",
       "      <th>log_report_type_71</th>\n",
       "      <th>log_report_type_72</th>\n",
       "      <th>log_report_type_73</th>\n",
       "      <th>log_report_type_74</th>\n",
       "      <th>log_report_type_75</th>\n",
       "      <th>log_report_type_76</th>\n",
       "      <th>log_report_type_77</th>\n",
       "      <th>log_report_type_78</th>\n",
       "      <th>log_report_type_79</th>\n",
       "      <th>log_report_type_8</th>\n",
       "      <th>log_report_type_80</th>\n",
       "      <th>log_report_type_81</th>\n",
       "      <th>log_report_type_82</th>\n",
       "      <th>log_report_type_83</th>\n",
       "      <th>log_report_type_84</th>\n",
       "      <th>log_report_type_85</th>\n",
       "      <th>log_report_type_86</th>\n",
       "      <th>log_report_type_87</th>\n",
       "      <th>log_report_type_88</th>\n",
       "      <th>log_report_type_89</th>\n",
       "      <th>log_report_type_9</th>\n",
       "      <th>log_report_type_90</th>\n",
       "      <th>log_report_type_91</th>\n",
       "      <th>log_report_type_92</th>\n",
       "      <th>log_report_type_93</th>\n",
       "      <th>log_report_type_94</th>\n",
       "      <th>log_report_type_95</th>\n",
       "      <th>log_report_type_96</th>\n",
       "      <th>log_report_type_97</th>\n",
       "      <th>log_report_type_98</th>\n",
       "      <th>log_report_type_99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "log_report_type  log_report_type_1  ...  log_report_type_99\n",
       "id                                  ...                    \n",
       "1                                0  ...                   0\n",
       "2                                0  ...                   0\n",
       "3                                0  ...                   0\n",
       "4                                0  ...                   0\n",
       "5                                0  ...                   0\n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = report_df.pivot_table(values='volume',index='id',columns='log_report_type',fill_value=0,aggfunc=np.sum)\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "cZE2vIZ3okI6",
    "outputId": "bc02547b-d1ca-4d13-ab4e-bcbae6a541c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_report_type_1</th>\n",
       "      <th>log_report_type_10</th>\n",
       "      <th>log_report_type_100</th>\n",
       "      <th>log_report_type_101</th>\n",
       "      <th>log_report_type_102</th>\n",
       "      <th>log_report_type_103</th>\n",
       "      <th>log_report_type_104</th>\n",
       "      <th>log_report_type_105</th>\n",
       "      <th>log_report_type_106</th>\n",
       "      <th>log_report_type_107</th>\n",
       "      <th>log_report_type_108</th>\n",
       "      <th>log_report_type_109</th>\n",
       "      <th>log_report_type_11</th>\n",
       "      <th>log_report_type_110</th>\n",
       "      <th>log_report_type_111</th>\n",
       "      <th>log_report_type_112</th>\n",
       "      <th>log_report_type_113</th>\n",
       "      <th>log_report_type_114</th>\n",
       "      <th>log_report_type_115</th>\n",
       "      <th>log_report_type_116</th>\n",
       "      <th>log_report_type_117</th>\n",
       "      <th>log_report_type_118</th>\n",
       "      <th>log_report_type_119</th>\n",
       "      <th>log_report_type_12</th>\n",
       "      <th>log_report_type_120</th>\n",
       "      <th>log_report_type_121</th>\n",
       "      <th>log_report_type_122</th>\n",
       "      <th>log_report_type_123</th>\n",
       "      <th>log_report_type_124</th>\n",
       "      <th>log_report_type_125</th>\n",
       "      <th>log_report_type_126</th>\n",
       "      <th>log_report_type_127</th>\n",
       "      <th>log_report_type_128</th>\n",
       "      <th>log_report_type_129</th>\n",
       "      <th>log_report_type_13</th>\n",
       "      <th>log_report_type_130</th>\n",
       "      <th>log_report_type_131</th>\n",
       "      <th>log_report_type_132</th>\n",
       "      <th>log_report_type_133</th>\n",
       "      <th>log_report_type_134</th>\n",
       "      <th>...</th>\n",
       "      <th>log_report_type_63</th>\n",
       "      <th>log_report_type_64</th>\n",
       "      <th>log_report_type_65</th>\n",
       "      <th>log_report_type_66</th>\n",
       "      <th>log_report_type_67</th>\n",
       "      <th>log_report_type_68</th>\n",
       "      <th>log_report_type_69</th>\n",
       "      <th>log_report_type_7</th>\n",
       "      <th>log_report_type_70</th>\n",
       "      <th>log_report_type_71</th>\n",
       "      <th>log_report_type_72</th>\n",
       "      <th>log_report_type_73</th>\n",
       "      <th>log_report_type_74</th>\n",
       "      <th>log_report_type_75</th>\n",
       "      <th>log_report_type_76</th>\n",
       "      <th>log_report_type_77</th>\n",
       "      <th>log_report_type_78</th>\n",
       "      <th>log_report_type_79</th>\n",
       "      <th>log_report_type_8</th>\n",
       "      <th>log_report_type_80</th>\n",
       "      <th>log_report_type_81</th>\n",
       "      <th>log_report_type_82</th>\n",
       "      <th>log_report_type_83</th>\n",
       "      <th>log_report_type_84</th>\n",
       "      <th>log_report_type_85</th>\n",
       "      <th>log_report_type_86</th>\n",
       "      <th>log_report_type_87</th>\n",
       "      <th>log_report_type_88</th>\n",
       "      <th>log_report_type_89</th>\n",
       "      <th>log_report_type_9</th>\n",
       "      <th>log_report_type_90</th>\n",
       "      <th>log_report_type_91</th>\n",
       "      <th>log_report_type_92</th>\n",
       "      <th>log_report_type_93</th>\n",
       "      <th>log_report_type_94</th>\n",
       "      <th>log_report_type_95</th>\n",
       "      <th>log_report_type_96</th>\n",
       "      <th>log_report_type_97</th>\n",
       "      <th>log_report_type_98</th>\n",
       "      <th>log_report_type_99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    log_report_type_1  ...  log_report_type_99\n",
       "id                     ...                    \n",
       "1                   0  ...                   0\n",
       "2                   0  ...                   0\n",
       "3                   0  ...                   0\n",
       "4                   0  ...                   0\n",
       "5                   0  ...                   0\n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df.columns = report_df.columns.categories\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "zbmVUO9cokJA",
    "outputId": "9e561920-483d-495f-ea3d-61afd239b093"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log_report_type_1</th>\n",
       "      <th>log_report_type_10</th>\n",
       "      <th>log_report_type_100</th>\n",
       "      <th>log_report_type_101</th>\n",
       "      <th>log_report_type_102</th>\n",
       "      <th>log_report_type_103</th>\n",
       "      <th>log_report_type_104</th>\n",
       "      <th>log_report_type_105</th>\n",
       "      <th>log_report_type_106</th>\n",
       "      <th>log_report_type_107</th>\n",
       "      <th>log_report_type_108</th>\n",
       "      <th>log_report_type_109</th>\n",
       "      <th>log_report_type_11</th>\n",
       "      <th>log_report_type_110</th>\n",
       "      <th>log_report_type_111</th>\n",
       "      <th>log_report_type_112</th>\n",
       "      <th>log_report_type_113</th>\n",
       "      <th>log_report_type_114</th>\n",
       "      <th>log_report_type_115</th>\n",
       "      <th>log_report_type_116</th>\n",
       "      <th>log_report_type_117</th>\n",
       "      <th>log_report_type_118</th>\n",
       "      <th>log_report_type_119</th>\n",
       "      <th>log_report_type_12</th>\n",
       "      <th>log_report_type_120</th>\n",
       "      <th>log_report_type_121</th>\n",
       "      <th>log_report_type_122</th>\n",
       "      <th>log_report_type_123</th>\n",
       "      <th>log_report_type_124</th>\n",
       "      <th>log_report_type_125</th>\n",
       "      <th>log_report_type_126</th>\n",
       "      <th>log_report_type_127</th>\n",
       "      <th>log_report_type_128</th>\n",
       "      <th>log_report_type_129</th>\n",
       "      <th>log_report_type_13</th>\n",
       "      <th>log_report_type_130</th>\n",
       "      <th>log_report_type_131</th>\n",
       "      <th>log_report_type_132</th>\n",
       "      <th>log_report_type_133</th>\n",
       "      <th>...</th>\n",
       "      <th>log_report_type_63</th>\n",
       "      <th>log_report_type_64</th>\n",
       "      <th>log_report_type_65</th>\n",
       "      <th>log_report_type_66</th>\n",
       "      <th>log_report_type_67</th>\n",
       "      <th>log_report_type_68</th>\n",
       "      <th>log_report_type_69</th>\n",
       "      <th>log_report_type_7</th>\n",
       "      <th>log_report_type_70</th>\n",
       "      <th>log_report_type_71</th>\n",
       "      <th>log_report_type_72</th>\n",
       "      <th>log_report_type_73</th>\n",
       "      <th>log_report_type_74</th>\n",
       "      <th>log_report_type_75</th>\n",
       "      <th>log_report_type_76</th>\n",
       "      <th>log_report_type_77</th>\n",
       "      <th>log_report_type_78</th>\n",
       "      <th>log_report_type_79</th>\n",
       "      <th>log_report_type_8</th>\n",
       "      <th>log_report_type_80</th>\n",
       "      <th>log_report_type_81</th>\n",
       "      <th>log_report_type_82</th>\n",
       "      <th>log_report_type_83</th>\n",
       "      <th>log_report_type_84</th>\n",
       "      <th>log_report_type_85</th>\n",
       "      <th>log_report_type_86</th>\n",
       "      <th>log_report_type_87</th>\n",
       "      <th>log_report_type_88</th>\n",
       "      <th>log_report_type_89</th>\n",
       "      <th>log_report_type_9</th>\n",
       "      <th>log_report_type_90</th>\n",
       "      <th>log_report_type_91</th>\n",
       "      <th>log_report_type_92</th>\n",
       "      <th>log_report_type_93</th>\n",
       "      <th>log_report_type_94</th>\n",
       "      <th>log_report_type_95</th>\n",
       "      <th>log_report_type_96</th>\n",
       "      <th>log_report_type_97</th>\n",
       "      <th>log_report_type_98</th>\n",
       "      <th>log_report_type_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  log_report_type_1  ...  log_report_type_98  log_report_type_99\n",
       "0  1                  0  ...                   0                   0\n",
       "1  2                  0  ...                   0                   0\n",
       "2  3                  0  ...                   0                   0\n",
       "3  4                  0  ...                   0                   0\n",
       "4  5                  0  ...                   0                   0\n",
       "\n",
       "[5 rows x 387 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df.reset_index(inplace=True)\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R9jlUkUTokJF",
    "outputId": "46525782-8120-4308-c592-45c8ae1292fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 140,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any nulls\n",
    "report_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QfixhZJzokJJ",
    "outputId": "997681b7-b73f-48cd-ab9b-f29f9628c27d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18552, 387)"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the processed dataset\n",
    "report_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLwrsbsDokJL"
   },
   "source": [
    "#### 3.4 Server Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "iGvDYyRTokJM",
    "outputId": "6d658c49-ffc5-427d-ab4e-31bfa3ea8202"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transit_server_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6597</td>\n",
       "      <td>transit_server_type_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8011</td>\n",
       "      <td>transit_server_type_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2597</td>\n",
       "      <td>transit_server_type_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>transit_server_type_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>transit_server_type_11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     transit_server_type\n",
       "0  6597  transit_server_type_11\n",
       "1  8011  transit_server_type_15\n",
       "2  2597  transit_server_type_15\n",
       "3  5022  transit_server_type_15\n",
       "4  5022  transit_server_type_11"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HT_UYysyokJP",
    "outputId": "5a56a41a-2a0b-4b84-f6ee-643e9d9b674f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31170, 2)"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data prior the processing\n",
    "server_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "f1HQsMr5okJR",
    "outputId": "ae6ed75a-c771-4cea-9e34-2385e7f46e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31170 entries, 0 to 31169\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   31170 non-null  int64 \n",
      " 1   transit_server_type  31170 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 487.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summary of the data\n",
    "server_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsmhygMaokJU"
   },
   "outputs": [],
   "source": [
    "# Changing the data to category type\n",
    "server_df = server_df.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "oTvwzyJYokJW",
    "outputId": "11d77b5c-01aa-40b0-a762-08b3191dbef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31170 entries, 0 to 31169\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   id                   31170 non-null  category\n",
      " 1   transit_server_type  31170 non-null  category\n",
      "dtypes: category(2)\n",
      "memory usage: 879.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Summary post data conversion\n",
    "server_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "IQzWFNN9okJb",
    "outputId": "a9da99de-cdea-45a0-a3a1-90fa362b5e86"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transit_server_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31170</td>\n",
       "      <td>31170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18552</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10354</td>\n",
       "      <td>transit_server_type_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11</td>\n",
       "      <td>7888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id     transit_server_type\n",
       "count   31170                   31170\n",
       "unique  18552                      53\n",
       "top     10354  transit_server_type_11\n",
       "freq       11                    7888"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data description\n",
    "server_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPb_Pb0EokJf"
   },
   "source": [
    "From the description we can infer the following\n",
    "1. There 18552 unique `id`s\n",
    "2. Most frequently occuring `id` is `10354` which is occuring 11 times\n",
    "3. There are 53 unique `transit_server_type`s\n",
    "4. Most frequent transit server type is `transit_server_type_11` which is occuring `7888` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TW60ZvN_okJg"
   },
   "source": [
    "**Let's visualize the 20 most frequent transit_server_type and their frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "Hsj7kVagokJg",
    "outputId": "aa6a0086-d742-4e6e-841e-683f5cbb9766"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAFvCAYAAACM4ZefAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df3zVdd3/8ec2YPx2DAcMpEAkWZKakPhNVMJLIR1spAYXIalZWUqUhhDqIDS7hmhZSGgmab+8JH5PhbjEKNFQKk3CX8HwAhk/3CA2fgzZ3t8/dLtAkO2Mnc95jvO4327cbnA+jPPg/Xnvw2uHs3NSQghBAAAAAOotNdEBAAAAQFPDEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIyaJTqgoXbu3KPq6oa9Ol/Hjm1VWlrRyEVNs8OhwaXDocGlw6HBpcOhwaXDocGlw6HBpcOhwaXDocGlw6HheDtSU1PUoUObjzzeZIfo6urQ4CG65uMdOHQ4NEgeHQ4NkkeHQ4Pk0eHQIHl0ODRIHh0ODZJHh0OD5NHh0CB5dDg0SPHr4OkcAAAAQIwYogEAAIAY1WuIfvbZZ5Wfn6+8vDwNHz5cf/jDHyRJxcXFGjlypIYMGaKRI0dq48aNtR/T0GMAAACAuzqH6BCCbr31Vk2fPl2LFi3S9OnTNXHiRFVXV2vKlCkaPXq0li1bptGjR6ugoKD24xp6DAAAAHBXr0eiU1NTVV5eLkkqLy9Xp06dtHPnTq1bt065ubmSpNzcXK1bt05lZWUqLS1t0DEAAACgKajz1TlSUlL04x//WN/85jfVunVr7dmzRw899JBKSkrUuXNnpaWlSZLS0tLUqVMnlZSUKITQoGOZmZn1Du/YsW1D/r61srLaHdfHNxaHDocGyaPDoUHy6HBokDw6HBokjw6HBsmjw6FB8uhwaJA8OhwaJI8OhwYpfh11DtEHDx7Ugw8+qFmzZqlfv37661//qm9/+9uaPn16XILqq7S0osEvWZKV1U47dpQ3clHT7HBocOlwaHDpcGhw6XBocOlwaHDpcGhw6XBocOlwaHDpcGg43o7U1JRjPmhb5xD92muvafv27erXr58kqV+/fmrVqpXS09O1bds2VVVVKS0tTVVVVdq+fbuys7MVQmjQMQAAAKApqHOI7tKli7Zu3aoNGzbo1FNP1fr161VaWqqPf/zjysnJUVFRkfLy8lRUVKScnJzap2Q09FhjaNe+lVqmH/uvVtdD+/srD6p8975GawIAAMCJo84hOisrS1OnTtX48eOVkpIiSbr77ruVkZGhqVOnatKkSZo1a5bat2+vwsLC2o9r6LHG0DK9mYbdsui4/owl9+Yp8f8JAQAAAEf1etvv4cOHa/jw4Ufc3qtXL82dO/eoH9PQYwAAAIA73rEQAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiFGzun7D5s2bdeONN9b+ury8XBUVFXrxxRdVXFysSZMmadeuXcrIyFBhYaF69OghSQ0+BgAAALir85HoU045RYsWLar9cfHFFys3N1eSNGXKFI0ePVrLli3T6NGjVVBQUPtxDT0GAAAAuIvp6RwHDhzQkiVLdMUVV6i0tFTr1q2rHahzc3O1bt06lZWVNfgYAAAA0BTU+XSOQ61YsUKdO3fWGWecobVr16pz585KS0uTJKWlpalTp04qKSlRCKFBxzIzM+vd0rFj21jSGyQrq90JcR9NoUHy6HBokDw6HBokjw6HBsmjw6FB8uhwaJA8OhwaJI8OhwbJo8OhQYpfR0xD9Lx583TFFVfEJSRWpaUVqq4ORz3WWIu1Y0d5o/w5HyUrq13c76MpNLh0ODS4dDg0uHQ4NLh0ODS4dDg0uHQ4NLh0ODS4dDg0HG9HamrKMR+0rffTObZt26aXXnpJw4YNkyRlZ2dr27ZtqqqqkiRVVVVp+/btys7ObvAxAAAAoCmo9xC9YMECXXTRRerQoYMkqWPHjsrJyVFRUZEkqaioSDk5OcrMzGzwMQAAAKApqPfTORYsWKDbbrvtsNumTp2qSZMmadasWWrfvr0KCwuP+xgAAADgrt5D9LJly464rVevXpo7d+5Rf39DjwEAAADueMdCAAAAIEYM0QAAAECMGKIBAACAGDFEAwAAADGK6c1WEJt27VupZfqxl/hYbwyzv/Kgynfva+wsAAAAHCeG6Dhqmd5Mw25Z1OCPX3JvnhL/Xj8AAAD4MJ7OAQAAAMSIIRoAAACIEUM0AAAAECOGaAAAACBGDNEAAABAjBiiAQAAgBgxRAMAAAAxYogGAAAAYsQQDQAAAMSIIRoAAACIEUM0AAAAEKN6DdGVlZWaMmWKLr30Ug0bNkx33HGHJKm4uFgjR47UkCFDNHLkSG3cuLH2Yxp6DAAAAHBXryH6nnvuUXp6upYtW6YlS5Zo/PjxkqQpU6Zo9OjRWrZsmUaPHq2CgoLaj2noMQAAAMBdnUP0nj17tHDhQo0fP14pKSmSpJNPPlmlpaVat26dcnNzJUm5ublat26dysrKGnwMAAAAaAqa1fUbNm3apIyMDM2cOVOrV69WmzZtNH78eLVs2VKdO3dWWlqaJCktLU2dOnVSSUmJQggNOpaZmVnv8I4d2zbk7xuTrKx2cb8PhwaHv6fk0eHQIHl0ODRIHh0ODZJHh0OD5NHh0CB5dDg0SB4dDg2SR4dDgxS/jjqH6KqqKm3atEmf/OQnNXHiRL3yyiu64YYbdP/998clqL5KSytUXR2OeqyxFmvHjvLj+vjG6DjehrpkZbWL+300lQ6HBpcOhwaXDocGlw6HBpcOhwaXDocGlw6HBpcOh4bj7UhNTTnmg7Z1DtHZ2dlq1qxZ7dMvzjrrLHXo0EEtW7bUtm3bVFVVpbS0NFVVVWn79u3Kzs5WCKFBx9D42rVvpZbpxz7NdQ37+ysPqnz3vsbMAgAAaNLqHKIzMzM1YMAArVq1SgMHDlRxcbFKS0vVo0cP5eTkqKioSHl5eSoqKlJOTk7tUzIaegyNq2V6Mw27ZdFx/RlL7s1T4r+WBAAA8FHnEC1J3//+9zV58mQVFhaqWbNmmj59utq3b6+pU6dq0qRJmjVrltq3b6/CwsLaj2noMQAAAMBdvYbo7t2761e/+tURt/fq1Utz58496sc09BgAAADgjncsBAAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLUrD6/afDgwWrRooXS09MlSd/97nd1wQUX6OWXX1ZBQYEqKyvVrVs33XPPPerYsaMkNfgYAAAA4K7ej0T/5Cc/0aJFi7Ro0SJdcMEFqq6u1oQJE1RQUKBly5apf//+mjFjhiQ1+BgAAADQFDT46Rxr165Venq6+vfvL0kaNWqUli5delzHAAAAgKagXk/nkN5/CkcIQf369dPNN9+skpISde3atfZ4ZmamqqurtWvXrgYfy8jIaKS/FgAAABA/9Rqif/Ob3yg7O1sHDhzQD37wA02bNk2XXHJJvNuOqWPHtnG/j6ysdnG/j6bQIEXT4fB3dWiQPDocGiSPDocGyaPDoUHy6HBokDw6HBokjw6HBsmjw6FBil9HvYbo7OxsSVKLFi00evRofeMb39DYsWO1ZcuW2t9TVlam1NRUZWRkKDs7u0HHYlFaWqHq6nDUY421WDt2lB/XxzdGh0NDY3TUJSurXdzvoyk0uHQ4NLh0ODS4dDg0uHQ4NLh0ODS4dDg0uHQ4NBxvR2pqyjEftK3zOdF79+5Vefn7dx5C0FNPPaWcnBz17dtX+/fv15o1ayRJjz/+uIYOHSpJDT4GAAAANAV1PhJdWlqqcePGqaqqStXV1erVq5emTJmi1NRUTZ8+XVOmTDnspeokNfgYAAAA0BTUOUR3795dCxcuPOqxc845R0uWLGnUYwAAAIA73rEQAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABCjOt/2G2gM7dq3Usv0Y2+3rKx2H3lsf+VBle/e19hZAAAADcIQjUi0TG+mYbcsavDHL7k3T+WN2AMAAHA8eDoHAAAAECOGaAAAACBGDNEAAABAjBiiAQAAgBgxRAMAAAAxYogGAAAAYhTTED1z5kydfvrpevPNNyVJL7/8soYPH64hQ4bouuuuU2lpae3vbegxAAAAwF29h+h//vOfevnll9WtWzdJUnV1tSZMmKCCggItW7ZM/fv314wZM47rGAAAANAU1GuIPnDggKZNm6apU6fW3rZ27Vqlp6erf//+kqRRo0Zp6dKlx3UMAAAAaArq9Y6F999/v4YPH65TTjml9raSkhJ17dq19teZmZmqrq7Wrl27GnwsIyOj3uEdO7at9+9tqGO9DXVUHBokj44oGhz+npJHh0OD5NHh0CB5dDg0SB4dDg2SR4dDg+TR4dAgeXQ4NEjx66hziP773/+utWvX6rvf/W5cAhqqtLRC1dXhqMcaa7F27Di+N5pujA6HBpeO422oS1ZWu7jfR1PpcGhw6XBocOlwaHDpcGhw6XBocOlwaHDpcGg43o7U1JRjPmhb5xD90ksvaf369br44oslSVu3btVXvvIVXX311dqyZUvt7ysrK1NqaqoyMjKUnZ3doGMAAABAU1Dnc6K/9rWv6bnnntOKFSu0YsUKdenSRb/4xS90/fXXa//+/VqzZo0k6fHHH9fQoUMlSX379m3QMQAAAKApqNdzoo8mNTVV06dP15QpU1RZWalu3brpnnvuOa5jAAAAQFMQ8xC9YsWK2p+fc845WrJkyVF/X0OPAQAAAO54x0IAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRs3q85u++c1vavPmzUpNTVXr1q11xx13KCcnR8XFxZo0aZJ27dqljIwMFRYWqkePHpLU4GMAAACAu3o9El1YWKjFixdr4cKFuu666zR58mRJ0pQpUzR69GgtW7ZMo0ePVkFBQe3HNPQYAAAA4K5eQ3S7du1qf15RUaGUlBSVlpZq3bp1ys3NlSTl5uZq3bp1Kisra/AxAAAAoCmo19M5JOm2227TqlWrFELQww8/rJKSEnXu3FlpaWmSpLS0NHXq1EklJSUKITToWGZmZr3DO3ZsG8vfs0GystrV/ZuSoEHy6IiiweHvKXl0ODRIHh0ODZJHh0OD5NHh0CB5dDg0SB4dDg2SR4dDgxS/jnoP0T/4wQ8kSQsXLtT06dM1fvz4uATVV2lphaqrw1GPNdZi7dhRflwf3xgdDg0uHcfbUJesrHZxv4+m0uHQ4NLh0ODS4dDg0uHQ4NLh0ODS4dDg0uHQcLwdqakpx3zQNuZX58jPz9fq1avVpUsXbdu2TVVVVZKkqqoqbd++XdnZ2crOzm7QMQAAAKApqHOI3rNnj0pKSmp/vWLFCp100knq2LGjcnJyVFRUJEkqKipSTk6OMjMzG3wMAAAAaArqfDrHvn37NH78eO3bt0+pqak66aSTNHv2bKWkpGjq1KmaNGmSZs2apfbt26uwsLD24xp6DAAAAHBX5xB98skn64knnjjqsV69emnu3LmNegwAAABwxzsWAgAAADFiiAYAAABixBANAAAAxIghGgAAAIgRQzQAAAAQI4ZoAAAAIEYM0QAAAECMGKIBAACAGDFEAwAAADFiiAYAAABixBANAAAAxIghGgAAAIgRQzQAAAAQI4ZoAAAAIEYM0QAAAECMGKIBAACAGDFEAwAAADGqc4jeuXOnvvrVr2rIkCEaNmyYbrrpJpWVlUmSXn75ZQ0fPlxDhgzRddddp9LS0tqPa+gxAAAAwF2dQ3RKSoquv/56LVu2TEuWLFH37t01Y8YMVVdXa8KECSooKNCyZcvUv39/zZgxQ5IafAwAAABoCuocojMyMjRgwIDaX5999tnasmWL1q5dq/T0dPXv31+SNGrUKC1dulSSGnwMAAAAaAqaxfKbq6ur9bvf/U6DBw9WSUmJunbtWnssMzNT1dXV2rVrV4OPZWRk1LulY8e2saQ3SFZWu7jfR1NokDw6omhw+HtKHh0ODZJHh0OD5NHh0CB5dDg0SB4dDg2SR4dDg+TR4dAgxa8jpiH6zjvvVOvWrTVmzBgtX748LkH1VVpaoerqcNRjjbVYO3aUH9fHN0aHQ4NLx/E21CUrq13c76OpdDg0uHQ4NLh0ODS4dDg0uHQ4NLh0ODS4dDg0HG9HamrKMR+0rfcQXVhYqLfffluzZ89WamqqsrOztWXLltrjZWVlSk1NVUZGRoOPAQAAAE1BvV7i7r777tPatWv1wAMPqEWLFpKkvn37av/+/VqzZo0k6fHHH9fQoUOP6xgAAADQFNT5SPRbb72lBx98UD169NCoUaMkSaeccooeeOABTZ8+XVOmTFFlZaW6deume+65R5KUmpraoGMAAABAU1DnEN27d2+98cYbRz12zjnnaMmSJY16DAAAAHDHOxYCAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRAAAAQIwYogEAAIAYMUQDAAAAMarzHQuBE0W79q3UMv3YWz4rq90xj++vPKjy3fsaMwsAADRBDNFIGi3Tm2nYLYuO689Ycm+eyhupBwAANF08nQMAAACIEUM0AAAAECOGaAAAACBGDNEAAABAjBiiAQAAgBgxRAMAAAAxqnOILiws1ODBg3X66afrzTffrL29uLhYI0eO1JAhQzRy5Eht3LjxuI8BAAAATUGdQ/TFF1+s3/zmN+rWrdtht0+ZMkWjR4/WsmXLNHr0aBUUFBz3MQAAAKApqHOI7t+/v7Kzsw+7rbS0VOvWrVNubq4kKTc3V+vWrVNZWVmDjwEAAABNRYPesbCkpESdO3dWWlqaJCktLU2dOnVSSUmJQggNOpaZmdlIfyUAAAAgvprs23537Ng27veRldUu7vfRFBokjw6HBimaDoe/q0OD5NHh0CB5dDg0SB4dDg2SR4dDg+TR4dAgeXQ4NEjx62jQEJ2dna1t27apqqpKaWlpqqqq0vbt25Wdna0QQoOOxaq0tELV1eGoxxprsXbsKD+uj2+MDocGlw6HhsboqEtWVru430dTaHDpcGhw6XBocOlwaHDpcGhw6XBocOlwaDjejtTUlGM+aNugl7jr2LGjcnJyVFRUJEkqKipSTk6OMjMzG3wMAAAAaCrqfCT6rrvu0h/+8Ae9++67uvbaa5WRkaEnn3xSU6dO1aRJkzRr1iy1b99ehYWFtR/T0GMAAABAU1DnEH377bfr9ttvP+L2Xr16ae7cuUf9mIYeAwAAAJoC3rEQAAAAiBFDNAAAABAjhmgAAAAgRk32daKBpqhd+1ZqmV73p92xXo5vf+VBle/e15hZAAAgRgzRQIRapjfTsFsWHdefseTePCX+lTcBAEhuPJ0DAAAAiBGPRANJqD5PK6nrHR55WgkAIJkxRANJiKeVAABwfBiiASQE32QJAGjKGKIBJITLo+E8tQUA0BAM0QCSmsMwz6PyAND0MEQDQII5DPIAgNgwRAMAJPHUFgCIBUM0AECSxyPiPLUFQFPBEA0AsOEwyANAfTBEAwDwITy1BUBdGKIBAPgQHhEHUBeGaAAADLk8P9zhUXmXtQAOlbAhuri4WJMmTdKuXbuUkZGhwsJC9ejRI1E5AABYcXk03KHDoQH4sIQN0VOmTNHo0aOVl5enRYsWqaCgQI899liicgAAAI7peB+Vj+p/Blw6HBri2ZGQIbq0tFTr1q3TnDlzJEm5ubm68847VVZWpszMzHr9GampKcc83qlDq+PurOs+6uN4OxwaXDocGhqjw6HBpcOhwaXDocGlw6HBpcOhwaXDoaFlejN95a4/NPjjf3H7pdqT4AaXDoeGujrq2i8pIYRwXPfeAGvXrtXEiRP15JNP1t522WWX6Z577tEZZ5wRdQ4AAAAQk9REBwAAAABNTUKG6OzsbG3btk1VVVWSpKqqKm3fvl3Z2dmJyAEAAABikpAhumPHjsrJyVFRUZEkqaioSDk5OfV+PjQAAACQSAl5TrQkrV+/XpMmTdLu3bvVvn17FRYW6tRTT01ECgAAABCThA3RAAAAQFPFNxYCAAAAMWKIBgAAAGLEEA0AAADEiCEaAAAAiBFDNAAAABAjhmgAAAAgRgzRgJmCgoJEJyTMgQMHtG3btiNuf+uttxJQ875kPh/4P457E0BiNUt0QCLNmzdPV1xxRULue8OGDfrHP/6hPn36qE+fPglpkKSvf/3revDBBxN2/1Li1mLLli1aunSpSkpKJL3/dvRDhgxRt27dImuYPn36Ebc9/fTTatu2rSTp1ltvjaTj2WefVWpqqi666CKtWbNGS5cu1emnn66rrroqkvuXpOeee07f+c53JEndu3fXj370I3384x+X9P46LFiwIO4NLufjzTffVEpKinr37q2NGzfqj3/8oz7xiU/os5/9bCT3/1EScb1wWAuHvSl5XLMkac2aNXr66acP6/j85z+v/v37R9pxNKtWrdL5558fyX3t3btXv/vd79SlSxddfvnlmjNnjlavXq3evXvrhhtuUJs2beLe8Oyzz2rgwIFq3rx53O/rWMrKytS2bVu1aNFCkrRo0SK9+uqrkf87smXLFm3dulV9+/atbZHity/Spk6dOrXR/9Qm4pvf/KauueaaSO5r3LhxuuyyyyRJK1eu1Lhx47R//349/PDDyszMjGR4HD9+vJYuXXrYj+eff16vvfaali5dqs9//vNxb5A81mLu3LmaOHGisrKy1KVLF7Vt21ZbtmzRjBkz1KZNG51xxhlxb5CkG2+8UZ07d1ZWVpaaN2+u5s2b65VXXtFnPvMZNW/eXOeee27cG3784x9r/vz5ev755/X2229ryZIl+uQnP6nly5frnXfe0YABA+LeIEnf+c53NHPmTE2ePFktWrTQ5MmTNWDAAJ188sl6/PHHNWrUqLg3OJyPX/3qV7rrrru0cOFCpaam6ic/+YlatGih3/72t0pJSdGZZ54Z9wbJ43rhshYOe9PlmjVr1iz94he/0Lnnnqt+/frp9NNPV0pKih588EGVlZVF8jlyLGPHjo3s3/WJEyeqpKREa9eu1cqVK7V582bl5ubq9ddf1zPPPKNLL7007g2XXXaZfvvb32rbtm3q1KmTTj755Ljf59GMHDlS+fn5atGihX72s5/pySef1Jlnnqk//vGPev311zVw4MC4NyxevFg33XSTXnzxRT3yyCM666yzlJ2dLen961lcPk/DCe5b3/rWUX+MGzcunH322ZF15OXl1f58zJgxYd26dSGEEDZv3nzYsXj67Gc/G773ve+F+fPnh/nz54d58+aFAQMG1P46Kg5rcemll4bS0tIjbi8tLQ2XXHJJJA0hhLB+/fowZsyY8Mgjj4Tq6uoQQgif+9znIrv/EELIzc0NBw8eDOXl5eGss84KO3fuDCGEsGfPnpCbmxtZx/Dhww/79erVq8OgQYPCK6+8EvLz8yNpcDgfw4YNC3v27Ak7duwIZ555ZigpKQkhvL83o/r8CMHjeuGyFg570+Wadckll4T9+/cfcfu+ffvCf/zHf0TSUFhYeNQf//Vf/xXOOeecSBpCCLXXx8rKynDuueeGysrKEEIIVVVVkV078/LywmuvvRbuvPPOMGDAgDBixIjw61//OuzevTuS+69x6N93xIgRoaKiIoQQwoEDByJbi+HDh4etW7eGEEL4y1/+EgYNGhT+/Oc/hxBC3K4XJ/xzoleuXKnzzz9fgwYNOuLHoQ/1x1tKSkrtzysqKpSTkyNJkf433JIlS7Rnzx7985//1NChQ/WFL3xBrVu31ogRIzRixIjIOhzWorq6WpmZmUfc3qFDB4UQIus49dRT9eijj6qyslJjx47V+vXrD1ufKDRr1kxpaWlq27atPvaxjykjI0OS1Lp1a6WlpUXWUVVVpcrKytpfn3vuubrvvvs0fvz4oz4XNR4czkdqaqpat26tk08+Wd27d1eXLl0kSZmZmZG2OFwvXNbCYW+6XLNCCEdd+5SUlMg6fvWrXyk9PV2tW7c+7EebNm0i/3yV/u/vXnPfqampka1FSkqK+vTpo9tvv11//vOfdf311+vZZ5/VhRdeqFtuuSWShhplZWWS3v+3Iz09XZLUvHlzVVVVRXL/IQR17txZkjRgwAD9/Oc/V0FBgZ599tm47YsT/jnROTk56tOnz1H/2+/++++PrGPz5s0aP368Qgjatm2bDhw4UDvEHzx4MJKGzMxM3X///Vq8eLHGjBmjCRMmJOSC47AWAwcO1PXXX68vfvGL6tq1q6T3n0v1xBNPRPZ8uhqpqam64YYbNGjQIH3ve9/T3r17I73/6urq2n8A7r777trbQwiRnQ/p/f+WXLNmzWHr/+lPf1r333+/pk2bFlmHw/mocfPNNx927L333ousw+F64bIWDnvT5ZqVn5+vq666Svn5+Yd1LFy4UPn5+ZE0fOITn9CQIUOO+tS/uXPnRtIgSaeffrq+/e1va//+/Ro4cKAmTZqkSy65RM8995x69uwZScOhw3rz5s112WWX6bLLLtPWrVu1cOHCSBok6Rvf+IbGjh2r6667Tv3799e3vvUtDRkyRKtWrdKFF14YWcfu3bvVvn17SdJpp52mRx55RNdff73+/e9/x+X+UkKUX8ImwOuvv66OHTsqKyvriGPvvPNOZI9+fvgbTz73uc8pIyND27Zt029/+6wT4dkAACAASURBVNvab1qJyrZt21RQUKCXX35Zq1evjvS+Hdaiurpaixcv1tNPP60tW7ZIkrp27aqhQ4cqLy9PqamJ+U+agwcPaseOHbXP44rCn//8Z/Xv31+tWrU67PaNGzfqmWee0Ve+8pXIWtwk4nzMnz9fQ4YMOeKbktavX68nnnhC3/ve9yJrqZGo64XjWiSK0zVrzZo1euqpp47oiOr50KtWrdLHPvYxde/e/Yhjf/3rX9WvX79IOvbv36///u//VkpKikaNGqXnn39ejz/+uE455RTdeOON6tChQ9wbJk6cqMLCwrjfT328+uqrevTRR7V+/XpVVVWpa9euys3N1eWXXx7JF+Bz585Vjx499JnPfOaw2zdt2qQZM2bE5YHTE36IBpoSl1dtceHw6jEODUi8qL/rH4C/E/450ccyb968SO/v2Wef1cqVKyW9/5X8XXfdFel/Pbk0HEuU52TLli3629/+pgMHDhx2+6pVqyJrGDduXO3PV65cqbFjx2rFihX6+te/rkWLFkXW8VGiPB/jx48/4seLL75Y+/NkaZA89uaxRH3tPNTXv/71yO9z8eLF+sIXvqApU6ZoyJAh+vvf/157bMaMGZH3SO9/wb1w4UK9/vrrCbn/o0m26/exJNvniORxTqKecU7450Qfy09/+tPIXif6xz/+sVatWqWDBw/qL3/5i9auXasLLrhAixcv1tatWw8bpk7khrpEdU4WL16su+++W1lZWaqoqNB9992nT3/605Le/0cxqkeWNm3aVPvzhx9+WD//+c+Vk5Ojd955RzfeeKPy8vIi6fgoUX6OrFmzRhdddFHtf8WFELR69WoNGjQokvt3aXDZm8cS1b442hcuNV/USNF9X8svfvELLVq0SJ07d9bq1at18803684779TAgQMj+waycePG6ac//amk97/gvu2223TOOefoRz/6kW6++eaEXyuk5Lt+H0uyfY44nJNEzDgn/BD9UY8ehRDi9kTzo3nmmWe0cOFC7du3TwMHDtQf//hHZWRkaMyYMRo5cmQkA6xDg+RxThz+UZQ8XqnE4XxI778axPe//33985//1C233KJWrVpp5syZkb5yjEODy9502BcOX9TU3O+Hv+v/a1/7mu64447IvtnS5Qtuh33B58j/cfkccTgniZhxTvgheuXKlZo8efIR7+ZTs9Gi4vAyYg4Nksc5cfhHUfJ4pRKH8yF5vBqEQ4PL3nTYFw5f1NSI+rv+P8zhC27JY1/wOfJ/XD5HHM5JImacE36IdnmJO4eXEXNokHzOSaL/UZSkyZMn1/78c5/7nPbu3asWLVpo27ZtuvjiiyNpcDkfNYYPH64BAwaooKBAFRUVkd+/Q4PD3nTYFw5f1EjS1VdfrTfeeOOw7/rv0aOH5syZE9lzoh2+4JY89oXE50gNl88RKfHnJCEzTmO/e4ub1157LWzfvv2oxzZv3hxZx5/+9Kewd+/eI24vLi4ODz/8cNI0hOBxTp544onw4osvHnH7//7v/4ZvfetbkTTE4oEHHojbn+1wPmIRz7VwaHDZm277YuvWreFrX/taOPfccyO/7/qK576oeafImh817yy6devWcN9998Xtfj/MYV/wOXJ0ifwccTgniZhxTvghur4c/mEOwaPDoSEEjw6HhhBCZG8tfCyshVeDy/lw6HBoCIF9cSiHDoeGEDw6HBpC8OhozIakfom7Qy1fvjzRCZI8OhwaJI8OhwZJkX6zzEdhLbwaXM6HQ4dDg8S+OJRDh0OD5NHh0CB5dDRmA0P0BxwufpJHh0OD5NHh0CApYc9xOxRr4dXgcj4cOhwaJPbFoRw6HBokjw6HBsmjozEbGKI/4HDxkzw6HBokjw6HBheshReX8+HQ4dDgwmUtHDocGiSPDocGyaOjMRsYooEmwOGrdxcOa+HQAD/sCyC5MER/wOXi59Dh0CB5dETZUFxcrP/5n/+RJO3Zs0e7du2qPfbII49E1vFRkm0tHBqOxeHzQ/LoSLa9eSwO50Py6HBokDw6HBokj45GbWi0b1FsAjZs2BCWL18eQgihoqKi9iWCQgihtLQ0qTocGlw6HBrmz58fhgwZEgYPHhxCCGH9+vXhy1/+ciT3fSjWwqchBI/z4dLh0MC+8OtwaHDpcGhw6YiqIWmGaJeLn0OHQ4NLh0NDCCHk5eWFioqKkJeXV3vb5ZdfHmkDa+HV4HI+HDocGkJgX7h1ODS4dDg0uHRE2ZA0T+d49NFHNW/ePLVr106SdOqpp+rdd99Nyg6HBpcOhwZJat68udq0aXPYbVG+FbvEWrg1uJwPhw6HBol94dbh0ODS4dDg0hFlQ9IM0Q4XP5cOhwaXDocGScrIyFBxcXHtdw0vWrRIXbp0ibSBtfBqcDkfDh0ODRL7wq3DocGlw6HBpSPKhmZx+VMNOVz8XDocGlw6HBokafLkybrllltUXFyswYMHq2XLlpo9e3akDayFV4PL+XDocGiQ2BduHQ4NLh0ODS4dUTakhGDwrZIRKC4u1i233KINGzYoMzOz9uL3sY99LOk6HBpcOhwaalRVVWnjxo0KIahnz56Rf/XOWng1uJwPhw6HhhrsC58OhwaXDocGl44oG5JmiJYSf/Fz6nBocOlwaJCkN998Uy+++KIk6bzzztNpp50WeQNr4dXgcj4cOhwaJPaFW4dDg0uHQ4NLR1QNaVOnTp0alz/Z0L/+9S+tXr1amzdvVtu2bZWZmZm0HQ4NLh0ODb/5zW9UUFCgdu3aqbS0VD/5yU+Unp6uT33qU5F2sBY+DZLH+XDpcGhgX/h1ODS4dDg0uHRE1ZA0Q7TLxc+hw6HBpcOhQZImTJiguXPn6vLLL9fgwYOVl5enO+64Q1dffXVkDayFV4PL+XDocGiQ2BduHQ4NLh0ODS4dkTbE5YXzDF166aXh3Xffrf11aWlpuPTSS5Oyw6HBpcOhIYQQRowYUa/b4om18GpwOR8OHQ4NIbAv3DocGlw6HBpcOqJsSJpX52jTpo06duxY++vMzMwjXgIlWTocGlw6HBok6fzzz9dtt92mK6+8UpK0YMECXXDBBfrXv/4lSZE875K18GpwOR8OHQ4NEvvCrcOhwaXDocGlI8qGpPnGwnvvvVdlZWWHXfw6dOigYcOGSYrm4ufS4dDg0uHQIEmDBw/+yGMpKSl65pln4t7AWng1uJwPhw6HBol94dbh0ODS4dDg0hFlQ9IM0Q4XP5cOhwaXDocGF6yFF5fz4dDh0ODCZS0cOhwaXDocGlw6omxImiEacFZQUKAxY8boE5/4RKJTEs5hLRwa4Id9AeBQSfO23wUFBXrzzTcTnWHR4dDg0uHQIEk9e/bUuHHj9KUvfUlPPfWUDh48GHkDa+HV4HI+HDocGiT2hVuHQ4NLh0ODS0eUDUnzEncbN27UPffco6VLl6pVq1bq2bOnUlOj/xrCocOhwaXDoUGSPv3pT+vqq69WVlaWfv/732vGjBkqLy9Xz549I/umDNbCq8HlfDh0ODRI7Au3DocGlw6HBpeOKBuSZoh2uPi5dDg0uHQ4NByqQ4cO2r17t/7xj3+osrJSs2fPVkpKis4+++y43zdr4dXgcj4cOhwaDsW+8OhwaHDpcGhw6YiyIWmG6BoO/zC7dDg0uHQkumHt2rX60Y9+pMLCQp1yyimaMmWKrr76al111VW69dZbdc0110TSIbEWLg01En0+nDoS3cC+8OxwaHDpcGhw6YikIS6vPm3o1VdfDZMmTQoXXHBBKCwsDJs2bQohhFBeXh4uuuiipOpwaHDpcGgIIYTc3Nzw+OOPh7179x5x7He/+10kDayFV4PL+XDocGgIgX3h1uHQ4NLh0ODSEWVD0gzRDhc/lw6HBpcOh4YQQnj++efrdVs8sRZeDS7nw6HDoSEE9oVbh0ODS4dDg0tHlA1JM0Q7XPxcOhwaXDocGkIIIT8/v163xRNr4dXgcj4cOhwaQmBfuHU4NLh0ODS4dETZkDQvcTd9+vR63ZYMHQ4NLh2Jbnj77be1cuVKVVRUaOXKlbU/ioqKtG/fvsg6JNbCpaFGos+HU0eiG9gXnh0ODS4dDg0uHVE2NIvLn2rk7bff1saNG2svfjXKy8sjvfg5dDg0uHQ4NEjS3/72N82fP1/vvvuuHn744drb27Ztq0mTJkXSwFp4NbicD4cOhwaJfeHW4dDg0uHQ4NKRiIYT/h0LFyxYoPnz52vt2rXq27dv7e1t27bVyJEjNWjQoKTpcGhw6XBoONT8+fP1hS984SOP/+lPf9KFF14Yl/tmLbwaXM6HQ4dDw6HYFx4dDg0uHQ4NLh0JaYjLk0QMzZs375jHV65cmTQdDg0uHQ4N9RHF8y5ZC68Gl/Ph0OHQUB/si2g7HBpcOhwaXDqibEiaIbouDv8wh+DR4dAQgkeHQ0MIIeTl5SU6gbUwa3A5Hw4dDg0hsC8O5dDh0BCCR4dDQwgeHY3ZkDTfWFiXYPKsFocOhwbJo8OhQZJSUlISncBamDW4nA+HDocGiX1xKIcOhwbJo8OhQfLoaMwGhugPOFz8JI8OhwbJo8OhwQVr4cXlfDh0ODS4cFkLhw6HBsmjw6FB8uhozAaGaKAJcPjq3YXDWjg0wA/7AkguDNEfcLn4OXQ4NEgeHVE0VFVV6fbbbz/m7/nOd74T9466JMtaODTUh8Pnh+TRkSx7sz4czofk0eHQIHl0ODRIHh08nSNGLhc/hw6HBpcOhwZJSktL0xtvvHHM33PRRRfFtYG18GpwOR8OHQ4NEvvCrcOhwaXDocGlI+qGE/7NViSPi59Lh0ODS4dDQ43zzjtP06ZNU35+vlq3bl17+2mnnRbJ/bMWXg0u58Ohw6GhBvvCp8OhwaXDocGlI+qGE/7NVmrce++92rNnT0L/YXbpcGhw6XBokKTBgwcfcVtKSoqeeeaZyBpYC68Gl/Ph0OHQILEv3DocGlw6HBpcOqJsSJoh2uHi59Lh0ODS4dDggrXw4nI+HDocGly4rIVDh0ODS4dDg0tHlA1JM0QD7l544QWtX79eY8aMUWlpqXbv3q2ePXsmOishHNbCoQF+2BcAaiTFNxbWeOGFF/TrX/9aklRaWqri4uKk7XBocOlwaHjooYc0c+ZMPfbYY5Kk9957T5MnT468g7XwaZA8zodLh0MD+8Kvw6HBpcOhwaUjqoakGaJdLn4OHQ4NLh0ODZJUVFSkX/7yl7XP3+rSpYsqKioibWAtvBpczodDh0ODxL5w63BocOlwaHDpiLIhaYZoh4ufS4dDg0uHQ4MktWzZUs2bNz/stqjf2Ym18GpwOR8OHQ4NEvvCrcOhwaXDocGlI8qGpHiJO8nj4ufS4dDg0uHQIL3/Sb5mzRqlpKSourpas2fPVu/evSNtYC28GlzOh0OHQ4PEvnDrcGhw6XBocOmIsiFphmiHi59Lh0ODS4dDgyTdcccdmjhxot566y2dddZZ6t+/v2bMmBFpA2vh1eByPhw6HBok9oVbh0ODS4dDg0tHpA0hSWzfvj1ce+214Ywzzgh9+/YN11xzTXj33XeTssOhwaXDoeFQe/fuDRUVFQm5b9bCq8HlfDh0ODQcin3h0eHQ4NLh0ODSEWVD0r3E3b59+1RdXa02bdokfYdDg0tHohvGjh2rK6+8UpdeeqlatmyZkIYarIVHQ41Enw+njkQ3sC88OxwaXDocGlw6omhImzp16tS4/elGxo4dq7S0NJ122mmHvYNNMnY4NLh0ODRIUocOHfTkk0/q7rvv1saNG5WRkaHs7OxIG1gLrwaX8+HQ4dAgsS/cOhwaXDocGlw6omxImiHa4eLn0uHQ4NLh0CBJPXr00Oc//3ldccUV2rlzpx566CHNmTNHY8aMiayBtfBqcDkfDh0ODRL7wq3DocGlw6HBpSPKhqR7OsfOnTv15JNPasGCBdqzZ4+WLl2atB0ODS4dDg2S9O9//1tLlizR/PnztWfPHi1btizyBtbCq8HlfDh0ODRI7Au3DocGlw6HBpeOKBqS5tU5aqSmvv/S2CEEJfLrB4cOhwaXjkQ3rFixQgsWLNBf//pXXXzxxbrtttvUr1+/yDsk1sKloUaiz4dTR6Ib2BeeHQ4NLh0ODS4dUTQkzSPRH7745efnJ+Ti59Dh0ODS4dAgSddee61GjBiR0G9YYi28GlzOh0OHQ4PEvnDrcGhw6XBocOmItCEur/lh6JprrgmLFi0K+/btS/oOhwaXDoeGgwcPhttuuy1h91+DtfBpCMHjfLh0ODSwL/w6HBpcOhwaXDqibEiKp3NUVVWpW7duGj58eNJ3ODS4dDg0SFJaWpreeOONhDawFl4NLufDocOhQWJfuHU4NLh0ODS4dETdkBRDtMPFz6XDocGlw6Ghxnnnnadp06YpPz//sJfkOe200yK5f9bCq8HlfDh0ODTUYF/4dDg0uHQ4NLh0RN2QNM+Jvvfee7Vnz56E/sPs0uHQ4NLh0CBJgwcPPuK2lJQUPfPMM5E1sBZeDS7nw6HDoUFiX7h1ODS4dDg0uHRE2ZA0Q7TDxc+lw6HBpcOhwQVr4cXlfDh0ODS4cFkLhw6HBpcOhwaXjigbkmaIBty98MILWr9+vcaMGaPS0lLt3r1bPXv2THRWQjishUMD/LAvANRITXRAlF544QX9+te/liSVlpaquLg4aTscGlw6HBoeeughzZw5U4899pgk6b333tPkyZMj72AtfBokj/Ph0uHQwL7w63BocOlwaHDpiKohaYZol4ufQ4dDg0uHQ4MkFRUV6Ze//GXt87e6dOmiioqKSBtYC68Gl/Ph0OHQILEv3DocGlw6HBpcOqJsSJoh2uHi59Lh0ODS4dAgSS1btlTz5s0Puy0lJSXSBtbCq8HlfDh0ODRI7Au3DocGlw6HBpeOKBuS4iXuJI+Ln0uHQ4NLh0OD9P4n+Zo1a5SSkqLq6mrNnj1bvXv3jrSBtfBqcDkfDh0ODRL7wq3DocGlw6HBpSPKhqQZoh0ufi4dDg0uHQ4NknTHHXdo4sSJeuutt3TWWWepf//+mjFjRqQNrIVXg8v5cOhwaJDYF24dDg0uHQ4NLh2RNsT9PRFNbN++PVx77bXhjDPOCH379g3XXHNNePfdd5Oyw6HBpcOh4VB79+4NFRUVCblv1sKrweV8OHQ4NByKfeHR4dDg0uHQ4NIRZUPSvcTdvn37VF1drTZt2iR9h0ODS0eiG1566SV98pOfVJs2bTR37ly9+uqr+upXv6ru3btH3sJaeDTUSPT5cOpIdAP7wrPDocGlw6HBpSOKhqT5xsKXXnpJe/bsUatWrfTUU0+poKBAmzZtSsoOhwaXDocGSZo2bZpat26tt956S3PmzFHXrl112223RdrAWng1uJwPhw6HBol94dbh0ODS4dDg0hFlQ9IM0Q4XP5cOhwaXDocGSWrWrJlSUlL0pz/9Sf/5n/+pG264Qbt37460gbXwanA5Hw4dDg0S+8Ktw6HBpcOhwaUjyoakGaIdLn4uHQ4NLh0ODZJ08OBBvfLKK1q+fLnOO+88SVJVVVWkDayFV4PL+XDocGiQ2BduHQ4NLh0ODS4dUTYkzRDtcPFz6XBocOlwaJCk8ePHq6CgQGeffbZ69+6t4uJiffzjH4+0gbXwanA5Hw4dDg0S+8Ktw6HBpcOhwaUj0oa4fLuioeXLl4fhw4eHH/7whyGEEDZs2BBuvPHGpOxwaHDpcGiojwceeCDu98FaeDW4nA+HDoeG+mBfJOe+cOhwaHDpiLIhaYboujj8wxyCR4dDQwgeHQ4NIYSQn5+f6ATWwqzB5Xw4dDg0hMC+OJRDh0NDCB4dDg0heHQ0ZkPSPJ2jLsuXL090giSPDocGyaPDoUGSgsErUbIWXg0u58Ohw6FBYl8cyqHDoUHy6HBokDw6GrOBIfoDDhc/yaPDoUHy6HBokBLz9q0fxlp4NbicD4cOhwaJfXEohw6HBsmjw6FB8uhozAaG6A84XPwkjw6HBsmjw6HBBWvhxeV8OHQ4NLhwWQuHDocGyaPDoUHy6GjMBoZooAlw+OrdhcNaODTAD/sCSC4M0R9wufg5dDg0SB4dUTX84Ac/OOZtjzzySCQdx5JMa+HQUBeHzw/JoyOZ9mZdHM6H5NHh0CB5dDg0SB4djdrQaN+iaO6uu+465m2lpaVJ0+HQ4NLh0BDC0b+rPy8vL5L7rsFaeDW4nA+HDoeGENgXbh0ODS4dDg0uHVE2NGu8cdzbmjVrjrjtpZdeqv15ZmZm0nQ4NLh0JLrh6aef1tNPP6133nlH48ePr729oqJCLVu2jOt9fxhr4dFQI9Hnw6kj0Q3sC88OhwaXDocGl44oG074Idrl4ufQ4dDg0uHQIEk9e/bUoEGD9Oqrr2rQoEG1t7dt21b/7//9v0gaWAuvBpfz4dDh0CCxL9w6HBpcOhwaXDoS0XDCD9EOFz+XDocGlw6HBknq06eP+vTpo8GDBysjIyOy+z0Ua+HV4HI+HDocGiT2hVuHQ4NLh0ODS0ciGlJCMHiWdwR27dqVsIufW4dDg0tHohseffRRffnLX9b06dOPevzWW2+NrIW18Giokejz4dSR6Ab2hWeHQ4NLh0ODS0eUDSf8I9E1F7+HHnroqMejuvg5dDg0uHQ4NEhSenq6JKl169aR3N/RsBZeDS7nw6HDoUFiX7h1ODS4dDg0uHQkouGEH6IdLn4uHQ4NLh0ODZI0atQoSdJNN92UsAbWwqvB5Xw4dDg0SOwLtw6HBpcOhwaXjkQ0JM3TOQBnc+bM0ZVXXql27dppwoQJevXVV3X77bdr4MCBiU6LnMNaODTAD/sCwKGS5s1W5syZo/LycknShAkTNHToUD333HNJ2eHQ4NLh0CBJ8+fPV7t27fSXv/xFZWVluvvuu3XfffdF2sBaeDW4nA+HDocGiX3h1uHQ4NLh0ODSEWVD0gzRDhc/lw6HBpcOhwZJSktLkyStXr1aw4YN0znnnBP5OzuxFl4NLufDocOhQWJfuHU4NLh0ODS4dETZkDRDtMPFz6XDocGlw6FBklq2bKmHHnpITz75pM4//3yFEPTee+9F2sBaeDW4nA+HDocGiX3h1uHQ4NLh0ODSEWVD0gzRDhc/lw6HBpcOhwZJ+uEPf6gdO3bou9/9rrKysrRp0yYNGzYs0gbWwqvB5Xw4dDg0SOwLtw6HBpcOhwaXjigb0qZOnTo1Ln+ymX79+mn16tW66qqrdOaZZ2rTpk1KTU1V//79k67DocGlw6FBkjp06KALL7xQvXr1kiSddNJJSXk+JI+1cGhwOR8OHQ4NEvvCrcOhwaXDocGlI8oGXp0DMLBhwwb97Gc/06ZNm3Tw4MHa23//+98nsCoxHNbCoQF+2BcADpU0Q7TLxc+hw6HBpcOhQZLy8/M1dOhQnXXWWbXP55Kkc889N7IG1sKrweV8OHQ4NEjsC7cOhwaXDocGl44oG5JmiHa4+Ll0ODS4dDg0SNLw4cO1ePHiSO/zw1gLrwaX8+HQ4dAgsS/cOhwaXDocGlw6omw44d+xsEZ1dbVuuOGGRGdYdDg0uHQ4NEjS2Wefrddff119+vRJWANr4dXgcj4cOhwaJPaFW4dDg0uHQ4NLR5QNSTNEO1z8XDocGlw6HBok6R//+Ifmz5+vnj171r51qRTtf4GxFl4NLufDocOhQWJfuHU4NLh0ODS4dETZkFRP5/jXv/6V0IufS4dDg0uHQ4Mkvfjii0e9Per/AmMtfBpczodDh0ODxL5w63BocOlwaHDpiLIhaYZoh4ufS4dDg0uHQ4ML1sKLy/lw6HBocOGyFg4dDg0uHQ4NLh1RNiTNEA04Ky8v189//nO99tprqqysrL39scceS2BVYjishUMD/LAvABwqaZ4T7XLxc+hwaHDpcGiQpMmTJ6tXr17auHGjxo8fr3nz5umMM86ItIG18GpwOR8OHQ4NEvvCrcOhwaXDocGlI8qGpHnb78mTJys1NVUbN27UF7/4RaWlpenMM89Myg6HBpcOhwZJevvtt/Xtb39bLVu2VG5urh588EGtWbMm0gbWwqvB5Xw4dDg0SOwLtw6HBpcOhwaXjkgbQpIYNmxYCCGE3NzcEEIIlZWVYeTIkUnZ4dDg0uHQEEIIV1xxRQghhBEjRoSdO3eG6urqcMkll0TawFp4NbicD4cOh4YQ2BduHQ4NLh0ODS4dUTYkzdM5WrRoIUlq3ry5du3apZNOOkllZWVJ2eHQ4NLh0CBJPXr00K5duzRs2DCNHDlS7dq1i/y/iVkLrwaX8+HQ4dAgsS/cOhwaXDocGlw6omxImiHa4eLn0uHQ4NLh0CBJM2bMkCRde+21+tSnPqXy8nJdcMEFkTawFl4NLufDocOhQWJfuHU4NLh0ODS4dETZkJSvzrFmzZrai1+zZon7OsKhw6HBpSNRDVVVVbryyiu1YMGCyO6zLsm8Fg4NH+bw+eHSkcx788MczodLh0ODS4dDg0tH3Bvi8iQRMwcPHgz5+fmJzrDocGhw6XBoqDF69Oiwf//+hN0/a+HV4HI+HDocGmqwL3w6HBpcOhwaXDqibkiKp3OkpaWpdevWqqysPOzda5Kxw6HBpcOhoUbPnj31pS99SUOGDFHr1q1rb//Sl74Uyf2zFl4NLufDocOhoQb7wqfDocGlw6HBpSPqhqQYoqXEX/ycOhwaXDocGiRp586d6t27tzZs2FB7W3l5OWvxgajXwqHB5Xw4dDg0SOwLtw6HBpcOhwaXjigbkmaIdrj4uXQ4NLh0ODRI0pYtW/TAAw8cdtuIESMibWAtvBpczodDh0ODxL5w63BocOlwaHDpiLIhaYZoh4ufS4dDg0tHohsOHjyo9957T9XV1dq/f7/CB9/nW15ern379kXWIbEWLg01En0+nDoS3cC+8OxwaHDpcGhw6Yiy4YQfol0ufg4dDg0uHQ4NkjR79mzNnDlTknT22WfX3t62bVtde+21kTSwyEYSdAAAALdJREFUFl4NLufDocOhQWJfuHU4NLh0ODS4dCSi4YQfoh0ufi4dDg0uHQ4NknTTTTfppptu0rRp01RQUBDZ/R6KtfBqcDkfDh0ODRL7wq3DocGlw6HBpSMhDZG9DkiCff/73090QgjBo8OhIQSPDocGF6yFF5fz4dDh0ODCZS0cOhwaQvDocGgIwaMjyoakfLMVAAAA4HikJjoAAAAAaGoYogEAAIAYMUQDAAAAMWKIBgAAAGL0/wEHyAsSN8KVoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "server_df.transit_server_type.value_counts().nlargest(20).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIaLJnwdokJm"
   },
   "source": [
    "`Wrangling data for model building`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "cl6KlPfKokJn",
    "outputId": "599e08b8-97b7-4bcb-fa73-44e972b2af33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>transit_server_type</th>\n",
       "      <th>transit_server_type_1</th>\n",
       "      <th>transit_server_type_10</th>\n",
       "      <th>transit_server_type_11</th>\n",
       "      <th>transit_server_type_12</th>\n",
       "      <th>transit_server_type_13</th>\n",
       "      <th>transit_server_type_14</th>\n",
       "      <th>transit_server_type_15</th>\n",
       "      <th>transit_server_type_17</th>\n",
       "      <th>transit_server_type_18</th>\n",
       "      <th>transit_server_type_19</th>\n",
       "      <th>transit_server_type_2</th>\n",
       "      <th>transit_server_type_20</th>\n",
       "      <th>transit_server_type_21</th>\n",
       "      <th>transit_server_type_22</th>\n",
       "      <th>transit_server_type_23</th>\n",
       "      <th>transit_server_type_24</th>\n",
       "      <th>transit_server_type_25</th>\n",
       "      <th>transit_server_type_26</th>\n",
       "      <th>transit_server_type_27</th>\n",
       "      <th>transit_server_type_28</th>\n",
       "      <th>transit_server_type_29</th>\n",
       "      <th>transit_server_type_3</th>\n",
       "      <th>transit_server_type_30</th>\n",
       "      <th>transit_server_type_31</th>\n",
       "      <th>transit_server_type_32</th>\n",
       "      <th>transit_server_type_33</th>\n",
       "      <th>transit_server_type_34</th>\n",
       "      <th>transit_server_type_35</th>\n",
       "      <th>transit_server_type_36</th>\n",
       "      <th>transit_server_type_37</th>\n",
       "      <th>transit_server_type_38</th>\n",
       "      <th>transit_server_type_39</th>\n",
       "      <th>transit_server_type_4</th>\n",
       "      <th>transit_server_type_40</th>\n",
       "      <th>transit_server_type_41</th>\n",
       "      <th>transit_server_type_42</th>\n",
       "      <th>transit_server_type_43</th>\n",
       "      <th>transit_server_type_44</th>\n",
       "      <th>transit_server_type_45</th>\n",
       "      <th>transit_server_type_46</th>\n",
       "      <th>transit_server_type_47</th>\n",
       "      <th>transit_server_type_48</th>\n",
       "      <th>transit_server_type_49</th>\n",
       "      <th>transit_server_type_5</th>\n",
       "      <th>transit_server_type_50</th>\n",
       "      <th>transit_server_type_51</th>\n",
       "      <th>transit_server_type_52</th>\n",
       "      <th>transit_server_type_53</th>\n",
       "      <th>transit_server_type_54</th>\n",
       "      <th>transit_server_type_6</th>\n",
       "      <th>transit_server_type_7</th>\n",
       "      <th>transit_server_type_8</th>\n",
       "      <th>transit_server_type_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "transit_server_type  transit_server_type_1  ...  transit_server_type_9\n",
       "id                                          ...                       \n",
       "1                                        0  ...                      0\n",
       "2                                        0  ...                      0\n",
       "3                                        0  ...                      0\n",
       "4                                        0  ...                      0\n",
       "5                                        0  ...                      0\n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_df = server_df.pivot_table(values='transit_server_type',index='id',columns='transit_server_type',fill_value=0,aggfunc=len)\n",
    "server_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "g9VZnK3iokJt",
    "outputId": "05883e73-23a5-4e01-a999-450f5720e639"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transit_server_type_1</th>\n",
       "      <th>transit_server_type_10</th>\n",
       "      <th>transit_server_type_11</th>\n",
       "      <th>transit_server_type_12</th>\n",
       "      <th>transit_server_type_13</th>\n",
       "      <th>transit_server_type_14</th>\n",
       "      <th>transit_server_type_15</th>\n",
       "      <th>transit_server_type_17</th>\n",
       "      <th>transit_server_type_18</th>\n",
       "      <th>transit_server_type_19</th>\n",
       "      <th>transit_server_type_2</th>\n",
       "      <th>transit_server_type_20</th>\n",
       "      <th>transit_server_type_21</th>\n",
       "      <th>transit_server_type_22</th>\n",
       "      <th>transit_server_type_23</th>\n",
       "      <th>transit_server_type_24</th>\n",
       "      <th>transit_server_type_25</th>\n",
       "      <th>transit_server_type_26</th>\n",
       "      <th>transit_server_type_27</th>\n",
       "      <th>transit_server_type_28</th>\n",
       "      <th>transit_server_type_29</th>\n",
       "      <th>transit_server_type_3</th>\n",
       "      <th>transit_server_type_30</th>\n",
       "      <th>transit_server_type_31</th>\n",
       "      <th>transit_server_type_32</th>\n",
       "      <th>transit_server_type_33</th>\n",
       "      <th>transit_server_type_34</th>\n",
       "      <th>transit_server_type_35</th>\n",
       "      <th>transit_server_type_36</th>\n",
       "      <th>transit_server_type_37</th>\n",
       "      <th>transit_server_type_38</th>\n",
       "      <th>transit_server_type_39</th>\n",
       "      <th>transit_server_type_4</th>\n",
       "      <th>transit_server_type_40</th>\n",
       "      <th>transit_server_type_41</th>\n",
       "      <th>transit_server_type_42</th>\n",
       "      <th>transit_server_type_43</th>\n",
       "      <th>transit_server_type_44</th>\n",
       "      <th>transit_server_type_45</th>\n",
       "      <th>transit_server_type_46</th>\n",
       "      <th>transit_server_type_47</th>\n",
       "      <th>transit_server_type_48</th>\n",
       "      <th>transit_server_type_49</th>\n",
       "      <th>transit_server_type_5</th>\n",
       "      <th>transit_server_type_50</th>\n",
       "      <th>transit_server_type_51</th>\n",
       "      <th>transit_server_type_52</th>\n",
       "      <th>transit_server_type_53</th>\n",
       "      <th>transit_server_type_54</th>\n",
       "      <th>transit_server_type_6</th>\n",
       "      <th>transit_server_type_7</th>\n",
       "      <th>transit_server_type_8</th>\n",
       "      <th>transit_server_type_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    transit_server_type_1  ...  transit_server_type_9\n",
       "id                         ...                       \n",
       "1                       0  ...                      0\n",
       "2                       0  ...                      0\n",
       "3                       0  ...                      0\n",
       "4                       0  ...                      0\n",
       "5                       0  ...                      0\n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_df.columns = server_df.columns.categories\n",
    "server_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "8Vyj70m5okJ1",
    "outputId": "04b41fff-cdd5-42b7-cc3d-f5a814999703"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transit_server_type_1</th>\n",
       "      <th>transit_server_type_10</th>\n",
       "      <th>transit_server_type_11</th>\n",
       "      <th>transit_server_type_12</th>\n",
       "      <th>transit_server_type_13</th>\n",
       "      <th>transit_server_type_14</th>\n",
       "      <th>transit_server_type_15</th>\n",
       "      <th>transit_server_type_17</th>\n",
       "      <th>transit_server_type_18</th>\n",
       "      <th>transit_server_type_19</th>\n",
       "      <th>transit_server_type_2</th>\n",
       "      <th>transit_server_type_20</th>\n",
       "      <th>transit_server_type_21</th>\n",
       "      <th>transit_server_type_22</th>\n",
       "      <th>transit_server_type_23</th>\n",
       "      <th>transit_server_type_24</th>\n",
       "      <th>transit_server_type_25</th>\n",
       "      <th>transit_server_type_26</th>\n",
       "      <th>transit_server_type_27</th>\n",
       "      <th>transit_server_type_28</th>\n",
       "      <th>transit_server_type_29</th>\n",
       "      <th>transit_server_type_3</th>\n",
       "      <th>transit_server_type_30</th>\n",
       "      <th>transit_server_type_31</th>\n",
       "      <th>transit_server_type_32</th>\n",
       "      <th>transit_server_type_33</th>\n",
       "      <th>transit_server_type_34</th>\n",
       "      <th>transit_server_type_35</th>\n",
       "      <th>transit_server_type_36</th>\n",
       "      <th>transit_server_type_37</th>\n",
       "      <th>transit_server_type_38</th>\n",
       "      <th>transit_server_type_39</th>\n",
       "      <th>transit_server_type_4</th>\n",
       "      <th>transit_server_type_40</th>\n",
       "      <th>transit_server_type_41</th>\n",
       "      <th>transit_server_type_42</th>\n",
       "      <th>transit_server_type_43</th>\n",
       "      <th>transit_server_type_44</th>\n",
       "      <th>transit_server_type_45</th>\n",
       "      <th>transit_server_type_46</th>\n",
       "      <th>transit_server_type_47</th>\n",
       "      <th>transit_server_type_48</th>\n",
       "      <th>transit_server_type_49</th>\n",
       "      <th>transit_server_type_5</th>\n",
       "      <th>transit_server_type_50</th>\n",
       "      <th>transit_server_type_51</th>\n",
       "      <th>transit_server_type_52</th>\n",
       "      <th>transit_server_type_53</th>\n",
       "      <th>transit_server_type_54</th>\n",
       "      <th>transit_server_type_6</th>\n",
       "      <th>transit_server_type_7</th>\n",
       "      <th>transit_server_type_8</th>\n",
       "      <th>transit_server_type_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  transit_server_type_1  ...  transit_server_type_8  transit_server_type_9\n",
       "0  1                      0  ...                      0                      0\n",
       "1  2                      0  ...                      0                      0\n",
       "2  3                      0  ...                      0                      0\n",
       "3  4                      0  ...                      0                      0\n",
       "4  5                      0  ...                      0                      0\n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_df= server_df.reset_index()\n",
    "server_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LPOc3d08okJ4",
    "outputId": "2d25c649-0570-4cf5-b0d8-b68217d3a0ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any nulls\n",
    "server_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JZ3ZslbKokJ6",
    "outputId": "149678f2-2334-4af9-db4d-d0997e53c624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18552, 54)"
      ]
     },
     "execution_count": 153,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the dataset post processing\n",
    "server_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJxwchewokJ-"
   },
   "source": [
    "#### 3.5 Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ft506PJeokJ-",
    "outputId": "785eccc2-06db-446e-9909-1210442aaf00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>area_code</th>\n",
       "      <th>outage_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13366</td>\n",
       "      <td>area_415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6783</td>\n",
       "      <td>area_474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9519</td>\n",
       "      <td>area_931</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10202</td>\n",
       "      <td>area_700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4555</td>\n",
       "      <td>area_600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id area_code  outage_duration\n",
       "0  13366  area_415                1\n",
       "1   6783  area_474                0\n",
       "2   9519  area_931                1\n",
       "3  10202  area_700                1\n",
       "4   4555  area_600                2"
      ]
     },
     "execution_count": 154,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stU5br4DokKD",
    "outputId": "4ad361aa-e2ff-4384-e119-a86630b4c78e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5904, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the train data before processing\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "pkGhHwwGokKG",
    "outputId": "5977463a-0292-4b24-b150-83814b1706f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5904 entries, 0 to 5903\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               5904 non-null   int64 \n",
      " 1   area_code        5904 non-null   object\n",
      " 2   outage_duration  5904 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 138.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summary of the dataset\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vbtYrDGokKJ"
   },
   "outputs": [],
   "source": [
    "# Changing the type of the data\n",
    "train_df = train_df.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "k65LZtNOokKL",
    "outputId": "c1cc52e5-f409-4b79-f3d8-bdd534bd68ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5904 entries, 0 to 5903\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   id               5904 non-null   category\n",
      " 1   area_code        5904 non-null   category\n",
      " 2   outage_duration  5904 non-null   category\n",
      "dtypes: category(3)\n",
      "memory usage: 282.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Summary post change of the type\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "Hq0QoV8-okKN",
    "outputId": "b7abb10d-f250-4df2-be96-c700b4e7e13c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>area_code</th>\n",
       "      <th>outage_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5904</td>\n",
       "      <td>5904</td>\n",
       "      <td>5904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5904</td>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>18550</td>\n",
       "      <td>area_821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>3827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id area_code  outage_duration\n",
       "count    5904      5904             5904\n",
       "unique   5904       876                3\n",
       "top     18550  area_821                0\n",
       "freq        1        67             3827"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data description\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIyA5g0PokKP"
   },
   "source": [
    "From the description above we can infer that,\n",
    "1. Each row has a unique `id` and there are a total of `5904` ids\n",
    "2. There are a total `876` `aread_codes`\n",
    "3. `area_821` is the most frequently occuring with the frequency of `67`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxJGcMbwokKP"
   },
   "source": [
    "**Lets understand the distribution of the outage_duration with help of a pie-chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "Mt71cpy2okKQ",
    "outputId": "93a52c13-2d9e-44c7-e4d7-9bba9de84655"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.outage_duration.value_counts().plot(kind='pie',autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3g-efTOmokKS"
   },
   "source": [
    "There is clear class imbalance in the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6BqJc6aokKT"
   },
   "source": [
    "## 4. Answers to the operations team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GG1rH3w1okKT"
   },
   "source": [
    "#### 4.1 Which areas are most prone to long outage durations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6C1nMTNokKU"
   },
   "outputs": [],
   "source": [
    "# Filtering the area with outage duration is long, and getting the count of occurence\n",
    "long_outage_area_count = train_df.loc[train_df.outage_duration==2].area_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "GWmPGtw1okKX",
    "outputId": "bc8820f6-aade-43f2-f060-ef32534a505d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting top 20 areas most prone to long outage duration\n",
    "long_outage_area_count.nlargest(20).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3k9Kw3c-okKa"
   },
   "outputs": [],
   "source": [
    "# Getting the names of all the areas with outage_duration long in a list in decreasing order\n",
    "long_outage_areas = list(long_outage_area_count[long_outage_area_count>0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "K8RPy59qokKf",
    "outputId": "74165eb0-b3bb-4a4b-c3b6-0a161594791a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area_1100', 'area_1107', 'area_600', 'area_821', 'area_734', 'area_704', 'area_1019', 'area_810', 'area_834', 'area_684', 'area_798', 'area_995', 'area_962', 'area_1086', 'area_653', 'area_1008', 'area_846', 'area_763', 'area_613', 'area_845', 'area_638', 'area_1042', 'area_1052', 'area_895', 'area_1075', 'area_976', 'area_1061', 'area_921', 'area_599', 'area_892', 'area_745', 'area_984', 'area_931', 'area_866', 'area_830', 'area_779', 'area_641', 'area_644', 'area_906', 'area_885', 'area_909', 'area_884', 'area_1051', 'area_897', 'area_769', 'area_1089', 'area_864', 'area_844', 'area_13', 'area_744', 'area_726', 'area_705', 'area_7', 'area_619', 'area_674', 'area_667', 'area_646', 'area_648', 'area_1050', 'area_1', 'area_1020', 'area_925', 'area_926', 'area_1046', 'area_991', 'area_1024', 'area_839', 'area_724', 'area_1016', 'area_73', 'area_924', 'area_655', 'area_840', 'area_1109', 'area_1111', 'area_826', 'area_1103', 'area_601', 'area_1026', 'area_972', 'area_1017', 'area_774', 'area_603', 'area_853', 'area_856', 'area_1090', 'area_974', 'area_867', 'area_1112', 'area_738', 'area_33', 'area_957', 'area_551', 'area_778', 'area_1021', 'area_559', 'area_767', 'area_541', 'area_560', 'area_540', 'area_794', 'area_535', 'area_1119', 'area_565', 'area_566', 'area_57', 'area_756', 'area_32', 'area_956', 'area_967', 'area_816', 'area_87', 'area_1014', 'area_1098', 'area_555', 'area_695', 'area_632', 'area_899', 'area_894', 'area_893', 'area_998', 'area_481', 'area_1072', 'area_989', 'area_642', 'area_1034', 'area_645', 'area_643', 'area_937', 'area_1084', 'area_915', 'area_1011', 'area_618', 'area_65', 'area_1049']\n"
     ]
    }
   ],
   "source": [
    "# printing the above list\n",
    "print(long_outage_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ3o_ZgookKi"
   },
   "source": [
    "#### 4.2 Which broadband types are suspect of long outage durations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "QLF60NYrokKi",
    "outputId": "67e7d163-f14a-4b4e-8903-ac59e837f6ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADSL 1              544\n",
      "ADSL 2               82\n",
      "ADSL 2+              26\n",
      "Fiber 1              17\n",
      "Fiber Ultra          10\n",
      "Cable                 9\n",
      "Fiber 2               4\n",
      "BPL                   4\n",
      "Fiber Ultra Max       3\n",
      "Fiber High Speed      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtering long outage_duration, merging the train_data with broadband_df, getting the sum along axis=0 , sorting in descending\n",
    "broadband_types_long_outage = train_df.loc[train_df.outage_duration==2][['id']]\\\n",
    "                                .merge(broadband_df,on='id').sum().drop(index='id')\\\n",
    "                                    .sort_values(ascending=False)\n",
    "print(broadband_types_long_outage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fbrFLofokKk"
   },
   "source": [
    "As we can see the long outage duration has ADSL 1 as the most common type of broadband followed by other ADSL types. Lets visualize the same in a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "9z3t_GVyokKk",
    "outputId": "793f5e57-c397-4101-cef3-83e3d31bd260"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "broadband_types_long_outage.plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbSBaF-vokKm"
   },
   "source": [
    "#### 4.3 Any other recommendations to improve the detection of outage durations.\n",
    "1. As we saw some of the datasets had predictor variables of categorical type which huge number of categories, we can group data and reduce them, to improve our detection of outage durations\n",
    "2. From the pie chart we saw that the training data has some class imbalance, hence using techniques like smoting etc. can improve the detection of the outage duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQnpJKwkokKn"
   },
   "source": [
    "## 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s98Sox5mokKn"
   },
   "source": [
    "Lets try building the model starting least complex to most complex with validation and comparing the performance of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4_WmURSpokKo",
    "outputId": "b31528b1-61cb-45ff-ba15-81585a40539c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>area_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3340</td>\n",
       "      <td>area_344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14067</td>\n",
       "      <td>area_933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1134</td>\n",
       "      <td>area_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>area_793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9793</td>\n",
       "      <td>area_344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id area_code\n",
       "0   3340  area_344\n",
       "1  14067  area_933\n",
       "2   1134   area_16\n",
       "3     27  area_793\n",
       "4   9793  area_344"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data set\n",
    "test_df = pd.read_csv('data/test_data-1593798292529.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFKdNaeCokKs"
   },
   "outputs": [],
   "source": [
    "test_df.area_code = test_df.area_code.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kknD-tLdokKz"
   },
   "source": [
    "### 5.1 Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVMwz0dxokK0"
   },
   "source": [
    "#### 5.1.1 Merging datasets and splitting data into input and output(X and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQG56qnVokK0"
   },
   "outputs": [],
   "source": [
    "# merging train data with the other dataframes\n",
    "\n",
    "train_dfs = [train_df,broadband_df,outage_df,report_df,server_df]\n",
    "test_dfs = [test_df,broadband_df,outage_df,report_df,server_df]\n",
    "train_final = reduce(lambda left,right: pd.merge(left,right,on='id'), train_dfs)\n",
    "test_final =  reduce(lambda left,right: pd.merge(left,right,on='id'), test_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i7vZR6kuokK3",
    "outputId": "7cae106d-8c9a-4526-f6e5-21bce9608a94",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5904, 457), (1477, 456))"
      ]
     },
     "execution_count": 170,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final.shape,test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlyFbSLwokK6"
   },
   "outputs": [],
   "source": [
    "train_final.set_index('id',inplace=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_final.drop('outage_duration',axis=1), train_df.outage_duration, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = test_final.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "12iVRARwokK9",
    "outputId": "34633c1d-39b1-418f-a7c8-d0884706c7d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4723, 455), (4723,), (1181, 455), (1181,), (1477, 455))"
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7gOdwkNokLC"
   },
   "source": [
    "Hence our final train data has 5904 rows which is what we started with and has 455 predictor variables and test data has 1477 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSWb_T-LokLD"
   },
   "source": [
    "#### 5.1.2 Pipeling of pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPAwYvhnokLE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gO3JOXkjokLK"
   },
   "outputs": [],
   "source": [
    "num_attr = list(X_train.select_dtypes('number').columns)\n",
    "cat_attr = list(X_train.select_dtypes('category').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyBtX9vwokLM"
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore',))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attr),\n",
    "        ('cat', categorical_transformer, cat_attr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41U5u1D-rGQ6"
   },
   "source": [
    "### 5.2 Neural Network - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnwC84SLq0Rm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ewymsv2q0e4"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1330, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 10)\n",
    "        self.fc7 = nn.Linear(10, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened        \n",
    "        x = self.dropout(F.leaky_relu(self.fc1(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc2(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc3(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc4(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc5(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc6(x)))\n",
    "        x = F.log_softmax(self.fc7(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6n8AFsd4HBs"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBZ5AGe7q0pj"
   },
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ijkzGpEUDWFL",
    "outputId": "531777a1-784c-4f42-ff73-3fb968f94693"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=1330, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc6): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (fc7): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m36RhBiG4Oao"
   },
   "outputs": [],
   "source": [
    "X_train_np = preprocessor.fit_transform(train_final.drop('outage_duration',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iK4KBqCZ4Oev"
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train_tensor =  torch.Tensor(train_final.outage_duration.values).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g9N4Rg9Qq0zv",
    "outputId": "3935f36e-cc19-4b9d-fe97-e1c6ece6bd6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training loss: 0.09529862552881241\n",
      "Training loss: 0.09639932215213776\n",
      "Training loss: 0.09903501719236374\n",
      "Training loss: 0.10958708077669144\n",
      "Training loss: 0.09818337857723236\n",
      "Training loss: 0.09385041147470474\n",
      "Training loss: 0.09996059536933899\n",
      "Training loss: 0.10471389442682266\n",
      "Training loss: 0.10062582790851593\n",
      "Training loss: 0.1040707603096962\n",
      "Training loss: 0.09562818706035614\n",
      "Training loss: 0.09487742185592651\n",
      "Training loss: 0.09802520275115967\n",
      "Training loss: 0.09475748240947723\n",
      "Training loss: 0.10438619554042816\n",
      "Training loss: 0.09597305953502655\n",
      "Training loss: 0.10294214636087418\n",
      "Training loss: 0.10204189270734787\n",
      "Training loss: 0.1119057685136795\n",
      "Training loss: 0.10148140788078308\n",
      "Training loss: 0.1013086587190628\n",
      "Training loss: 0.09794578701257706\n",
      "Training loss: 0.09896228462457657\n",
      "Training loss: 0.1002935990691185\n",
      "Training loss: 0.10168478637933731\n",
      "Training loss: 0.09483490139245987\n",
      "Training loss: 0.10115958005189896\n",
      "Training loss: 0.09722196310758591\n",
      "Training loss: 0.09404218941926956\n",
      "Training loss: 0.10296474397182465\n",
      "Training loss: 0.0961933508515358\n",
      "Training loss: 0.1005658507347107\n",
      "Training loss: 0.0943659245967865\n",
      "Training loss: 0.09894619882106781\n",
      "Training loss: 0.10052158683538437\n",
      "Training loss: 0.09934397041797638\n",
      "Training loss: 0.09788219630718231\n",
      "Training loss: 0.09477954357862473\n",
      "Training loss: 0.09399238228797913\n",
      "Training loss: 0.09846377372741699\n",
      "Training loss: 0.09692308306694031\n",
      "Training loss: 0.09819536656141281\n",
      "Training loss: 0.0923846885561943\n",
      "Training loss: 0.09357796609401703\n",
      "Training loss: 0.09398535639047623\n",
      "Training loss: 0.10007960349321365\n",
      "Training loss: 0.09498191624879837\n",
      "Training loss: 0.09914049506187439\n",
      "Training loss: 0.10630320012569427\n",
      "Training loss: 0.10337062180042267\n",
      "Training loss: 0.09331618249416351\n",
      "Training loss: 0.09623800963163376\n",
      "Training loss: 0.09938115626573563\n",
      "Training loss: 0.10136643052101135\n",
      "Training loss: 0.09763899445533752\n",
      "Training loss: 0.09813982993364334\n",
      "Training loss: 0.09232375770807266\n",
      "Training loss: 0.10027648508548737\n",
      "Training loss: 0.09627413004636765\n",
      "Training loss: 0.10028623044490814\n",
      "Training loss: 0.0983019545674324\n",
      "Training loss: 0.10017803311347961\n",
      "Training loss: 0.10500454902648926\n",
      "Training loss: 0.09703408926725388\n",
      "Training loss: 0.09962495416402817\n",
      "Training loss: 0.09897293150424957\n",
      "Training loss: 0.10067025572061539\n",
      "Training loss: 0.10395903885364532\n",
      "Training loss: 0.10162390023469925\n",
      "Training loss: 0.09731189161539078\n",
      "Training loss: 0.09442781656980515\n",
      "Training loss: 0.09757624566555023\n",
      "Training loss: 0.09346403181552887\n",
      "Training loss: 0.09981545060873032\n",
      "Training loss: 0.09495075047016144\n",
      "Training loss: 0.09494152665138245\n",
      "Training loss: 0.093240886926651\n",
      "Training loss: 0.09211087971925735\n",
      "Training loss: 0.09911197423934937\n",
      "Training loss: 0.10229557007551193\n",
      "Training loss: 0.09552949666976929\n",
      "Training loss: 0.09532593190670013\n",
      "Training loss: 0.0980721116065979\n",
      "Training loss: 0.09587940573692322\n",
      "Training loss: 0.09036385267972946\n",
      "Training loss: 0.09821565449237823\n",
      "Training loss: 0.09468064457178116\n",
      "Training loss: 0.10355507582426071\n",
      "Training loss: 0.09658664464950562\n",
      "Training loss: 0.09734021872282028\n",
      "Training loss: 0.09345578402280807\n",
      "Training loss: 0.09916208684444427\n",
      "Training loss: 0.10444629937410355\n",
      "Training loss: 0.08687470853328705\n",
      "Training loss: 0.09423286467790604\n",
      "Training loss: 0.1014898419380188\n",
      "Training loss: 0.09743070602416992\n",
      "Training loss: 0.0990198403596878\n",
      "Training loss: 0.09818025678396225\n",
      "Training loss: 0.09508066624403\n",
      "Training loss: 0.09677079319953918\n",
      "Training loss: 0.09443438053131104\n",
      "Training loss: 0.09776873886585236\n",
      "Training loss: 0.09731356799602509\n",
      "Training loss: 0.09420142322778702\n",
      "Training loss: 0.0965614765882492\n",
      "Training loss: 0.09428778290748596\n",
      "Training loss: 0.10010542720556259\n",
      "Training loss: 0.09302093833684921\n",
      "Training loss: 0.09882502257823944\n",
      "Training loss: 0.10461921989917755\n",
      "Training loss: 0.09961701184511185\n",
      "Training loss: 0.09863647073507309\n",
      "Training loss: 0.09954892843961716\n",
      "Training loss: 0.09632235020399094\n",
      "Training loss: 0.09401953965425491\n",
      "Training loss: 0.09584902226924896\n",
      "Training loss: 0.09497737139463425\n",
      "Training loss: 0.09701886773109436\n",
      "Training loss: 0.09868794679641724\n",
      "Training loss: 0.09156689047813416\n",
      "Training loss: 0.09659536182880402\n",
      "Training loss: 0.10012459009885788\n",
      "Training loss: 0.09460289776325226\n",
      "Training loss: 0.0952848494052887\n",
      "Training loss: 0.09941881895065308\n",
      "Training loss: 0.09333343058824539\n",
      "Training loss: 0.09177953004837036\n",
      "Training loss: 0.09127012640237808\n",
      "Training loss: 0.09042337536811829\n",
      "Training loss: 0.10036330670118332\n",
      "Training loss: 0.0916118174791336\n",
      "Training loss: 0.08935625106096268\n",
      "Training loss: 0.09341755509376526\n",
      "Training loss: 0.094013512134552\n",
      "Training loss: 0.09523114562034607\n",
      "Training loss: 0.09245061129331589\n",
      "Training loss: 0.09359783679246902\n",
      "Training loss: 0.10002091526985168\n",
      "Training loss: 0.09642437100410461\n",
      "Training loss: 0.09422379732131958\n",
      "Training loss: 0.0912005826830864\n",
      "Training loss: 0.10111944377422333\n",
      "Training loss: 0.0939057320356369\n",
      "Training loss: 0.0987405925989151\n",
      "Training loss: 0.09126508235931396\n",
      "Training loss: 0.09493006765842438\n",
      "Training loss: 0.10047531127929688\n",
      "Training loss: 0.09122437238693237\n",
      "Training loss: 0.09522239118814468\n",
      "Training loss: 0.09442637115716934\n",
      "Training loss: 0.09415093809366226\n",
      "Training loss: 0.09567183256149292\n",
      "Training loss: 0.09358716011047363\n",
      "Training loss: 0.09066759794950485\n",
      "Training loss: 0.09920559823513031\n",
      "Training loss: 0.09183283895254135\n",
      "Training loss: 0.09595483541488647\n",
      "Training loss: 0.09337295591831207\n",
      "Training loss: 0.09532581269741058\n",
      "Training loss: 0.09067978709936142\n",
      "Training loss: 0.0985618457198143\n",
      "Training loss: 0.09556737542152405\n",
      "Training loss: 0.09334582835435867\n",
      "Training loss: 0.09708070755004883\n",
      "Training loss: 0.09343049675226212\n",
      "Training loss: 0.09533702582120895\n",
      "Training loss: 0.09303632378578186\n",
      "Training loss: 0.09726064652204514\n",
      "Training loss: 0.10014313459396362\n",
      "Training loss: 0.09311410039663315\n",
      "Training loss: 0.10039311647415161\n",
      "Training loss: 0.10196549445390701\n",
      "Training loss: 0.10177449882030487\n",
      "Training loss: 0.10079606622457504\n",
      "Training loss: 0.09842388331890106\n",
      "Training loss: 0.09296699613332748\n",
      "Training loss: 0.0986940935254097\n",
      "Training loss: 0.1040981337428093\n",
      "Training loss: 0.09431225806474686\n",
      "Training loss: 0.09986819326877594\n",
      "Training loss: 0.09802878648042679\n",
      "Training loss: 0.10069048404693604\n",
      "Training loss: 0.09731884300708771\n",
      "Training loss: 0.10105245560407639\n",
      "Training loss: 0.09297385066747665\n",
      "Training loss: 0.09551955759525299\n",
      "Training loss: 0.10307925939559937\n",
      "Training loss: 0.09947159141302109\n",
      "Training loss: 0.09854990988969803\n",
      "Training loss: 0.10063384473323822\n",
      "Training loss: 0.09436187148094177\n",
      "Training loss: 0.09736380726099014\n",
      "Training loss: 0.09069803357124329\n",
      "Training loss: 0.09890883415937424\n",
      "Training loss: 0.09429145604372025\n",
      "Training loss: 0.09193522483110428\n",
      "Training loss: 0.10287027060985565\n",
      "Training loss: 0.09579862654209137\n",
      "Training loss: 0.09431961178779602\n",
      "Training loss: 0.0933884009718895\n",
      "Training loss: 0.09650229662656784\n",
      "Training loss: 0.09575410187244415\n",
      "Training loss: 0.09808336198329926\n",
      "Training loss: 0.09173412621021271\n",
      "Training loss: 0.09005921334028244\n",
      "Training loss: 0.09017672389745712\n",
      "Training loss: 0.10112380981445312\n",
      "Training loss: 0.09500960260629654\n",
      "Training loss: 0.10284650325775146\n",
      "Training loss: 0.09926709532737732\n",
      "Training loss: 0.09008313715457916\n",
      "Training loss: 0.09752324223518372\n",
      "Training loss: 0.0947943702340126\n",
      "Training loss: 0.11231911182403564\n",
      "Training loss: 0.09416095167398453\n",
      "Training loss: 0.09540528804063797\n",
      "Training loss: 0.09106835722923279\n",
      "Training loss: 0.09558548778295517\n",
      "Training loss: 0.09504646807909012\n",
      "Training loss: 0.09443304687738419\n",
      "Training loss: 0.10066598653793335\n",
      "Training loss: 0.0932321846485138\n",
      "Training loss: 0.09381981194019318\n",
      "Training loss: 0.10043992102146149\n",
      "Training loss: 0.09503442794084549\n",
      "Training loss: 0.09565911442041397\n",
      "Training loss: 0.09479504823684692\n",
      "Training loss: 0.09908874332904816\n",
      "Training loss: 0.09533297270536423\n",
      "Training loss: 0.09797260910272598\n",
      "Training loss: 0.09427714347839355\n",
      "Training loss: 0.09461385011672974\n",
      "Training loss: 0.09230265021324158\n",
      "Training loss: 0.10023357719182968\n",
      "Training loss: 0.09850428253412247\n",
      "Training loss: 0.10353877395391464\n",
      "Training loss: 0.10172630101442337\n",
      "Training loss: 0.09388308972120285\n",
      "Training loss: 0.09355892241001129\n",
      "Training loss: 0.0955643281340599\n",
      "Training loss: 0.10093424469232559\n",
      "Training loss: 0.09947087615728378\n",
      "Training loss: 0.0880756601691246\n",
      "Training loss: 0.09586168080568314\n",
      "Training loss: 0.09332404285669327\n",
      "Training loss: 0.10034476220607758\n",
      "Training loss: 0.10198551416397095\n",
      "Training loss: 0.1044226735830307\n",
      "Training loss: 0.10280051082372665\n",
      "Training loss: 0.09187538176774979\n",
      "Training loss: 0.0999845415353775\n",
      "Training loss: 0.10166150331497192\n",
      "Training loss: 0.09911781549453735\n",
      "Training loss: 0.09584537148475647\n",
      "Training loss: 0.0999668687582016\n",
      "Training loss: 0.10445686429738998\n",
      "Training loss: 0.09403704106807709\n",
      "Training loss: 0.09212964028120041\n",
      "Training loss: 0.09519633650779724\n",
      "Training loss: 0.10412874817848206\n",
      "Training loss: 0.1001458540558815\n",
      "Training loss: 0.10274709016084671\n",
      "Training loss: 0.09737817198038101\n",
      "Training loss: 0.09623155742883682\n",
      "Training loss: 0.0949525311589241\n",
      "Training loss: 0.0967705175280571\n",
      "Training loss: 0.09779045730829239\n",
      "Training loss: 0.09505616873502731\n",
      "Training loss: 0.09235988557338715\n",
      "Training loss: 0.10249881446361542\n",
      "Training loss: 0.09382748603820801\n",
      "Training loss: 0.09892380237579346\n",
      "Training loss: 0.09675294160842896\n",
      "Training loss: 0.09410127252340317\n",
      "Training loss: 0.09491139650344849\n",
      "Training loss: 0.10068588703870773\n",
      "Training loss: 0.10000055283308029\n",
      "Training loss: 0.10134455561637878\n",
      "Training loss: 0.10175201296806335\n",
      "Training loss: 0.09892967343330383\n",
      "Training loss: 0.09659987688064575\n",
      "Training loss: 0.08919202536344528\n",
      "Training loss: 0.09336074441671371\n",
      "Training loss: 0.10608798265457153\n",
      "Training loss: 0.09652034193277359\n",
      "Training loss: 0.09229899197816849\n",
      "Training loss: 0.09048029780387878\n",
      "Training loss: 0.08806288242340088\n",
      "Training loss: 0.09090252965688705\n",
      "Training loss: 0.0946328267455101\n",
      "Training loss: 0.09122845530509949\n",
      "Training loss: 0.10062097012996674\n",
      "Training loss: 0.09348779916763306\n",
      "Training loss: 0.09074301272630692\n",
      "Training loss: 0.091039277613163\n",
      "Training loss: 0.08791130781173706\n",
      "Training loss: 0.09167493134737015\n",
      "Training loss: 0.09701207280158997\n",
      "Training loss: 0.10437510162591934\n",
      "Training loss: 0.10565289109945297\n",
      "Training loss: 0.091287761926651\n",
      "Training loss: 0.09736558794975281\n",
      "Training loss: 0.08737150579690933\n",
      "Training loss: 0.09395458549261093\n",
      "Training loss: 0.09745089709758759\n",
      "Training loss: 0.09653154760599136\n",
      "Training loss: 0.09903977811336517\n",
      "Training loss: 0.09721535444259644\n",
      "Training loss: 0.09779931604862213\n",
      "Training loss: 0.09676504880189896\n",
      "Training loss: 0.09468869119882584\n",
      "Training loss: 0.0913766622543335\n",
      "Training loss: 0.09614714980125427\n",
      "Training loss: 0.09307146072387695\n",
      "Training loss: 0.09961545467376709\n",
      "Training loss: 0.10081502795219421\n",
      "Training loss: 0.09080615639686584\n",
      "Training loss: 0.09282679855823517\n",
      "Training loss: 0.09122290462255478\n",
      "Training loss: 0.09288380295038223\n",
      "Training loss: 0.09245616942644119\n",
      "Training loss: 0.09719277918338776\n",
      "Training loss: 0.09668520838022232\n",
      "Training loss: 0.09302694350481033\n",
      "Training loss: 0.0955287367105484\n",
      "Training loss: 0.09795907139778137\n",
      "Training loss: 0.08786368370056152\n",
      "Training loss: 0.09962736070156097\n",
      "Training loss: 0.09480928629636765\n",
      "Training loss: 0.09663974493741989\n",
      "Training loss: 0.09203888475894928\n",
      "Training loss: 0.09607859700918198\n",
      "Training loss: 0.0977921187877655\n",
      "Training loss: 0.09109827131032944\n",
      "Training loss: 0.09378962218761444\n",
      "Training loss: 0.09523335844278336\n",
      "Training loss: 0.0861622542142868\n",
      "Training loss: 0.09204158186912537\n",
      "Training loss: 0.09203397482633591\n",
      "Training loss: 0.0924828052520752\n",
      "Training loss: 0.0974169373512268\n",
      "Training loss: 0.0960959941148758\n",
      "Training loss: 0.0914861187338829\n",
      "Training loss: 0.09145357459783554\n",
      "Training loss: 0.09646381437778473\n",
      "Training loss: 0.09297049790620804\n",
      "Training loss: 0.09249485284090042\n",
      "Training loss: 0.09402009099721909\n",
      "Training loss: 0.09906837344169617\n",
      "Training loss: 0.1002555787563324\n",
      "Training loss: 0.09622383862733841\n",
      "Training loss: 0.09181130677461624\n",
      "Training loss: 0.09452559053897858\n",
      "Training loss: 0.09791398793458939\n",
      "Training loss: 0.09425399452447891\n",
      "Training loss: 0.08802977204322815\n",
      "Training loss: 0.0911015123128891\n",
      "Training loss: 0.09394016861915588\n",
      "Training loss: 0.09251176565885544\n",
      "Training loss: 0.09686306864023209\n",
      "Training loss: 0.09785573929548264\n",
      "Training loss: 0.09254489839076996\n",
      "Training loss: 0.09496331214904785\n",
      "Training loss: 0.09887249767780304\n",
      "Training loss: 0.09375013411045074\n",
      "Training loss: 0.09821484237909317\n",
      "Training loss: 0.09111955761909485\n",
      "Training loss: 0.09053795039653778\n",
      "Training loss: 0.100910484790802\n",
      "Training loss: 0.10157109051942825\n",
      "Training loss: 0.09965016692876816\n",
      "Training loss: 0.0950155258178711\n",
      "Training loss: 0.09835878759622574\n",
      "Training loss: 0.09644565731287003\n",
      "Training loss: 0.09324558079242706\n",
      "Training loss: 0.1011844202876091\n",
      "Training loss: 0.09270786494016647\n",
      "Training loss: 0.09597860276699066\n",
      "Training loss: 0.10232967883348465\n",
      "Training loss: 0.09788248687982559\n",
      "Training loss: 0.09307539463043213\n",
      "Training loss: 0.0932006761431694\n",
      "Training loss: 0.09111196547746658\n",
      "Training loss: 0.08983588218688965\n",
      "Training loss: 0.09525980055332184\n",
      "Training loss: 0.09046002477407455\n",
      "Training loss: 0.09159643203020096\n",
      "Training loss: 0.09447186440229416\n",
      "Training loss: 0.08932705223560333\n",
      "Training loss: 0.09195053577423096\n",
      "Training loss: 0.09150140732526779\n",
      "Training loss: 0.0999818816781044\n",
      "Training loss: 0.09277120232582092\n",
      "Training loss: 0.09270650893449783\n",
      "Training loss: 0.09480242431163788\n",
      "Training loss: 0.09583351016044617\n",
      "Training loss: 0.09534665942192078\n",
      "Training loss: 0.09848914295434952\n",
      "Training loss: 0.09210997074842453\n",
      "Training loss: 0.08889391273260117\n",
      "Training loss: 0.09881240129470825\n",
      "Training loss: 0.0889175608754158\n",
      "Training loss: 0.09470537304878235\n",
      "Training loss: 0.10304058343172073\n",
      "Training loss: 0.09514015913009644\n",
      "Training loss: 0.09693652391433716\n",
      "Training loss: 0.1010606586933136\n",
      "Training loss: 0.09916550666093826\n",
      "Training loss: 0.09291251003742218\n",
      "Training loss: 0.10167792439460754\n",
      "Training loss: 0.09364824742078781\n",
      "Training loss: 0.10427971184253693\n",
      "Training loss: 0.09894412010908127\n",
      "Training loss: 0.09177228063344955\n",
      "Training loss: 0.10209408402442932\n",
      "Training loss: 0.09720678627490997\n",
      "Training loss: 0.09146532416343689\n",
      "Training loss: 0.09764248877763748\n",
      "Training loss: 0.10052184015512466\n",
      "Training loss: 0.09660273045301437\n",
      "Training loss: 0.09521092474460602\n",
      "Training loss: 0.09772995859384537\n",
      "Training loss: 0.09446147829294205\n",
      "Training loss: 0.10358797758817673\n",
      "Training loss: 0.09367834776639938\n",
      "Training loss: 0.09523905813694\n",
      "Training loss: 0.0963752493262291\n",
      "Training loss: 0.10485944151878357\n",
      "Training loss: 0.09442077577114105\n",
      "Training loss: 0.09370561689138412\n",
      "Training loss: 0.09152848273515701\n",
      "Training loss: 0.0907566025853157\n",
      "Training loss: 0.0995512455701828\n",
      "Training loss: 0.09508999437093735\n",
      "Training loss: 0.09591343253850937\n",
      "Training loss: 0.09481851756572723\n",
      "Training loss: 0.1003108024597168\n",
      "Training loss: 0.10419383645057678\n",
      "Training loss: 0.09677761048078537\n",
      "Training loss: 0.1058635339140892\n",
      "Training loss: 0.09850850701332092\n",
      "Training loss: 0.09330353885889053\n",
      "Training loss: 0.09863103181123734\n",
      "Training loss: 0.09020831435918808\n",
      "Training loss: 0.08848817646503448\n",
      "Training loss: 0.09986493736505508\n",
      "Training loss: 0.09413934499025345\n",
      "Training loss: 0.09839242696762085\n",
      "Training loss: 0.09546513855457306\n",
      "Training loss: 0.093857042491436\n",
      "Training loss: 0.08826857060194016\n",
      "Training loss: 0.08897052705287933\n",
      "Training loss: 0.09692032635211945\n",
      "Training loss: 0.0913248360157013\n",
      "Training loss: 0.09600453823804855\n",
      "Training loss: 0.09316929429769516\n",
      "Training loss: 0.08917031437158585\n",
      "Training loss: 0.09256920218467712\n",
      "Training loss: 0.09385710209608078\n",
      "Training loss: 0.09907086938619614\n",
      "Training loss: 0.09422832727432251\n",
      "Training loss: 0.09731313586235046\n",
      "Training loss: 0.08773723244667053\n",
      "Training loss: 0.09630551934242249\n",
      "Training loss: 0.08983654528856277\n",
      "Training loss: 0.09823590517044067\n",
      "Training loss: 0.08582460880279541\n",
      "Training loss: 0.09865961968898773\n",
      "Training loss: 0.09157031029462814\n",
      "Training loss: 0.09720472991466522\n",
      "Training loss: 0.08852146565914154\n",
      "Training loss: 0.0926671102643013\n",
      "Training loss: 0.09104476124048233\n",
      "Training loss: 0.09100715816020966\n",
      "Training loss: 0.09225544333457947\n",
      "Training loss: 0.09357991814613342\n",
      "Training loss: 0.09700710326433182\n",
      "Training loss: 0.09327433258295059\n",
      "Training loss: 0.08796678483486176\n",
      "Training loss: 0.08708367496728897\n",
      "Training loss: 0.0977909192442894\n",
      "Training loss: 0.09481142461299896\n",
      "Training loss: 0.09661704301834106\n",
      "Training loss: 0.09183339774608612\n",
      "Training loss: 0.10288077592849731\n",
      "Training loss: 0.0919579491019249\n",
      "Training loss: 0.09976206719875336\n",
      "Training loss: 0.09317035228013992\n",
      "Training loss: 0.09059935063123703\n",
      "Training loss: 0.09866131097078323\n",
      "Training loss: 0.0982561782002449\n",
      "Training loss: 0.09481067210435867\n",
      "Training loss: 0.09095743298530579\n",
      "Training loss: 0.09310076385736465\n",
      "Training loss: 0.09484948962926865\n",
      "Training loss: 0.0972956120967865\n",
      "Training loss: 0.09634524583816528\n",
      "Training loss: 0.09385750442743301\n",
      "Training loss: 0.0877070426940918\n",
      "Training loss: 0.08926789462566376\n",
      "Training loss: 0.09337882697582245\n",
      "Training loss: 0.09440597146749496\n",
      "Training loss: 0.08921811729669571\n",
      "Training loss: 0.09609600901603699\n",
      "Training loss: 0.08882920444011688\n",
      "Training loss: 0.08887951821088791\n",
      "Training loss: 0.08826470375061035\n",
      "Training loss: 0.0871734693646431\n",
      "Training loss: 0.09231047332286835\n",
      "Training loss: 0.09355529397726059\n",
      "Training loss: 0.08611933141946793\n",
      "Training loss: 0.09056426584720612\n",
      "Training loss: 0.0885448306798935\n",
      "Training loss: 0.09287206083536148\n",
      "Training loss: 0.09008549153804779\n",
      "Training loss: 0.09327838569879532\n",
      "Training loss: 0.08932217210531235\n",
      "Training loss: 0.09811103343963623\n",
      "Training loss: 0.10202734172344208\n",
      "Training loss: 0.09016349911689758\n",
      "Training loss: 0.09199022501707077\n",
      "Training loss: 0.08870521187782288\n",
      "Training loss: 0.09367154538631439\n",
      "Training loss: 0.09188079088926315\n",
      "Training loss: 0.08695963025093079\n",
      "Training loss: 0.09238878637552261\n",
      "Training loss: 0.09643317759037018\n",
      "Training loss: 0.0907524824142456\n",
      "Training loss: 0.09173166006803513\n",
      "Training loss: 0.09448081254959106\n",
      "Training loss: 0.09735020250082016\n",
      "Training loss: 0.0948498547077179\n",
      "Training loss: 0.09291739016771317\n",
      "Training loss: 0.10742034763097763\n",
      "Training loss: 0.09785892814397812\n",
      "Training loss: 0.09822814166545868\n",
      "Training loss: 0.09732768684625626\n",
      "Training loss: 0.09603862464427948\n",
      "Training loss: 0.09571213275194168\n",
      "Training loss: 0.09242817014455795\n",
      "Training loss: 0.0935315415263176\n",
      "Training loss: 0.09317886829376221\n",
      "Training loss: 0.0945485457777977\n",
      "Training loss: 0.09114308655261993\n",
      "Training loss: 0.09331724047660828\n",
      "Training loss: 0.09524087607860565\n",
      "Training loss: 0.09588439762592316\n",
      "Training loss: 0.09142317622900009\n",
      "Training loss: 0.09138695895671844\n",
      "Training loss: 0.10366284847259521\n",
      "Training loss: 0.09639755636453629\n",
      "Training loss: 0.09591759741306305\n",
      "Training loss: 0.09098290652036667\n",
      "Training loss: 0.10275571793317795\n",
      "Training loss: 0.09710174053907394\n",
      "Training loss: 0.0914701446890831\n",
      "Training loss: 0.09355464577674866\n",
      "Training loss: 0.09724673628807068\n",
      "Training loss: 0.09522425383329391\n",
      "Training loss: 0.09383806586265564\n",
      "Training loss: 0.09213617444038391\n",
      "Training loss: 0.09022153913974762\n",
      "Training loss: 0.1027355045080185\n",
      "Training loss: 0.08779864758253098\n",
      "Training loss: 0.09332264214754105\n",
      "Training loss: 0.0923767015337944\n",
      "Training loss: 0.09556575119495392\n",
      "Training loss: 0.09659576416015625\n",
      "Training loss: 0.09667418897151947\n",
      "Training loss: 0.10440800338983536\n",
      "Training loss: 0.09327524155378342\n",
      "Training loss: 0.0946674793958664\n",
      "Training loss: 0.09485284239053726\n",
      "Training loss: 0.09017576277256012\n",
      "Training loss: 0.09698215126991272\n",
      "Training loss: 0.09489772468805313\n",
      "Training loss: 0.10336893051862717\n",
      "Training loss: 0.08594963699579239\n",
      "Training loss: 0.09433453530073166\n",
      "Training loss: 0.09043672680854797\n",
      "Training loss: 0.09410320222377777\n",
      "Training loss: 0.0966542661190033\n",
      "Training loss: 0.08828800171613693\n",
      "Training loss: 0.10579316318035126\n",
      "Training loss: 0.090583935379982\n",
      "Training loss: 0.0932902842760086\n",
      "Training loss: 0.09392864257097244\n",
      "Training loss: 0.08742076903581619\n",
      "Training loss: 0.09972124546766281\n",
      "Training loss: 0.08843491971492767\n",
      "Training loss: 0.09566479921340942\n",
      "Training loss: 0.09537868946790695\n",
      "Training loss: 0.09427477419376373\n",
      "Training loss: 0.09428445994853973\n",
      "Training loss: 0.0898892804980278\n",
      "Training loss: 0.09340379387140274\n",
      "Training loss: 0.08990078419446945\n",
      "Training loss: 0.09168734401464462\n",
      "Training loss: 0.09360828995704651\n",
      "Training loss: 0.08855137228965759\n",
      "Training loss: 0.09291653335094452\n",
      "Training loss: 0.09179817885160446\n",
      "Training loss: 0.09338117390871048\n",
      "Training loss: 0.1005677804350853\n",
      "Training loss: 0.09296504408121109\n",
      "Training loss: 0.08535002917051315\n",
      "Training loss: 0.08892684429883957\n",
      "Training loss: 0.09727529436349869\n",
      "Training loss: 0.09366147965192795\n",
      "Training loss: 0.08887084573507309\n",
      "Training loss: 0.09046132862567902\n",
      "Training loss: 0.09017567336559296\n",
      "Training loss: 0.09457560628652573\n",
      "Training loss: 0.09949896484613419\n",
      "Training loss: 0.09319392591714859\n",
      "Training loss: 0.09407006204128265\n",
      "Training loss: 0.09719496220350266\n",
      "Training loss: 0.09091101586818695\n",
      "Training loss: 0.0896013155579567\n",
      "Training loss: 0.08800598233938217\n",
      "Training loss: 0.09249216318130493\n",
      "Training loss: 0.09088314324617386\n",
      "Training loss: 0.0979170873761177\n",
      "Training loss: 0.09350116550922394\n",
      "Training loss: 0.08857157081365585\n",
      "Training loss: 0.09185443818569183\n",
      "Training loss: 0.0903589278459549\n",
      "Training loss: 0.09010805189609528\n",
      "Training loss: 0.09190995246171951\n",
      "Training loss: 0.08635929971933365\n",
      "Training loss: 0.09683734178543091\n",
      "Training loss: 0.09148631244897842\n",
      "Training loss: 0.10575839877128601\n",
      "Training loss: 0.09574848413467407\n",
      "Training loss: 0.09007920324802399\n",
      "Training loss: 0.09997990727424622\n",
      "Training loss: 0.0902552604675293\n",
      "Training loss: 0.0873810350894928\n",
      "Training loss: 0.0898115485906601\n",
      "Training loss: 0.09653181582689285\n",
      "Training loss: 0.0920867919921875\n",
      "Training loss: 0.09801674634218216\n",
      "Training loss: 0.09362353384494781\n",
      "Training loss: 0.0945819765329361\n",
      "Training loss: 0.09930544346570969\n",
      "Training loss: 0.09330173581838608\n",
      "Training loss: 0.09642785787582397\n",
      "Training loss: 0.0906822606921196\n",
      "Training loss: 0.09735921770334244\n",
      "Training loss: 0.09447815269231796\n",
      "Training loss: 0.09610487520694733\n",
      "Training loss: 0.09078668057918549\n",
      "Training loss: 0.09320343285799026\n",
      "Training loss: 0.0899374932050705\n",
      "Training loss: 0.0899946391582489\n",
      "Training loss: 0.09362870454788208\n",
      "Training loss: 0.09133906662464142\n",
      "Training loss: 0.08706987649202347\n",
      "Training loss: 0.09594246000051498\n",
      "Training loss: 0.09334038943052292\n",
      "Training loss: 0.08733784407377243\n",
      "Training loss: 0.09005033224821091\n",
      "Training loss: 0.09230251610279083\n",
      "Training loss: 0.09217562526464462\n",
      "Training loss: 0.09143856167793274\n",
      "Training loss: 0.0904955118894577\n",
      "Training loss: 0.09721430391073227\n",
      "Training loss: 0.09185276925563812\n",
      "Training loss: 0.09154118597507477\n",
      "Training loss: 0.09494542330503464\n",
      "Training loss: 0.0901889055967331\n",
      "Training loss: 0.08471816033124924\n",
      "Training loss: 0.09358757734298706\n",
      "Training loss: 0.08712835609912872\n",
      "Training loss: 0.09605444222688675\n",
      "Training loss: 0.09371037781238556\n",
      "Training loss: 0.09191332012414932\n",
      "Training loss: 0.09032221883535385\n",
      "Training loss: 0.09007324278354645\n",
      "Training loss: 0.09000783413648605\n",
      "Training loss: 0.09531795233488083\n",
      "Training loss: 0.09004537761211395\n",
      "Training loss: 0.09225887060165405\n",
      "Training loss: 0.0936925858259201\n",
      "Training loss: 0.09224682301282883\n",
      "Training loss: 0.10019904375076294\n",
      "Training loss: 0.0987388864159584\n",
      "Training loss: 0.10221990197896957\n",
      "Training loss: 0.09256678074598312\n",
      "Training loss: 0.092000812292099\n",
      "Training loss: 0.09586357325315475\n",
      "Training loss: 0.09366396069526672\n",
      "Training loss: 0.09641990810632706\n",
      "Training loss: 0.09319999814033508\n",
      "Training loss: 0.09853801131248474\n",
      "Training loss: 0.09540536254644394\n",
      "Training loss: 0.09519881755113602\n",
      "Training loss: 0.09383183717727661\n",
      "Training loss: 0.09723367542028427\n",
      "Training loss: 0.08817251771688461\n",
      "Training loss: 0.0908556878566742\n",
      "Training loss: 0.08616957813501358\n",
      "Training loss: 0.0891328826546669\n",
      "Training loss: 0.09706006199121475\n",
      "Training loss: 0.10353182256221771\n",
      "Training loss: 0.09866652637720108\n",
      "Training loss: 0.09519040584564209\n",
      "Training loss: 0.09425343573093414\n",
      "Training loss: 0.09625644981861115\n",
      "Training loss: 0.08940226584672928\n",
      "Training loss: 0.08764594793319702\n",
      "Training loss: 0.09322389960289001\n",
      "Training loss: 0.10126742720603943\n",
      "Training loss: 0.10306520015001297\n",
      "Training loss: 0.09417688101530075\n",
      "Training loss: 0.09458968043327332\n",
      "Training loss: 0.09648991376161575\n",
      "Training loss: 0.09039746969938278\n",
      "Training loss: 0.08880513161420822\n",
      "Training loss: 0.08854071795940399\n",
      "Training loss: 0.0941421389579773\n",
      "Training loss: 0.09018243849277496\n",
      "Training loss: 0.09291618317365646\n",
      "Training loss: 0.09164799749851227\n",
      "Training loss: 0.09413627535104752\n",
      "Training loss: 0.09438150376081467\n",
      "Training loss: 0.08874796330928802\n",
      "Training loss: 0.09442275017499924\n",
      "Training loss: 0.09148239344358444\n",
      "Training loss: 0.08899960666894913\n",
      "Training loss: 0.08899881690740585\n",
      "Training loss: 0.08939337730407715\n",
      "Training loss: 0.09407822787761688\n",
      "Training loss: 0.09412065893411636\n",
      "Training loss: 0.08637518435716629\n",
      "Training loss: 0.09716229140758514\n",
      "Training loss: 0.0902983620762825\n",
      "Training loss: 0.0864407867193222\n",
      "Training loss: 0.08959048241376877\n",
      "Training loss: 0.08851342648267746\n",
      "Training loss: 0.09644007682800293\n",
      "Training loss: 0.08437410742044449\n",
      "Training loss: 0.09155184775590897\n",
      "Training loss: 0.09133177250623703\n",
      "Training loss: 0.09499847888946533\n",
      "Training loss: 0.0889098197221756\n",
      "Training loss: 0.08796587586402893\n",
      "Training loss: 0.0889950543642044\n",
      "Training loss: 0.08503831923007965\n",
      "Training loss: 0.09144128113985062\n",
      "Training loss: 0.0921069011092186\n",
      "Training loss: 0.09748850762844086\n",
      "Training loss: 0.09718409925699234\n",
      "Training loss: 0.0971108227968216\n",
      "Training loss: 0.089023157954216\n",
      "Training loss: 0.08871064335107803\n",
      "Training loss: 0.08882012963294983\n",
      "Training loss: 0.08626624941825867\n",
      "Training loss: 0.09272277355194092\n",
      "Training loss: 0.0954841896891594\n",
      "Training loss: 0.09144974499940872\n",
      "Training loss: 0.08990602940320969\n",
      "Training loss: 0.08572972565889359\n",
      "Training loss: 0.09115613996982574\n",
      "Training loss: 0.09926031529903412\n",
      "Training loss: 0.09226322174072266\n",
      "Training loss: 0.09334224462509155\n",
      "Training loss: 0.09038098156452179\n",
      "Training loss: 0.09323490411043167\n",
      "Training loss: 0.09777263551950455\n",
      "Training loss: 0.09626135230064392\n",
      "Training loss: 0.09507588297128677\n",
      "Training loss: 0.09355918318033218\n",
      "Training loss: 0.09108401089906693\n",
      "Training loss: 0.09205019474029541\n",
      "Training loss: 0.08996544778347015\n",
      "Training loss: 0.08878428488969803\n",
      "Training loss: 0.09009143710136414\n",
      "Training loss: 0.09143378585577011\n",
      "Training loss: 0.08712824434041977\n",
      "Training loss: 0.09271138906478882\n",
      "Training loss: 0.08930205553770065\n",
      "Training loss: 0.09014087170362473\n",
      "Training loss: 0.08876674622297287\n",
      "Training loss: 0.08795779943466187\n",
      "Training loss: 0.08999531716108322\n",
      "Training loss: 0.08584997057914734\n",
      "Training loss: 0.08514546602964401\n",
      "Training loss: 0.08638530224561691\n",
      "Training loss: 0.08389286696910858\n",
      "Training loss: 0.08261997252702713\n",
      "Training loss: 0.09077207744121552\n",
      "Training loss: 0.08487764000892639\n",
      "Training loss: 0.0959450975060463\n",
      "Training loss: 0.08743540197610855\n",
      "Training loss: 0.08999647200107574\n",
      "Training loss: 0.08552251756191254\n",
      "Training loss: 0.09656105190515518\n",
      "Training loss: 0.09253282099962234\n",
      "Training loss: 0.08909504860639572\n",
      "Training loss: 0.09514912217855453\n",
      "Training loss: 0.09041503816843033\n",
      "Training loss: 0.09243350476026535\n",
      "Training loss: 0.09191552549600601\n",
      "Training loss: 0.09237378090620041\n",
      "Training loss: 0.08740998059511185\n",
      "Training loss: 0.08695731312036514\n",
      "Training loss: 0.09096100181341171\n",
      "Training loss: 0.09899640828371048\n",
      "Training loss: 0.0898110494017601\n",
      "Training loss: 0.08537357300519943\n",
      "Training loss: 0.08679798990488052\n",
      "Training loss: 0.08949711173772812\n",
      "Training loss: 0.08819758892059326\n",
      "Training loss: 0.09719189256429672\n",
      "Training loss: 0.08301178365945816\n",
      "Training loss: 0.08802059292793274\n",
      "Training loss: 0.08184175938367844\n",
      "Training loss: 0.09207166731357574\n",
      "Training loss: 0.09100649505853653\n",
      "Training loss: 0.09543700516223907\n",
      "Training loss: 0.08476360887289047\n",
      "Training loss: 0.08726107329130173\n",
      "Training loss: 0.08581919968128204\n",
      "Training loss: 0.08625112473964691\n",
      "Training loss: 0.09112215042114258\n",
      "Training loss: 0.0915551707148552\n",
      "Training loss: 0.08811026811599731\n",
      "Training loss: 0.09416107088327408\n",
      "Training loss: 0.0850565955042839\n",
      "Training loss: 0.08635462820529938\n",
      "Training loss: 0.0882561206817627\n",
      "Training loss: 0.08749958127737045\n",
      "Training loss: 0.08505428582429886\n",
      "Training loss: 0.0944766104221344\n",
      "Training loss: 0.09658221900463104\n",
      "Training loss: 0.09232228994369507\n",
      "Training loss: 0.08750595152378082\n",
      "Training loss: 0.08986794203519821\n",
      "Training loss: 0.09849468618631363\n",
      "Training loss: 0.09431667625904083\n",
      "Training loss: 0.09523756802082062\n",
      "Training loss: 0.09589824080467224\n",
      "Training loss: 0.09396898746490479\n",
      "Training loss: 0.08953376859426498\n",
      "Training loss: 0.09274844080209732\n",
      "Training loss: 0.09014583379030228\n",
      "Training loss: 0.09163238853216171\n",
      "Training loss: 0.09570738673210144\n",
      "Training loss: 0.09317360818386078\n",
      "Training loss: 0.09020478278398514\n",
      "Training loss: 0.08711610734462738\n",
      "Training loss: 0.09088940173387527\n",
      "Training loss: 0.08541058748960495\n",
      "Training loss: 0.08793442696332932\n",
      "Training loss: 0.0914802998304367\n",
      "Training loss: 0.08824451267719269\n",
      "Training loss: 0.088630810379982\n",
      "Training loss: 0.08808807283639908\n",
      "Training loss: 0.09036983549594879\n",
      "Training loss: 0.09138751029968262\n",
      "Training loss: 0.09526893496513367\n",
      "Training loss: 0.08470562845468521\n",
      "Training loss: 0.09625371545553207\n",
      "Training loss: 0.09304510056972504\n",
      "Training loss: 0.0894882008433342\n",
      "Training loss: 0.08933253586292267\n",
      "Training loss: 0.09194143861532211\n",
      "Training loss: 0.09780499339103699\n",
      "Training loss: 0.09182968735694885\n",
      "Training loss: 0.0920514166355133\n",
      "Training loss: 0.09546402841806412\n",
      "Training loss: 0.09140278398990631\n",
      "Training loss: 0.09954012930393219\n",
      "Training loss: 0.09172778576612473\n",
      "Training loss: 0.09747208654880524\n",
      "Training loss: 0.08880677074193954\n",
      "Training loss: 0.08820182085037231\n",
      "Training loss: 0.10059438645839691\n",
      "Training loss: 0.08821868896484375\n",
      "Training loss: 0.09653877466917038\n",
      "Training loss: 0.08611061424016953\n",
      "Training loss: 0.08957130461931229\n",
      "Training loss: 0.09234701097011566\n",
      "Training loss: 0.0983072891831398\n",
      "Training loss: 0.09076599776744843\n",
      "Training loss: 0.09638804197311401\n",
      "Training loss: 0.09926435351371765\n",
      "Training loss: 0.09248550236225128\n",
      "Training loss: 0.09223996102809906\n",
      "Training loss: 0.08883586525917053\n",
      "Training loss: 0.09100285917520523\n",
      "Training loss: 0.0953400582075119\n",
      "Training loss: 0.09625551849603653\n",
      "Training loss: 0.09422096610069275\n",
      "Training loss: 0.0937601700425148\n",
      "Training loss: 0.09539607912302017\n",
      "Training loss: 0.0874059721827507\n",
      "Training loss: 0.0957549586892128\n",
      "Training loss: 0.08904802799224854\n",
      "Training loss: 0.09111183136701584\n",
      "Training loss: 0.09305647760629654\n",
      "Training loss: 0.09281375259160995\n",
      "Training loss: 0.09420135617256165\n",
      "Training loss: 0.09208626300096512\n",
      "Training loss: 0.08574163913726807\n",
      "Training loss: 0.09515069425106049\n",
      "Training loss: 0.088522769510746\n",
      "Training loss: 0.09113415330648422\n",
      "Training loss: 0.0968092605471611\n",
      "Training loss: 0.09681858867406845\n",
      "Training loss: 0.09578552097082138\n",
      "Training loss: 0.09531540423631668\n",
      "Training loss: 0.09289371967315674\n",
      "Training loss: 0.09367155283689499\n",
      "Training loss: 0.09491237998008728\n",
      "Training loss: 0.08823373913764954\n",
      "Training loss: 0.09304332733154297\n",
      "Training loss: 0.09800383448600769\n",
      "Training loss: 0.09396068006753922\n",
      "Training loss: 0.08783047646284103\n",
      "Training loss: 0.08480667322874069\n",
      "Training loss: 0.09340913593769073\n",
      "Training loss: 0.09640751034021378\n",
      "Training loss: 0.09377267956733704\n",
      "Training loss: 0.08853146433830261\n",
      "Training loss: 0.08966801315546036\n",
      "Training loss: 0.0937943086028099\n",
      "Training loss: 0.09358729422092438\n",
      "Training loss: 0.09046123176813126\n",
      "Training loss: 0.0940929725766182\n",
      "Training loss: 0.09110734611749649\n",
      "Training loss: 0.09060044586658478\n",
      "Training loss: 0.09241042286157608\n",
      "Training loss: 0.0894271656870842\n",
      "Training loss: 0.09458354860544205\n",
      "Training loss: 0.08938518166542053\n",
      "Training loss: 0.08889951556921005\n",
      "Training loss: 0.09403301030397415\n",
      "Training loss: 0.08748619258403778\n",
      "Training loss: 0.08990085870027542\n",
      "Training loss: 0.0865989476442337\n",
      "Training loss: 0.09391322731971741\n",
      "Training loss: 0.08776220679283142\n",
      "Training loss: 0.0991075411438942\n",
      "Training loss: 0.08443256467580795\n",
      "Training loss: 0.08614615350961685\n",
      "Training loss: 0.0860816165804863\n",
      "Training loss: 0.09582037478685379\n",
      "Training loss: 0.08862275630235672\n",
      "Training loss: 0.09048577398061752\n",
      "Training loss: 0.08522957563400269\n",
      "Training loss: 0.08940526843070984\n",
      "Training loss: 0.08848720788955688\n",
      "Training loss: 0.08761916309595108\n",
      "Training loss: 0.09011805802583694\n",
      "Training loss: 0.09389592707157135\n",
      "Training loss: 0.08810955286026001\n",
      "Training loss: 0.08846370875835419\n",
      "Training loss: 0.09231957793235779\n",
      "Training loss: 0.10527834296226501\n",
      "Training loss: 0.09731493145227432\n",
      "Training loss: 0.08650661259889603\n",
      "Training loss: 0.08439033478498459\n",
      "Training loss: 0.08966753631830215\n",
      "Training loss: 0.08741946518421173\n",
      "Training loss: 0.0953022837638855\n",
      "Training loss: 0.09758032113313675\n",
      "Training loss: 0.08884147554636002\n",
      "Training loss: 0.09338810294866562\n",
      "Training loss: 0.09113608300685883\n",
      "Training loss: 0.08759413659572601\n",
      "Training loss: 0.09024515002965927\n",
      "Training loss: 0.08875319361686707\n",
      "Training loss: 0.09315596520900726\n",
      "Training loss: 0.09519746899604797\n",
      "Training loss: 0.09363996237516403\n",
      "Training loss: 0.0879507064819336\n",
      "Training loss: 0.1027374342083931\n",
      "Training loss: 0.08951985090970993\n",
      "Training loss: 0.08939802646636963\n",
      "Training loss: 0.08581943809986115\n",
      "Training loss: 0.08825160562992096\n",
      "Training loss: 0.09239528328180313\n",
      "Training loss: 0.08956172317266464\n",
      "Training loss: 0.09194585680961609\n",
      "Training loss: 0.08560983836650848\n",
      "Training loss: 0.09345279633998871\n",
      "Training loss: 0.09013354033231735\n",
      "Training loss: 0.09471132606267929\n",
      "Training loss: 0.0899578109383583\n",
      "Training loss: 0.08853317052125931\n",
      "Training loss: 0.08945315331220627\n",
      "Training loss: 0.08847235888242722\n",
      "Training loss: 0.09301792830228806\n",
      "Training loss: 0.0871715322136879\n",
      "Training loss: 0.08559300750494003\n",
      "Training loss: 0.09091699123382568\n",
      "Training loss: 0.09298200160264969\n",
      "Training loss: 0.0938330665230751\n",
      "Training loss: 0.09072209149599075\n",
      "Training loss: 0.09099525958299637\n",
      "Training loss: 0.08818960934877396\n",
      "Training loss: 0.09080750495195389\n",
      "Training loss: 0.0960976630449295\n",
      "Training loss: 0.08676233142614365\n",
      "Training loss: 0.08879625052213669\n",
      "Training loss: 0.0891510546207428\n",
      "Training loss: 0.09356943517923355\n",
      "Training loss: 0.08630699664354324\n",
      "Training loss: 0.0941852256655693\n",
      "Training loss: 0.0912894681096077\n",
      "Training loss: 0.10419787466526031\n",
      "Training loss: 0.09927588701248169\n",
      "Training loss: 0.09468249976634979\n",
      "Training loss: 0.09435629844665527\n",
      "Training loss: 0.09223092347383499\n",
      "Training loss: 0.09352811425924301\n",
      "Training loss: 0.08719690889120102\n",
      "Training loss: 0.09215199202299118\n",
      "Training loss: 0.09102771431207657\n",
      "Training loss: 0.09853509068489075\n",
      "Training loss: 0.09329284727573395\n",
      "Training loss: 0.08946260809898376\n",
      "Training loss: 0.09883662313222885\n",
      "Training loss: 0.09568775445222855\n",
      "Training loss: 0.09094181656837463\n",
      "Training loss: 0.09146736562252045\n",
      "Training loss: 0.09716672450304031\n",
      "Training loss: 0.08931100368499756\n",
      "Training loss: 0.09086368978023529\n",
      "Training loss: 0.09061596542596817\n",
      "Training loss: 0.09868396818637848\n",
      "Training loss: 0.0877472311258316\n",
      "Training loss: 0.09085994958877563\n",
      "Training loss: 0.08923139423131943\n",
      "Training loss: 0.09736760705709457\n",
      "Training loss: 0.08764179050922394\n",
      "Training loss: 0.08982150256633759\n",
      "Training loss: 0.0939629077911377\n",
      "Training loss: 0.09380022436380386\n",
      "Training loss: 0.098190538585186\n",
      "Training loss: 0.09009955078363419\n",
      "Training loss: 0.08818094432353973\n",
      "Training loss: 0.08708765357732773\n",
      "Training loss: 0.08997917920351028\n",
      "Training loss: 0.08366560190916061\n",
      "Training loss: 0.08990253508090973\n",
      "Training loss: 0.09643805772066116\n",
      "Training loss: 0.09188287705183029\n",
      "Training loss: 0.0902271643280983\n",
      "Training loss: 0.0868023931980133\n",
      "Training loss: 0.09376489371061325\n",
      "Training loss: 0.0867035910487175\n",
      "Training loss: 0.09005340188741684\n",
      "Training loss: 0.092791847884655\n",
      "Training loss: 0.08794162422418594\n",
      "Training loss: 0.09389618039131165\n",
      "Training loss: 0.08970548212528229\n",
      "Training loss: 0.08986042439937592\n",
      "Training loss: 0.08464886993169785\n",
      "Training loss: 0.09412440657615662\n",
      "Training loss: 0.0960606038570404\n",
      "Training loss: 0.08853442221879959\n",
      "Training loss: 0.09414729475975037\n",
      "Training loss: 0.08417167514562607\n",
      "Training loss: 0.08595004677772522\n",
      "Training loss: 0.09440893679857254\n",
      "Training loss: 0.09433962404727936\n",
      "Training loss: 0.09180284291505814\n",
      "Training loss: 0.09010083228349686\n",
      "Training loss: 0.08624276518821716\n",
      "Training loss: 0.09288010746240616\n",
      "Training loss: 0.08662557601928711\n",
      "Training loss: 0.09570753574371338\n",
      "Training loss: 0.09410671889781952\n",
      "Training loss: 0.08473758399486542\n",
      "Training loss: 0.08927787095308304\n",
      "Training loss: 0.08668877184391022\n",
      "Training loss: 0.08645379543304443\n",
      "Training loss: 0.09166515618562698\n",
      "Training loss: 0.09781204164028168\n",
      "Training loss: 0.0849686786532402\n",
      "Training loss: 0.08613821864128113\n",
      "Training loss: 0.08618150651454926\n",
      "Training loss: 0.09302080422639847\n",
      "Training loss: 0.09513216465711594\n",
      "Training loss: 0.0896516814827919\n",
      "Training loss: 0.0927465558052063\n",
      "Training loss: 0.09057534486055374\n",
      "Training loss: 0.09869489073753357\n",
      "Training loss: 0.0900646224617958\n",
      "Training loss: 0.08914092928171158\n",
      "Training loss: 0.08329169452190399\n",
      "Training loss: 0.08651057630777359\n",
      "Training loss: 0.0898432508111\n",
      "Training loss: 0.08849971741437912\n",
      "Training loss: 0.09035465121269226\n",
      "Training loss: 0.08954329043626785\n",
      "Training loss: 0.08582569658756256\n",
      "Training loss: 0.0974678099155426\n",
      "Training loss: 0.0873061940073967\n",
      "Training loss: 0.08432721346616745\n",
      "Training loss: 0.08840953558683395\n",
      "Training loss: 0.0884242057800293\n",
      "Training loss: 0.08816780894994736\n",
      "Training loss: 0.08376574516296387\n",
      "Training loss: 0.08320389688014984\n",
      "Training loss: 0.0884161964058876\n",
      "Training loss: 0.08933277428150177\n",
      "Training loss: 0.0938103199005127\n",
      "Training loss: 0.08638450503349304\n",
      "Training loss: 0.08868838101625443\n",
      "Training loss: 0.08310097455978394\n",
      "Training loss: 0.08540459722280502\n",
      "Training loss: 0.0851893201470375\n",
      "Training loss: 0.08580520749092102\n",
      "Training loss: 0.08322568982839584\n",
      "Training loss: 0.08572540432214737\n",
      "Training loss: 0.09445544332265854\n",
      "Training loss: 0.08456359058618546\n",
      "Training loss: 0.08288920670747757\n",
      "Training loss: 0.09154701977968216\n",
      "Training loss: 0.08899158239364624\n",
      "Training loss: 0.08600013703107834\n",
      "Training loss: 0.08406678587198257\n",
      "Training loss: 0.08735281229019165\n",
      "Training loss: 0.08463657647371292\n",
      "Training loss: 0.08232299238443375\n",
      "Training loss: 0.08513087034225464\n",
      "Training loss: 0.08983982354402542\n",
      "Training loss: 0.09424170106649399\n",
      "Training loss: 0.08979944884777069\n",
      "Training loss: 0.09285072237253189\n",
      "Training loss: 0.08993194252252579\n",
      "Training loss: 0.09344914555549622\n",
      "Training loss: 0.10102768987417221\n",
      "Training loss: 0.09199487417936325\n",
      "Training loss: 0.088487908244133\n",
      "Training loss: 0.08440704643726349\n",
      "Training loss: 0.09251613914966583\n",
      "Training loss: 0.09001026302576065\n",
      "Training loss: 0.08817779272794724\n",
      "Training loss: 0.09406206011772156\n",
      "Training loss: 0.10370668768882751\n",
      "Training loss: 0.0905490517616272\n",
      "Training loss: 0.0893043577671051\n",
      "Training loss: 0.09855702519416809\n",
      "Training loss: 0.08888529986143112\n",
      "Training loss: 0.09529604762792587\n",
      "Training loss: 0.08727290481328964\n",
      "Training loss: 0.08665081858634949\n",
      "Training loss: 0.08742517232894897\n",
      "Training loss: 0.0896090716123581\n",
      "Training loss: 0.08947928249835968\n",
      "Training loss: 0.08837586641311646\n",
      "Training loss: 0.09051566570997238\n",
      "Training loss: 0.08831464499235153\n",
      "Training loss: 0.08867426961660385\n",
      "Training loss: 0.08497873693704605\n",
      "Training loss: 0.08457678556442261\n",
      "Training loss: 0.08390466868877411\n",
      "Training loss: 0.0823555439710617\n",
      "Training loss: 0.08498343825340271\n",
      "Training loss: 0.08646859973669052\n",
      "Training loss: 0.0831654891371727\n",
      "Training loss: 0.08814488351345062\n",
      "Training loss: 0.07909634709358215\n",
      "Training loss: 0.12363702803850174\n",
      "Training loss: 0.09007124602794647\n",
      "Training loss: 0.08392070978879929\n",
      "Training loss: 0.09200283885002136\n",
      "Training loss: 0.0818948745727539\n",
      "Training loss: 0.08836530894041061\n",
      "Training loss: 0.09013072401285172\n",
      "Training loss: 0.08161066472530365\n",
      "Training loss: 0.08221422135829926\n",
      "Training loss: 0.08722567558288574\n",
      "Training loss: 0.0915035679936409\n",
      "Training loss: 0.08764337003231049\n",
      "Training loss: 0.09129803627729416\n",
      "Training loss: 0.08473534137010574\n",
      "Training loss: 0.08774113655090332\n",
      "Training loss: 0.09788912534713745\n",
      "Training loss: 0.08857809752225876\n",
      "Training loss: 0.08077464997768402\n",
      "Training loss: 0.09751790016889572\n",
      "Training loss: 0.08584436774253845\n",
      "Training loss: 0.09185054898262024\n",
      "Training loss: 0.08478441834449768\n",
      "Training loss: 0.09057820588350296\n",
      "Training loss: 0.08575968444347382\n",
      "Training loss: 0.08837474137544632\n",
      "Training loss: 0.08623708784580231\n",
      "Training loss: 0.08324696123600006\n",
      "Training loss: 0.08782187104225159\n",
      "Training loss: 0.09852296859025955\n",
      "Training loss: 0.09150544553995132\n",
      "Training loss: 0.08815930038690567\n",
      "Training loss: 0.09510976821184158\n",
      "Training loss: 0.08852658420801163\n",
      "Training loss: 0.08815973997116089\n",
      "Training loss: 0.0848953053355217\n",
      "Training loss: 0.08898531645536423\n",
      "Training loss: 0.0877724289894104\n",
      "Training loss: 0.08423161506652832\n",
      "Training loss: 0.09140338003635406\n",
      "Training loss: 0.08319373428821564\n",
      "Training loss: 0.08840090781450272\n",
      "Training loss: 0.09376592189073563\n",
      "Training loss: 0.08769901841878891\n",
      "Training loss: 0.08568409085273743\n",
      "Training loss: 0.08880666643381119\n",
      "Training loss: 0.0841081514954567\n",
      "Training loss: 0.0924331471323967\n",
      "Training loss: 0.08502386510372162\n",
      "Training loss: 0.08410204946994781\n",
      "Training loss: 0.09001445770263672\n",
      "Training loss: 0.08520745486021042\n",
      "Training loss: 0.08312583714723587\n",
      "Training loss: 0.08556748181581497\n",
      "Training loss: 0.08640727400779724\n",
      "Training loss: 0.08513697236776352\n",
      "Training loss: 0.08367674052715302\n",
      "Training loss: 0.0847712978720665\n",
      "Training loss: 0.0821642056107521\n",
      "Training loss: 0.08355080336332321\n",
      "Training loss: 0.08960140496492386\n",
      "Training loss: 0.08533108979463577\n",
      "Training loss: 0.09245879203081131\n",
      "Training loss: 0.08250950276851654\n",
      "Training loss: 0.09162060171365738\n",
      "Training loss: 0.08749064803123474\n",
      "Training loss: 0.08569099754095078\n",
      "Training loss: 0.09006092697381973\n",
      "Training loss: 0.08637607842683792\n",
      "Training loss: 0.09981728345155716\n",
      "Training loss: 0.08890564739704132\n",
      "Training loss: 0.08460962772369385\n",
      "Training loss: 0.08854657411575317\n",
      "Training loss: 0.08602484315633774\n",
      "Training loss: 0.09202442318201065\n",
      "Training loss: 0.08634670823812485\n",
      "Training loss: 0.08847849071025848\n",
      "Training loss: 0.08997318148612976\n",
      "Training loss: 0.09151856601238251\n",
      "Training loss: 0.08375036716461182\n",
      "Training loss: 0.08423308283090591\n",
      "Training loss: 0.0987381637096405\n",
      "Training loss: 0.08817160874605179\n",
      "Training loss: 0.08842933177947998\n",
      "Training loss: 0.09041307866573334\n",
      "Training loss: 0.0854012593626976\n",
      "Training loss: 0.08744847029447556\n",
      "Training loss: 0.0833953320980072\n",
      "Training loss: 0.08884739130735397\n",
      "Training loss: 0.08240412920713425\n",
      "Training loss: 0.08592198044061661\n",
      "Training loss: 0.09457308799028397\n",
      "Training loss: 0.09381891041994095\n",
      "Training loss: 0.08989494293928146\n",
      "Training loss: 0.08786433935165405\n",
      "Training loss: 0.08673948794603348\n",
      "Training loss: 0.08313757181167603\n",
      "Training loss: 0.08643190562725067\n",
      "Training loss: 0.08705329149961472\n",
      "Training loss: 0.08586794137954712\n",
      "Training loss: 0.09242831915616989\n",
      "Training loss: 0.0975419208407402\n",
      "Training loss: 0.09193538129329681\n",
      "Training loss: 0.08623834699392319\n",
      "Training loss: 0.08665508031845093\n",
      "Training loss: 0.08730021864175797\n",
      "Training loss: 0.08140451461076736\n",
      "Training loss: 0.0912887454032898\n",
      "Training loss: 0.09425038844347\n",
      "Training loss: 0.0880817249417305\n",
      "Training loss: 0.09267088770866394\n",
      "Training loss: 0.08503947407007217\n",
      "Training loss: 0.09083037823438644\n",
      "Training loss: 0.0843961089849472\n",
      "Training loss: 0.08397404849529266\n",
      "Training loss: 0.09347674995660782\n",
      "Training loss: 0.08565836399793625\n",
      "Training loss: 0.0838765949010849\n",
      "Training loss: 0.08230070024728775\n",
      "Training loss: 0.08493960648775101\n",
      "Training loss: 0.0849132388830185\n",
      "Training loss: 0.07751673460006714\n",
      "Training loss: 0.0792633444070816\n",
      "Training loss: 0.08515502512454987\n",
      "Training loss: 0.08450429886579514\n",
      "Training loss: 0.08438028395175934\n",
      "Training loss: 0.08757270127534866\n",
      "Training loss: 0.08525555580854416\n",
      "Training loss: 0.0879802331328392\n",
      "Training loss: 0.08456751704216003\n",
      "Training loss: 0.07837125658988953\n",
      "Training loss: 0.08856899291276932\n",
      "Training loss: 0.08273067325353622\n",
      "Training loss: 0.08243202418088913\n",
      "Training loss: 0.08638614416122437\n",
      "Training loss: 0.08586598932743073\n",
      "Training loss: 0.08656375110149384\n",
      "Training loss: 0.08149922639131546\n",
      "Training loss: 0.08116106688976288\n",
      "Training loss: 0.09101563692092896\n",
      "Training loss: 0.0899588093161583\n",
      "Training loss: 0.08461036533117294\n",
      "Training loss: 0.08822038769721985\n",
      "Training loss: 0.08478690683841705\n",
      "Training loss: 0.08262984454631805\n",
      "Training loss: 0.08419609069824219\n",
      "Training loss: 0.08169190585613251\n",
      "Training loss: 0.09044138342142105\n",
      "Training loss: 0.08462675660848618\n",
      "Training loss: 0.09184164553880692\n",
      "Training loss: 0.0856580063700676\n",
      "Training loss: 0.08890674263238907\n",
      "Training loss: 0.08766771107912064\n",
      "Training loss: 0.08482623100280762\n",
      "Training loss: 0.08916390687227249\n",
      "Training loss: 0.08144095540046692\n",
      "Training loss: 0.08841833472251892\n",
      "Training loss: 0.08565717935562134\n",
      "Training loss: 0.08410068601369858\n",
      "Training loss: 0.09084255248308182\n",
      "Training loss: 0.07894580811262131\n",
      "Training loss: 0.08784114569425583\n",
      "Training loss: 0.08601425588130951\n",
      "Training loss: 0.08642604947090149\n",
      "Training loss: 0.07981114089488983\n",
      "Training loss: 0.08776087313890457\n",
      "Training loss: 0.08483558148145676\n",
      "Training loss: 0.08105864375829697\n",
      "Training loss: 0.08499043434858322\n",
      "Training loss: 0.08951159566640854\n",
      "Training loss: 0.08591512590646744\n",
      "Training loss: 0.07890603691339493\n",
      "Training loss: 0.08268282562494278\n",
      "Training loss: 0.08025509119033813\n",
      "Training loss: 0.09194711595773697\n",
      "Training loss: 0.0901351049542427\n",
      "Training loss: 0.08364276587963104\n",
      "Training loss: 0.08501837402582169\n",
      "Training loss: 0.09354791045188904\n",
      "Training loss: 0.08636169880628586\n",
      "Training loss: 0.08415107429027557\n",
      "Training loss: 0.08271794021129608\n",
      "Training loss: 0.08942114561796188\n",
      "Training loss: 0.08426648378372192\n",
      "Training loss: 0.08388648182153702\n",
      "Training loss: 0.08307202905416489\n",
      "Training loss: 0.08813167363405228\n",
      "Training loss: 0.08460590988397598\n",
      "Training loss: 0.08194216340780258\n",
      "Training loss: 0.08688250929117203\n",
      "Training loss: 0.0791461169719696\n",
      "Training loss: 0.08549697697162628\n",
      "Training loss: 0.0863553062081337\n",
      "Training loss: 0.08330361545085907\n",
      "Training loss: 0.09382741898298264\n",
      "Training loss: 0.08153046667575836\n",
      "Training loss: 0.08620072901248932\n",
      "Training loss: 0.08218155801296234\n",
      "Training loss: 0.0866316631436348\n",
      "Training loss: 0.08250503242015839\n",
      "Training loss: 0.09100562334060669\n",
      "Training loss: 0.09175426512956619\n",
      "Training loss: 0.08819488435983658\n",
      "Training loss: 0.0870252177119255\n",
      "Training loss: 0.08556929230690002\n",
      "Training loss: 0.0852028951048851\n",
      "Training loss: 0.07768989354372025\n",
      "Training loss: 0.08453220129013062\n",
      "Training loss: 0.09524031728506088\n",
      "Training loss: 0.08433398604393005\n",
      "Training loss: 0.08505693823099136\n",
      "Training loss: 0.0883430764079094\n",
      "Training loss: 0.09669850021600723\n",
      "Training loss: 0.08574569225311279\n",
      "Training loss: 0.08285431563854218\n",
      "Training loss: 0.08929755538702011\n",
      "Training loss: 0.08300240337848663\n",
      "Training loss: 0.08067657053470612\n",
      "Training loss: 0.08774635940790176\n",
      "Training loss: 0.08261214941740036\n",
      "Training loss: 0.08667740225791931\n",
      "Training loss: 0.08615151047706604\n",
      "Training loss: 0.0871175155043602\n",
      "Training loss: 0.08726291358470917\n",
      "Training loss: 0.09402156621217728\n",
      "Training loss: 0.10210562497377396\n",
      "Training loss: 0.08504503965377808\n",
      "Training loss: 0.08304920792579651\n",
      "Training loss: 0.07972082495689392\n",
      "Training loss: 0.08851325511932373\n",
      "Training loss: 0.08284594863653183\n",
      "Training loss: 0.08787824958562851\n",
      "Training loss: 0.08661048859357834\n",
      "Training loss: 0.08847367763519287\n",
      "Training loss: 0.08646649867296219\n",
      "Training loss: 0.09202457964420319\n",
      "Training loss: 0.084421306848526\n",
      "Training loss: 0.08731132745742798\n",
      "Training loss: 0.08672190457582474\n",
      "Training loss: 0.09020465612411499\n",
      "Training loss: 0.07754510641098022\n",
      "Training loss: 0.08884309977293015\n",
      "Training loss: 0.0904228612780571\n",
      "Training loss: 0.08780191838741302\n",
      "Training loss: 0.08727802336215973\n",
      "Training loss: 0.08771923184394836\n",
      "Training loss: 0.08386451750993729\n",
      "Training loss: 0.08784662932157516\n",
      "Training loss: 0.09220289438962936\n",
      "Training loss: 0.08166467398405075\n",
      "Training loss: 0.08482573926448822\n",
      "Training loss: 0.09060006588697433\n",
      "Training loss: 0.091824010014534\n",
      "Training loss: 0.0831468403339386\n",
      "Training loss: 0.08068294823169708\n",
      "Training loss: 0.08835473656654358\n",
      "Training loss: 0.08726536482572556\n",
      "Training loss: 0.08686668425798416\n",
      "Training loss: 0.08493096381425858\n",
      "Training loss: 0.08475375175476074\n",
      "Training loss: 0.08554047346115112\n",
      "Training loss: 0.08380711823701859\n",
      "Training loss: 0.09880885481834412\n",
      "Training loss: 0.08666627109050751\n",
      "Training loss: 0.08740247786045074\n",
      "Training loss: 0.08932537585496902\n",
      "Training loss: 0.09101764112710953\n",
      "Training loss: 0.08885736763477325\n",
      "Training loss: 0.08719717711210251\n",
      "Training loss: 0.08281292021274567\n",
      "Training loss: 0.0826009139418602\n",
      "Training loss: 0.08787091821432114\n",
      "Training loss: 0.08934828639030457\n",
      "Training loss: 0.08276775479316711\n",
      "Training loss: 0.08693879097700119\n",
      "Training loss: 0.08974239975214005\n",
      "Training loss: 0.08760201185941696\n",
      "Training loss: 0.09519422799348831\n",
      "Training loss: 0.09099704772233963\n",
      "Training loss: 0.08446412533521652\n",
      "Training loss: 0.08803296089172363\n",
      "Training loss: 0.09790990501642227\n",
      "Training loss: 0.0932183638215065\n",
      "Training loss: 0.08453202992677689\n",
      "Training loss: 0.08292489498853683\n",
      "Training loss: 0.09543576836585999\n",
      "Training loss: 0.089973583817482\n",
      "Training loss: 0.08319280296564102\n",
      "Training loss: 0.09536470472812653\n",
      "Training loss: 0.0965871661901474\n",
      "Training loss: 0.08547096699476242\n",
      "Training loss: 0.09774254262447357\n",
      "Training loss: 0.08655613660812378\n",
      "Training loss: 0.08962193131446838\n",
      "Training loss: 0.09461788088083267\n",
      "Training loss: 0.09745889902114868\n",
      "Training loss: 0.08602911233901978\n",
      "Training loss: 0.08292336016893387\n",
      "Training loss: 0.08228057622909546\n",
      "Training loss: 0.08533326536417007\n",
      "Training loss: 0.08330508321523666\n",
      "Training loss: 0.08302121609449387\n",
      "Training loss: 0.08279602229595184\n",
      "Training loss: 0.08771109580993652\n",
      "Training loss: 0.08345333486795425\n",
      "Training loss: 0.0845920592546463\n",
      "Training loss: 0.08214909583330154\n",
      "Training loss: 0.09978637844324112\n",
      "Training loss: 0.09056338667869568\n",
      "Training loss: 0.08497913926839828\n",
      "Training loss: 0.08767435699701309\n",
      "Training loss: 0.08548051863908768\n",
      "Training loss: 0.09329554438591003\n",
      "Training loss: 0.09037713706493378\n",
      "Training loss: 0.08218655735254288\n",
      "Training loss: 0.08714265376329422\n",
      "Training loss: 0.09296977519989014\n",
      "Training loss: 0.08769688010215759\n",
      "Training loss: 0.08431831002235413\n",
      "Training loss: 0.08638709038496017\n",
      "Training loss: 0.09205787628889084\n",
      "Training loss: 0.09235449135303497\n",
      "Training loss: 0.08209838718175888\n",
      "Training loss: 0.09274104982614517\n",
      "Training loss: 0.08871379494667053\n",
      "Training loss: 0.08454225212335587\n",
      "Training loss: 0.08256880939006805\n",
      "Training loss: 0.08853411674499512\n",
      "Training loss: 0.08630694448947906\n",
      "Training loss: 0.08619817346334457\n",
      "Training loss: 0.08252730220556259\n",
      "Training loss: 0.0891263410449028\n",
      "Training loss: 0.09196829795837402\n",
      "Training loss: 0.081292524933815\n",
      "Training loss: 0.0819658413529396\n",
      "Training loss: 0.08736275136470795\n",
      "Training loss: 0.08178000897169113\n",
      "Training loss: 0.08584072440862656\n",
      "Training loss: 0.08741427212953568\n",
      "Training loss: 0.08601177483797073\n",
      "Training loss: 0.07796237617731094\n",
      "Training loss: 0.09043130278587341\n",
      "Training loss: 0.08624096214771271\n",
      "Training loss: 0.08423606306314468\n",
      "Training loss: 0.08655212819576263\n",
      "Training loss: 0.08079436421394348\n",
      "Training loss: 0.09278792142868042\n",
      "Training loss: 0.08268249034881592\n",
      "Training loss: 0.08130817115306854\n",
      "Training loss: 0.09510530531406403\n",
      "Training loss: 0.08198737353086472\n",
      "Training loss: 0.08358542621135712\n",
      "Training loss: 0.08686447143554688\n",
      "Training loss: 0.087809257209301\n",
      "Training loss: 0.09077101200819016\n",
      "Training loss: 0.08866098523139954\n",
      "Training loss: 0.09589925408363342\n",
      "Training loss: 0.08236391097307205\n",
      "Training loss: 0.0866965502500534\n",
      "Training loss: 0.08501461148262024\n",
      "Training loss: 0.0856790691614151\n",
      "Training loss: 0.08778762072324753\n",
      "Training loss: 0.08917439728975296\n",
      "Training loss: 0.08011411130428314\n",
      "Training loss: 0.08490325510501862\n",
      "Training loss: 0.08786088973283768\n",
      "Training loss: 0.08713535964488983\n",
      "Training loss: 0.08317659795284271\n",
      "Training loss: 0.08347656577825546\n",
      "Training loss: 0.08439694344997406\n",
      "Training loss: 0.08823470026254654\n",
      "Training loss: 0.08934151381254196\n",
      "Training loss: 0.08808992803096771\n",
      "Training loss: 0.09364587813615799\n",
      "Training loss: 0.08012517541646957\n",
      "Training loss: 0.09097185730934143\n",
      "Training loss: 0.08783794194459915\n",
      "Training loss: 0.08307883143424988\n",
      "Training loss: 0.08118657767772675\n",
      "Training loss: 0.08602103590965271\n",
      "Training loss: 0.08859732747077942\n",
      "Training loss: 0.08025822043418884\n",
      "Training loss: 0.08696962147951126\n",
      "Training loss: 0.0787101536989212\n",
      "Training loss: 0.09070554375648499\n",
      "Training loss: 0.0849141925573349\n",
      "Training loss: 0.08709196746349335\n",
      "Training loss: 0.08719675242900848\n",
      "Training loss: 0.08694833517074585\n",
      "Training loss: 0.08345720916986465\n",
      "Training loss: 0.08218082785606384\n",
      "Training loss: 0.08672814816236496\n",
      "Training loss: 0.08937017619609833\n",
      "Training loss: 0.0879923403263092\n",
      "Training loss: 0.08526386320590973\n",
      "Training loss: 0.0804450511932373\n",
      "Training loss: 0.08334078639745712\n",
      "Training loss: 0.08896952122449875\n",
      "Training loss: 0.08736931532621384\n",
      "Training loss: 0.08660758286714554\n",
      "Training loss: 0.08548344671726227\n",
      "Training loss: 0.08523581922054291\n",
      "Training loss: 0.09296264499425888\n",
      "Training loss: 0.08511757850646973\n",
      "Training loss: 0.08149435371160507\n",
      "Training loss: 0.07896716147661209\n",
      "Training loss: 0.08148213475942612\n",
      "Training loss: 0.0890880674123764\n",
      "Training loss: 0.08741376549005508\n",
      "Training loss: 0.08450372517108917\n",
      "Training loss: 0.08188734948635101\n",
      "Training loss: 0.08955149352550507\n",
      "Training loss: 0.08226869255304337\n",
      "Training loss: 0.08290362358093262\n",
      "Training loss: 0.08210774511098862\n",
      "Training loss: 0.08914928883314133\n",
      "Training loss: 0.08266046643257141\n",
      "Training loss: 0.0850958451628685\n",
      "Training loss: 0.08300390839576721\n",
      "Training loss: 0.08000898361206055\n",
      "Training loss: 0.08910836279392242\n",
      "Training loss: 0.08661872148513794\n",
      "Training loss: 0.08302228897809982\n",
      "Training loss: 0.08187656104564667\n",
      "Training loss: 0.09520231187343597\n",
      "Training loss: 0.0904768779873848\n",
      "Training loss: 0.08627603948116302\n",
      "Training loss: 0.0884917750954628\n",
      "Training loss: 0.09313642233610153\n",
      "Training loss: 0.08084630966186523\n",
      "Training loss: 0.09146726131439209\n",
      "Training loss: 0.08608128130435944\n",
      "Training loss: 0.0850582867860794\n",
      "Training loss: 0.08238399773836136\n",
      "Training loss: 0.08739101141691208\n",
      "Training loss: 0.09171874821186066\n",
      "Training loss: 0.09009455144405365\n",
      "Training loss: 0.08516295999288559\n",
      "Training loss: 0.08284549415111542\n",
      "Training loss: 0.08604654669761658\n",
      "Training loss: 0.07839439809322357\n",
      "Training loss: 0.0816890299320221\n",
      "Training loss: 0.08516652882099152\n",
      "Training loss: 0.08005277812480927\n",
      "Training loss: 0.0869314894080162\n",
      "Training loss: 0.08373087644577026\n",
      "Training loss: 0.08715305477380753\n",
      "Training loss: 0.08095910400152206\n",
      "Training loss: 0.09127236902713776\n",
      "Training loss: 0.08953158557415009\n",
      "Training loss: 0.09118279814720154\n",
      "Training loss: 0.08701641112565994\n",
      "Training loss: 0.08838675171136856\n",
      "Training loss: 0.08007463067770004\n",
      "Training loss: 0.0847797840833664\n",
      "Training loss: 0.08382605016231537\n",
      "Training loss: 0.08803366869688034\n",
      "Training loss: 0.0882270410656929\n",
      "Training loss: 0.08313262462615967\n",
      "Training loss: 0.08730097115039825\n",
      "Training loss: 0.08709964901208878\n",
      "Training loss: 0.08522836118936539\n",
      "Training loss: 0.08262253552675247\n",
      "Training loss: 0.08378105610609055\n",
      "Training loss: 0.07947181910276413\n",
      "Training loss: 0.08532876521348953\n",
      "Training loss: 0.09395014494657516\n",
      "Training loss: 0.08360280841588974\n",
      "Training loss: 0.07797319442033768\n",
      "Training loss: 0.08254007995128632\n",
      "Training loss: 0.09193127602338791\n",
      "Training loss: 0.0822867751121521\n",
      "Training loss: 0.0807623341679573\n",
      "Training loss: 0.08098572492599487\n",
      "Training loss: 0.08350887894630432\n",
      "Training loss: 0.08191300183534622\n",
      "Training loss: 0.08647584170103073\n",
      "Training loss: 0.08596920967102051\n",
      "Training loss: 0.08080203831195831\n",
      "Training loss: 0.08686738461256027\n",
      "Training loss: 0.08352073282003403\n",
      "Training loss: 0.08362225443124771\n",
      "Training loss: 0.08785675466060638\n",
      "Training loss: 0.08233083784580231\n",
      "Training loss: 0.08711407333612442\n",
      "Training loss: 0.0894857868552208\n",
      "Training loss: 0.0950363278388977\n",
      "Training loss: 0.09096487611532211\n",
      "Training loss: 0.09133108705282211\n",
      "Training loss: 0.08835794776678085\n",
      "Training loss: 0.08723930269479752\n",
      "Training loss: 0.08661939203739166\n",
      "Training loss: 0.08939729630947113\n",
      "Training loss: 0.08705908060073853\n",
      "Training loss: 0.08516660332679749\n",
      "Training loss: 0.087452232837677\n",
      "Training loss: 0.08194638043642044\n",
      "Training loss: 0.08516491204500198\n",
      "Training loss: 0.08995433151721954\n",
      "Training loss: 0.08812371641397476\n",
      "Training loss: 0.08827351033687592\n",
      "Training loss: 0.08649328351020813\n",
      "Training loss: 0.08506858348846436\n",
      "Training loss: 0.08960739523172379\n",
      "Training loss: 0.08504010736942291\n",
      "Training loss: 0.07808461785316467\n",
      "Training loss: 0.0820050835609436\n",
      "Training loss: 0.0920959860086441\n",
      "Training loss: 0.08115342259407043\n",
      "Training loss: 0.08283483982086182\n",
      "Training loss: 0.08219990134239197\n",
      "Training loss: 0.08280690014362335\n",
      "Training loss: 0.08490785956382751\n",
      "Training loss: 0.08652788400650024\n",
      "Training loss: 0.08381487429141998\n",
      "Training loss: 0.0876394510269165\n",
      "Training loss: 0.08344510942697525\n",
      "Training loss: 0.08878413587808609\n",
      "Training loss: 0.08127004653215408\n",
      "Training loss: 0.0875731036067009\n",
      "Training loss: 0.07635308057069778\n",
      "Training loss: 0.08217664062976837\n",
      "Training loss: 0.08160128444433212\n",
      "Training loss: 0.08002040535211563\n",
      "Training loss: 0.0789019986987114\n",
      "Training loss: 0.07843529433012009\n",
      "Training loss: 0.08867636322975159\n",
      "Training loss: 0.07571957260370255\n",
      "Training loss: 0.08550822734832764\n",
      "Training loss: 0.07752664387226105\n",
      "Training loss: 0.0797000378370285\n",
      "Training loss: 0.08445370942354202\n",
      "Training loss: 0.089389368891716\n",
      "Training loss: 0.10088977962732315\n",
      "Training loss: 0.08500095456838608\n",
      "Training loss: 0.07901788502931595\n",
      "Training loss: 0.08465102314949036\n",
      "Training loss: 0.08153790980577469\n",
      "Training loss: 0.08192003518342972\n",
      "Training loss: 0.08517756313085556\n",
      "Training loss: 0.09222369641065598\n",
      "Training loss: 0.0767480656504631\n",
      "Training loss: 0.08206373453140259\n",
      "Training loss: 0.08079975098371506\n",
      "Training loss: 0.08316062390804291\n",
      "Training loss: 0.08696367591619492\n",
      "Training loss: 0.08060377836227417\n",
      "Training loss: 0.07835535705089569\n",
      "Training loss: 0.0877147987484932\n",
      "Training loss: 0.08735861629247665\n",
      "Training loss: 0.07696591317653656\n",
      "Training loss: 0.0784853994846344\n",
      "Training loss: 0.08389359712600708\n",
      "Training loss: 0.08698827028274536\n",
      "Training loss: 0.07873035967350006\n",
      "Training loss: 0.07902371883392334\n",
      "Training loss: 0.08261658996343613\n",
      "Training loss: 0.08564746379852295\n",
      "Training loss: 0.07927556335926056\n",
      "Training loss: 0.08350272476673126\n",
      "Training loss: 0.07974503934383392\n",
      "Training loss: 0.07925491034984589\n",
      "Training loss: 0.07953186333179474\n",
      "Training loss: 0.08205553889274597\n",
      "Training loss: 0.0782715231180191\n",
      "Training loss: 0.08333364129066467\n",
      "Training loss: 0.079746775329113\n",
      "Training loss: 0.0833253264427185\n",
      "Training loss: 0.08652814477682114\n",
      "Training loss: 0.07978855073451996\n",
      "Training loss: 0.07715319842100143\n",
      "Training loss: 0.08709202706813812\n",
      "Training loss: 0.08569957315921783\n",
      "Training loss: 0.08539942651987076\n",
      "Training loss: 0.09656663984060287\n",
      "Training loss: 0.0948173776268959\n",
      "Training loss: 0.08948874473571777\n",
      "Training loss: 0.08864523470401764\n",
      "Training loss: 0.09234566241502762\n",
      "Training loss: 0.09222274273633957\n",
      "Training loss: 0.08345476537942886\n",
      "Training loss: 0.08340306580066681\n",
      "Training loss: 0.08423903584480286\n",
      "Training loss: 0.10075144469738007\n",
      "Training loss: 0.09558359533548355\n",
      "Training loss: 0.10653577744960785\n",
      "Training loss: 0.08778832852840424\n",
      "Training loss: 0.09129223227500916\n",
      "Training loss: 0.08657906949520111\n",
      "Training loss: 0.09552586823701859\n",
      "Training loss: 0.08673999458551407\n",
      "Training loss: 0.08559852093458176\n",
      "Training loss: 0.08880868554115295\n",
      "Training loss: 0.09123193472623825\n",
      "Training loss: 0.08652296662330627\n",
      "Training loss: 0.08959576487541199\n",
      "Training loss: 0.08626969903707504\n",
      "Training loss: 0.08671294897794724\n",
      "Training loss: 0.08912956714630127\n",
      "Training loss: 0.08838258683681488\n",
      "Training loss: 0.09089656174182892\n",
      "Training loss: 0.08194153755903244\n",
      "Training loss: 0.08557062596082687\n",
      "Training loss: 0.08298178762197495\n",
      "Training loss: 0.08771061897277832\n",
      "Training loss: 0.08285574615001678\n",
      "Training loss: 0.08777658641338348\n",
      "Training loss: 0.08653359860181808\n",
      "Training loss: 0.08369226008653641\n",
      "Training loss: 0.08945851773023605\n",
      "Training loss: 0.09084150195121765\n",
      "Training loss: 0.07999381422996521\n",
      "Training loss: 0.08837848156690598\n",
      "Training loss: 0.08892723172903061\n",
      "Training loss: 0.09109323471784592\n",
      "Training loss: 0.08478133380413055\n",
      "Training loss: 0.08521267771720886\n",
      "Training loss: 0.0874958261847496\n",
      "Training loss: 0.08621332049369812\n",
      "Training loss: 0.09189067780971527\n",
      "Training loss: 0.08354683220386505\n",
      "Training loss: 0.08596332371234894\n",
      "Training loss: 0.08714298903942108\n",
      "Training loss: 0.08115629106760025\n",
      "Training loss: 0.07752850651741028\n",
      "Training loss: 0.07801588624715805\n",
      "Training loss: 0.08394776284694672\n",
      "Training loss: 0.08199912309646606\n",
      "Training loss: 0.08281929045915604\n",
      "Training loss: 0.07718311995267868\n",
      "Training loss: 0.08904984593391418\n",
      "Training loss: 0.08574724197387695\n",
      "Training loss: 0.08663332462310791\n",
      "Training loss: 0.07938914000988007\n",
      "Training loss: 0.08506204187870026\n",
      "Training loss: 0.08354255557060242\n",
      "Training loss: 0.08437412232160568\n",
      "Training loss: 0.08401105552911758\n",
      "Training loss: 0.07909679412841797\n",
      "Training loss: 0.08399928361177444\n",
      "Training loss: 0.0820205807685852\n",
      "Training loss: 0.07929622381925583\n",
      "Training loss: 0.08386071771383286\n",
      "Training loss: 0.08155202120542526\n",
      "Training loss: 0.08195656538009644\n",
      "Training loss: 0.09137967973947525\n",
      "Training loss: 0.08746175467967987\n",
      "Training loss: 0.08707606047391891\n",
      "Training loss: 0.08150403946638107\n",
      "Training loss: 0.08534900099039078\n",
      "Training loss: 0.0866151750087738\n",
      "Training loss: 0.0880478248000145\n",
      "Training loss: 0.08396060019731522\n",
      "Training loss: 0.08018054068088531\n",
      "Training loss: 0.07789333909749985\n",
      "Training loss: 0.08711115270853043\n",
      "Training loss: 0.08595047146081924\n",
      "Training loss: 0.08184807002544403\n",
      "Training loss: 0.08153165131807327\n",
      "Training loss: 0.0868426039814949\n",
      "Training loss: 0.07722009718418121\n",
      "Training loss: 0.08531005680561066\n",
      "Training loss: 0.08245112001895905\n",
      "Training loss: 0.08639200776815414\n",
      "Training loss: 0.07740148901939392\n",
      "Training loss: 0.08063926547765732\n",
      "Training loss: 0.07973193377256393\n",
      "Training loss: 0.08023387938737869\n",
      "Training loss: 0.07892744988203049\n",
      "Training loss: 0.07975221425294876\n",
      "Training loss: 0.08017756789922714\n",
      "Training loss: 0.0828104093670845\n",
      "Training loss: 0.0820402279496193\n",
      "Training loss: 0.07460463792085648\n",
      "Training loss: 0.08469747006893158\n",
      "Training loss: 0.07892685383558273\n",
      "Training loss: 0.07807952165603638\n",
      "Training loss: 0.07445832341909409\n",
      "Training loss: 0.07983458042144775\n",
      "Training loss: 0.07728582620620728\n",
      "Training loss: 0.07388307899236679\n",
      "Training loss: 0.0831078514456749\n",
      "Training loss: 0.07749044895172119\n",
      "Training loss: 0.07393412292003632\n",
      "Training loss: 0.07510663568973541\n",
      "Training loss: 0.08154696226119995\n",
      "Training loss: 0.07551050931215286\n",
      "Training loss: 0.0832565501332283\n",
      "Training loss: 0.07314471155405045\n",
      "Training loss: 0.07776989042758942\n",
      "Training loss: 0.07432708889245987\n",
      "Training loss: 0.07582666724920273\n",
      "Training loss: 0.082764632999897\n",
      "Training loss: 0.07184070348739624\n",
      "Training loss: 0.07749132812023163\n",
      "Training loss: 0.07879314571619034\n",
      "Training loss: 0.08410605043172836\n",
      "Training loss: 0.07925356179475784\n",
      "Training loss: 0.07701309770345688\n",
      "Training loss: 0.07710735499858856\n",
      "Training loss: 0.07352659106254578\n",
      "Training loss: 0.0762714296579361\n",
      "Training loss: 0.08043674379587173\n",
      "Training loss: 0.08864413946866989\n",
      "Training loss: 0.07814255356788635\n",
      "Training loss: 0.0871230885386467\n",
      "Training loss: 0.07884581387042999\n",
      "Training loss: 0.08207608759403229\n",
      "Training loss: 0.08463386446237564\n",
      "Training loss: 0.07688800990581512\n",
      "Training loss: 0.08331465721130371\n",
      "Training loss: 0.08199326694011688\n",
      "Training loss: 0.08655743300914764\n",
      "Training loss: 0.08394324779510498\n",
      "Training loss: 0.08436107635498047\n",
      "Training loss: 0.08471593260765076\n",
      "Training loss: 0.0895501971244812\n",
      "Training loss: 0.08718910068273544\n",
      "Training loss: 0.07910525053739548\n",
      "Training loss: 0.07820963859558105\n",
      "Training loss: 0.0828372910618782\n",
      "Training loss: 0.08332035690546036\n",
      "Training loss: 0.07885544002056122\n",
      "Training loss: 0.0832471176981926\n",
      "Training loss: 0.07894491404294968\n",
      "Training loss: 0.07798757404088974\n",
      "Training loss: 0.08071829378604889\n",
      "Training loss: 0.07941139489412308\n",
      "Training loss: 0.08330950886011124\n",
      "Training loss: 0.07972106337547302\n",
      "Training loss: 0.0793960839509964\n",
      "Training loss: 0.07752802222967148\n",
      "Training loss: 0.0906868427991867\n",
      "Training loss: 0.08439977467060089\n",
      "Training loss: 0.0758390873670578\n",
      "Training loss: 0.07569172233343124\n",
      "Training loss: 0.07190193980932236\n",
      "Training loss: 0.07893645763397217\n",
      "Training loss: 0.07948989421129227\n",
      "Training loss: 0.08626991510391235\n",
      "Training loss: 0.0793093889951706\n",
      "Training loss: 0.08047767728567123\n",
      "Training loss: 0.0779200866818428\n",
      "Training loss: 0.07644879072904587\n",
      "Training loss: 0.07994837313890457\n",
      "Training loss: 0.07798850536346436\n",
      "Training loss: 0.08059398084878922\n",
      "Training loss: 0.08386918157339096\n",
      "Training loss: 0.08461639285087585\n",
      "Training loss: 0.0900886133313179\n",
      "Training loss: 0.08910719305276871\n",
      "Training loss: 0.08271651715040207\n",
      "Training loss: 0.08260222524404526\n",
      "Training loss: 0.08211451768875122\n",
      "Training loss: 0.08333276212215424\n",
      "Training loss: 0.07720315456390381\n",
      "Training loss: 0.08202672749757767\n",
      "Training loss: 0.08560913801193237\n",
      "Training loss: 0.08028273284435272\n",
      "Training loss: 0.0767308920621872\n",
      "Training loss: 0.08256257325410843\n",
      "Training loss: 0.07884173840284348\n",
      "Training loss: 0.08109492063522339\n",
      "Training loss: 0.08745571970939636\n",
      "Training loss: 0.08248169720172882\n",
      "Training loss: 0.08307066559791565\n",
      "Training loss: 0.8257420659065247\n",
      "Training loss: 0.07586333900690079\n",
      "Training loss: 0.07851841300725937\n",
      "Training loss: 0.0835561752319336\n",
      "Training loss: 0.08443567901849747\n",
      "Training loss: 0.08760689198970795\n",
      "Training loss: 0.08486245572566986\n",
      "Training loss: 0.08950667828321457\n",
      "Training loss: 0.08712011575698853\n",
      "Training loss: 0.08950288593769073\n",
      "Training loss: 0.07739462703466415\n",
      "Training loss: 0.08450304716825485\n",
      "Training loss: 0.07824638485908508\n",
      "Training loss: 0.08163183182477951\n",
      "Training loss: 0.08323803544044495\n",
      "Training loss: 0.07922900468111038\n",
      "Training loss: 0.08233663439750671\n",
      "Training loss: 0.0827673077583313\n",
      "Training loss: 0.07652691751718521\n",
      "Training loss: 0.08219003677368164\n",
      "Training loss: 0.0790475606918335\n",
      "Training loss: 0.07846047729253769\n",
      "Training loss: 0.0788230374455452\n",
      "Training loss: 0.07399995625019073\n",
      "Training loss: 0.0798037126660347\n",
      "Training loss: 0.08235149085521698\n",
      "Training loss: 0.08212318271398544\n",
      "Training loss: 0.08034626394510269\n",
      "Training loss: 0.08001429587602615\n",
      "Training loss: 0.0874427929520607\n",
      "Training loss: 0.07804959267377853\n",
      "Training loss: 0.07947586476802826\n",
      "Training loss: 0.08504438400268555\n",
      "Training loss: 0.08167625218629837\n",
      "Training loss: 0.08723311871290207\n",
      "Training loss: 0.08738606423139572\n",
      "Training loss: 0.08549746870994568\n",
      "Training loss: 0.09008848667144775\n",
      "Training loss: 0.09608227014541626\n",
      "Training loss: 0.07788176834583282\n",
      "Training loss: 0.07985390722751617\n",
      "Training loss: 0.08696155250072479\n",
      "Training loss: 0.08257845789194107\n",
      "Training loss: 0.08741683512926102\n",
      "Training loss: 0.0856352150440216\n",
      "Training loss: 0.08563315868377686\n",
      "Training loss: 0.08623147755861282\n",
      "Training loss: 0.08287883549928665\n",
      "Training loss: 0.07790021598339081\n",
      "Training loss: 0.0858130007982254\n",
      "Training loss: 0.08429549634456635\n",
      "Training loss: 0.0813249945640564\n",
      "Training loss: 0.08441437780857086\n",
      "Training loss: 0.08590670675039291\n",
      "Training loss: 0.08763224631547928\n",
      "Training loss: 0.08122110366821289\n",
      "Training loss: 0.07993622869253159\n",
      "Training loss: 0.07397498190402985\n",
      "Training loss: 0.08406755328178406\n",
      "Training loss: 0.08524096012115479\n",
      "Training loss: 0.08588407933712006\n",
      "Training loss: 0.08114679157733917\n",
      "Training loss: 0.0939469039440155\n",
      "Training loss: 0.08441613614559174\n",
      "Training loss: 0.07933465391397476\n",
      "Training loss: 0.08370103687047958\n",
      "Training loss: 0.07671071588993073\n",
      "Training loss: 0.07643800228834152\n",
      "Training loss: 0.07850348204374313\n",
      "Training loss: 0.0822688564658165\n",
      "Training loss: 0.08064176142215729\n",
      "Training loss: 0.0764864981174469\n",
      "Training loss: 0.07975278794765472\n",
      "Training loss: 0.08010676503181458\n",
      "Training loss: 0.0766778513789177\n",
      "Training loss: 0.07829013466835022\n",
      "Training loss: 0.08086381107568741\n",
      "Training loss: 0.07599129527807236\n",
      "Training loss: 0.08552274107933044\n",
      "Training loss: 0.08289425820112228\n",
      "Training loss: 0.09859207272529602\n",
      "Training loss: 0.0846451073884964\n",
      "Training loss: 0.08086913079023361\n",
      "Training loss: 0.08681359142065048\n",
      "Training loss: 0.0841517299413681\n",
      "Training loss: 0.08193179219961166\n",
      "Training loss: 0.0805448368191719\n",
      "Training loss: 0.08401884138584137\n",
      "Training loss: 0.07942438125610352\n",
      "Training loss: 0.08081678301095963\n",
      "Training loss: 0.08277545869350433\n",
      "Training loss: 0.08720707148313522\n",
      "Training loss: 0.0817071795463562\n",
      "Training loss: 0.07533334195613861\n",
      "Training loss: 0.079719178378582\n",
      "Training loss: 0.07980762422084808\n",
      "Training loss: 0.08121543377637863\n",
      "Training loss: 0.08136995136737823\n",
      "Training loss: 0.08185196667909622\n",
      "Training loss: 0.07521936297416687\n",
      "Training loss: 0.08591875433921814\n",
      "Training loss: 0.08400001376867294\n",
      "Training loss: 0.07909557968378067\n",
      "Training loss: 0.08707403391599655\n",
      "Training loss: 0.0822250247001648\n",
      "Training loss: 0.08603157848119736\n",
      "Training loss: 0.0882919654250145\n",
      "Training loss: 0.08155754208564758\n",
      "Training loss: 0.07657565176486969\n",
      "Training loss: 0.08741797506809235\n",
      "Training loss: 0.08230960369110107\n",
      "Training loss: 0.08006955683231354\n",
      "Training loss: 0.08322873711585999\n",
      "Training loss: 0.08265965431928635\n",
      "Training loss: 0.08724266290664673\n",
      "Training loss: 0.08957874029874802\n",
      "Training loss: 0.08510531485080719\n",
      "Training loss: 0.08228109776973724\n",
      "Training loss: 0.08463294059038162\n",
      "Training loss: 0.08113867044448853\n",
      "Training loss: 0.0815715491771698\n",
      "Training loss: 0.07935820519924164\n",
      "Training loss: 0.0897931456565857\n",
      "Training loss: 0.0832090750336647\n",
      "Training loss: 0.08147545903921127\n",
      "Training loss: 0.08405879139900208\n",
      "Training loss: 0.08346085995435715\n",
      "Training loss: 0.08257811516523361\n",
      "Training loss: 0.08195090293884277\n",
      "Training loss: 0.08457978069782257\n",
      "Training loss: 0.08217960596084595\n",
      "Training loss: 0.08127594739198685\n",
      "Training loss: 0.07856396585702896\n",
      "Training loss: 0.08016830682754517\n",
      "Training loss: 0.07699330151081085\n",
      "Training loss: 0.08692977577447891\n",
      "Training loss: 0.08393586426973343\n",
      "Training loss: 0.08270750939846039\n",
      "Training loss: 0.08427644520998001\n",
      "Training loss: 0.07836322486400604\n",
      "Training loss: 0.09067895263433456\n",
      "Training loss: 0.08415304124355316\n",
      "Training loss: 0.08895399421453476\n",
      "Training loss: 0.08291032165288925\n",
      "Training loss: 0.07908373326063156\n",
      "Training loss: 0.08221543580293655\n",
      "Training loss: 0.07981931418180466\n",
      "Training loss: 0.08134628087282181\n",
      "Training loss: 0.07928383350372314\n",
      "Training loss: 0.08761494606733322\n",
      "Training loss: 0.07487768679857254\n",
      "Training loss: 0.08394152671098709\n",
      "Training loss: 0.08201473206281662\n",
      "Training loss: 0.08560331910848618\n",
      "Training loss: 0.08426334708929062\n",
      "Training loss: 0.08619019389152527\n",
      "Training loss: 0.08151611685752869\n",
      "Training loss: 0.08398888260126114\n",
      "Training loss: 0.0858706682920456\n",
      "Training loss: 0.08705025166273117\n",
      "Training loss: 0.08753305673599243\n",
      "Training loss: 0.08139769732952118\n",
      "Training loss: 0.08197733014822006\n",
      "Training loss: 0.08586175739765167\n",
      "Training loss: 0.08228078484535217\n",
      "Training loss: 0.08342906832695007\n",
      "Training loss: 0.08406887948513031\n",
      "Training loss: 0.07759770005941391\n",
      "Training loss: 0.08266068249940872\n",
      "Training loss: 0.07518423348665237\n",
      "Training loss: 0.07844021171331406\n",
      "Training loss: 0.08461233973503113\n",
      "Training loss: 0.0785626620054245\n",
      "Training loss: 0.0819045826792717\n",
      "Training loss: 0.0835617184638977\n",
      "Training loss: 0.08052102476358414\n",
      "Training loss: 0.08396116644144058\n",
      "Training loss: 0.07365921884775162\n",
      "Training loss: 0.08220101147890091\n",
      "Training loss: 0.08581985533237457\n",
      "Training loss: 0.08020583540201187\n",
      "Training loss: 0.08096083253622055\n",
      "Training loss: 0.0799364373087883\n",
      "Training loss: 0.08353407680988312\n",
      "Training loss: 0.09369812160730362\n",
      "Training loss: 0.08367399126291275\n",
      "Training loss: 0.08814983814954758\n",
      "Training loss: 0.08442668616771698\n",
      "Training loss: 0.09143802523612976\n",
      "Training loss: 0.084231898188591\n",
      "Training loss: 0.09168834984302521\n",
      "Training loss: 0.08040548861026764\n",
      "Training loss: 0.07741224765777588\n",
      "Training loss: 0.0862778052687645\n",
      "Training loss: 0.07856468111276627\n",
      "Training loss: 0.08028743416070938\n",
      "Training loss: 0.0851449966430664\n",
      "Training loss: 0.07846108078956604\n",
      "Training loss: 0.08468537777662277\n",
      "Training loss: 0.07786700129508972\n",
      "Training loss: 0.07929861545562744\n",
      "Training loss: 0.08524814248085022\n",
      "Training loss: 0.07051029801368713\n",
      "Training loss: 0.08326810598373413\n",
      "Training loss: 0.07652497291564941\n",
      "Training loss: 0.08680053055286407\n",
      "Training loss: 0.08013875782489777\n",
      "Training loss: 0.08471450954675674\n",
      "Training loss: 0.08349142968654633\n",
      "Training loss: 0.08000592142343521\n",
      "Training loss: 0.08133566379547119\n",
      "Training loss: 0.08172691613435745\n",
      "Training loss: 0.07532794028520584\n",
      "Training loss: 0.0787084698677063\n",
      "Training loss: 0.08282386511564255\n",
      "Training loss: 0.08697193115949631\n",
      "Training loss: 0.07985945791006088\n",
      "Training loss: 0.07970600575208664\n",
      "Training loss: 0.07742399722337723\n",
      "Training loss: 0.0827021673321724\n",
      "Training loss: 0.08010660111904144\n",
      "Training loss: 0.07740558683872223\n",
      "Training loss: 0.0845317468047142\n",
      "Training loss: 0.07970735430717468\n",
      "Training loss: 0.08644261211156845\n",
      "Training loss: 0.07878981530666351\n",
      "Training loss: 0.08121522516012192\n",
      "Training loss: 0.07492510229349136\n",
      "Training loss: 0.07645166665315628\n",
      "Training loss: 0.0904134213924408\n",
      "Training loss: 0.08045322448015213\n",
      "Training loss: 0.07667941600084305\n",
      "Training loss: 0.08095009624958038\n",
      "Training loss: 0.08034111559391022\n",
      "Training loss: 0.07831552624702454\n",
      "Training loss: 0.08626437187194824\n",
      "Training loss: 0.07760663330554962\n",
      "Training loss: 0.07837697863578796\n",
      "Training loss: 0.08345512300729752\n",
      "Training loss: 0.08234553039073944\n",
      "Training loss: 0.08606278151273727\n",
      "Training loss: 0.08171391487121582\n",
      "Training loss: 0.08712666481733322\n",
      "Training loss: 0.0734376534819603\n",
      "Training loss: 0.07760253548622131\n",
      "Training loss: 0.08299495279788971\n",
      "Training loss: 0.08165523409843445\n",
      "Training loss: 0.08072179555892944\n",
      "Training loss: 0.07895009964704514\n",
      "Training loss: 0.08194635808467865\n",
      "Training loss: 0.07665938884019852\n",
      "Training loss: 0.08084587007761002\n",
      "Training loss: 0.0796586275100708\n",
      "Training loss: 0.08883866667747498\n",
      "Training loss: 0.0758783146739006\n",
      "Training loss: 0.08013635873794556\n",
      "Training loss: 0.07777728885412216\n",
      "Training loss: 0.08504770696163177\n",
      "Training loss: 0.07461415231227875\n",
      "Training loss: 0.08009859174489975\n",
      "Training loss: 0.1057596206665039\n",
      "Training loss: 0.08342146128416061\n",
      "Training loss: 0.08335792273283005\n",
      "Training loss: 0.08742321282625198\n",
      "Training loss: 0.08566784113645554\n",
      "Training loss: 0.07782818377017975\n",
      "Training loss: 0.08396146446466446\n",
      "Training loss: 0.07796221971511841\n",
      "Training loss: 0.07769303768873215\n",
      "Training loss: 0.08719519525766373\n",
      "Training loss: 0.08159967511892319\n",
      "Training loss: 0.08235623687505722\n",
      "Training loss: 0.08087660372257233\n",
      "Training loss: 0.08493538945913315\n",
      "Training loss: 0.08026669174432755\n",
      "Training loss: 0.08087969571352005\n",
      "Training loss: 0.07769367843866348\n",
      "Training loss: 0.08027490973472595\n",
      "Training loss: 0.08399677276611328\n",
      "Training loss: 0.08450692147016525\n",
      "Training loss: 0.07702864706516266\n",
      "Training loss: 0.07644708454608917\n",
      "Training loss: 0.08126937597990036\n",
      "Training loss: 0.08184105157852173\n",
      "Training loss: 0.08232033997774124\n",
      "Training loss: 0.08838146924972534\n",
      "Training loss: 0.07599273324012756\n",
      "Training loss: 0.07941925525665283\n",
      "Training loss: 0.07666051387786865\n",
      "Training loss: 0.08470822870731354\n",
      "Training loss: 0.07751420140266418\n",
      "Training loss: 0.07768511772155762\n",
      "Training loss: 0.0895414873957634\n",
      "Training loss: 0.08323080837726593\n",
      "Training loss: 0.0841042622923851\n",
      "Training loss: 0.08070939779281616\n",
      "Training loss: 0.08558216691017151\n",
      "Training loss: 0.07789669930934906\n",
      "Training loss: 0.0799015611410141\n",
      "Training loss: 0.08191812038421631\n",
      "Training loss: 0.07818887382745743\n",
      "Training loss: 0.0823218822479248\n",
      "Training loss: 0.08266549557447433\n",
      "Training loss: 0.08570479601621628\n",
      "Training loss: 0.07912997901439667\n",
      "Training loss: 0.07856256514787674\n",
      "Training loss: 0.07624503970146179\n",
      "Training loss: 0.07610570639371872\n",
      "Training loss: 0.08502655476331711\n",
      "Training loss: 0.08299492299556732\n",
      "Training loss: 0.07718352973461151\n",
      "Training loss: 0.08093871921300888\n",
      "Training loss: 0.07847712188959122\n",
      "Training loss: 0.0846400037407875\n",
      "Training loss: 0.08561902493238449\n",
      "Training loss: 0.084768146276474\n",
      "Training loss: 0.08739050477743149\n",
      "Training loss: 0.08084747195243835\n",
      "Training loss: 0.08351054787635803\n",
      "Training loss: 0.0793260782957077\n",
      "Training loss: 0.08681321889162064\n",
      "Training loss: 0.08590277284383774\n",
      "Training loss: 0.0906306728720665\n",
      "Training loss: 0.08672308176755905\n",
      "Training loss: 0.08790533244609833\n",
      "Training loss: 0.08224371820688248\n",
      "Training loss: 0.08045157045125961\n",
      "Training loss: 0.08620341122150421\n",
      "Training loss: 0.08434038609266281\n",
      "Training loss: 0.07584734261035919\n",
      "Training loss: 0.08535102009773254\n",
      "Training loss: 0.08037131279706955\n",
      "Training loss: 0.08337405323982239\n",
      "Training loss: 0.08426162600517273\n",
      "Training loss: 0.08266868442296982\n",
      "Training loss: 0.07875006645917892\n",
      "Training loss: 0.07829462736845016\n",
      "Training loss: 0.08946041017770767\n",
      "Training loss: 0.07639774680137634\n",
      "Training loss: 0.09044992178678513\n",
      "Training loss: 0.08396092057228088\n",
      "Training loss: 0.08426118642091751\n",
      "Training loss: 0.08239035308361053\n",
      "Training loss: 0.0809403583407402\n",
      "Training loss: 0.07990040630102158\n",
      "Training loss: 0.0836082175374031\n",
      "Training loss: 0.08339548110961914\n",
      "Training loss: 0.08671443164348602\n",
      "Training loss: 0.07804598659276962\n",
      "Training loss: 0.08116290718317032\n",
      "Training loss: 0.08040722459554672\n",
      "Training loss: 0.09294035285711288\n",
      "Training loss: 0.08850410580635071\n",
      "Training loss: 0.08224651962518692\n",
      "Training loss: 0.08491457998752594\n",
      "Training loss: 0.08655860275030136\n",
      "Training loss: 0.08168362081050873\n",
      "Training loss: 0.09045198559761047\n",
      "Training loss: 0.08864950388669968\n",
      "Training loss: 0.0833168774843216\n",
      "Training loss: 0.0878169983625412\n",
      "Training loss: 0.08090271055698395\n",
      "Training loss: 0.08838611096143723\n",
      "Training loss: 0.081950344145298\n",
      "Training loss: 0.08161265403032303\n",
      "Training loss: 0.08253678679466248\n",
      "Training loss: 0.07577639818191528\n",
      "Training loss: 0.08349721878767014\n",
      "Training loss: 0.08280334621667862\n",
      "Training loss: 0.07780933380126953\n",
      "Training loss: 0.08720173686742783\n",
      "Training loss: 0.08424658328294754\n",
      "Training loss: 0.08206120133399963\n",
      "Training loss: 0.07667309045791626\n",
      "Training loss: 0.08291257172822952\n",
      "Training loss: 0.0832957774400711\n",
      "Training loss: 0.086342453956604\n",
      "Training loss: 0.08746188133955002\n",
      "Training loss: 0.08568579703569412\n",
      "Training loss: 0.09044380486011505\n",
      "Training loss: 0.08986421674489975\n",
      "Training loss: 0.08286851644515991\n",
      "Training loss: 0.0812671110033989\n",
      "Training loss: 0.08869943022727966\n",
      "Training loss: 0.08262269198894501\n",
      "Training loss: 0.08577539771795273\n",
      "Training loss: 0.08368692547082901\n",
      "Training loss: 0.0823599249124527\n",
      "Training loss: 0.08431887626647949\n",
      "Training loss: 0.08820585906505585\n",
      "Training loss: 0.09284000098705292\n",
      "Training loss: 0.08557523041963577\n",
      "Training loss: 0.07988104969263077\n",
      "Training loss: 0.0870608538389206\n",
      "Training loss: 0.07761658728122711\n",
      "Training loss: 0.08306486159563065\n",
      "Training loss: 0.08049768209457397\n",
      "Training loss: 0.07964464277029037\n",
      "Training loss: 0.08471407741308212\n",
      "Training loss: 0.0815545916557312\n",
      "Training loss: 0.07540059089660645\n",
      "Training loss: 0.07861924171447754\n",
      "Training loss: 0.07776501029729843\n",
      "Training loss: 0.08146452158689499\n",
      "Training loss: 0.08580867946147919\n",
      "Training loss: 0.09150959551334381\n",
      "Training loss: 0.08019443601369858\n",
      "Training loss: 0.08039127290248871\n",
      "Training loss: 0.07900362461805344\n",
      "Training loss: 0.08222387731075287\n",
      "Training loss: 0.07793281972408295\n",
      "Training loss: 0.08228238672018051\n",
      "Training loss: 0.08334089070558548\n",
      "Training loss: 0.08063416928052902\n",
      "Training loss: 0.0864492729306221\n",
      "Training loss: 0.07779005914926529\n",
      "Training loss: 0.07733645290136337\n",
      "Training loss: 0.08272095769643784\n",
      "Training loss: 0.08297188580036163\n",
      "Training loss: 0.07028467208147049\n",
      "Training loss: 0.08819558471441269\n",
      "Training loss: 0.07588128000497818\n",
      "Training loss: 0.08610823005437851\n",
      "Training loss: 0.08104552328586578\n",
      "Training loss: 0.0865652933716774\n",
      "Training loss: 0.08794110268354416\n",
      "Training loss: 0.07837960124015808\n",
      "Training loss: 0.08265403658151627\n",
      "Training loss: 0.08136820793151855\n",
      "Training loss: 0.0781598836183548\n",
      "Training loss: 0.08425546437501907\n",
      "Training loss: 0.07319944351911545\n",
      "Training loss: 0.07921712845563889\n",
      "Training loss: 0.077618308365345\n",
      "Training loss: 0.07667906582355499\n",
      "Training loss: 0.07563064992427826\n",
      "Training loss: 0.08950167894363403\n",
      "Training loss: 0.08079390972852707\n",
      "Training loss: 0.07933206111192703\n",
      "Training loss: 0.07248683273792267\n",
      "Training loss: 0.07766962051391602\n",
      "Training loss: 0.07437547296285629\n",
      "Training loss: 0.0791800245642662\n",
      "Training loss: 0.07973182201385498\n",
      "Training loss: 0.08092612028121948\n",
      "Training loss: 0.07327096164226532\n",
      "Training loss: 0.08714985102415085\n",
      "Training loss: 0.08097552508115768\n",
      "Training loss: 0.08037617057561874\n",
      "Training loss: 0.08384887874126434\n",
      "Training loss: 0.0810164287686348\n",
      "Training loss: 0.08156470209360123\n",
      "Training loss: 0.076383076608181\n",
      "Training loss: 0.07983437925577164\n",
      "Training loss: 0.07840075343847275\n",
      "Training loss: 0.08010304719209671\n",
      "Training loss: 0.07712087780237198\n",
      "Training loss: 0.07961227744817734\n",
      "Training loss: 0.0810864269733429\n",
      "Training loss: 0.08413274586200714\n",
      "Training loss: 0.0736062154173851\n",
      "Training loss: 0.08059706538915634\n",
      "Training loss: 0.07973746955394745\n",
      "Training loss: 0.07829597592353821\n",
      "Training loss: 0.08236588537693024\n",
      "Training loss: 0.08721652626991272\n",
      "Training loss: 0.080155149102211\n",
      "Training loss: 0.08660656213760376\n",
      "Training loss: 0.08779771625995636\n",
      "Training loss: 0.07983369380235672\n",
      "Training loss: 0.08350098133087158\n",
      "Training loss: 0.08499301970005035\n",
      "Training loss: 0.08330226689577103\n",
      "Training loss: 0.080254927277565\n",
      "Training loss: 0.08304066956043243\n",
      "Training loss: 0.08590710908174515\n",
      "Training loss: 0.08112694323062897\n",
      "Training loss: 0.08576434850692749\n",
      "Training loss: 0.08548828214406967\n",
      "Training loss: 0.08251833915710449\n",
      "Training loss: 0.08346324414014816\n",
      "Training loss: 0.07811769843101501\n",
      "Training loss: 0.08733144402503967\n",
      "Training loss: 0.0827956423163414\n",
      "Training loss: 0.0838664248585701\n",
      "Training loss: 0.08148827403783798\n",
      "Training loss: 0.0834113210439682\n",
      "Training loss: 0.08667326718568802\n",
      "Training loss: 0.07850565016269684\n",
      "Training loss: 0.08400677889585495\n",
      "Training loss: 0.08306649327278137\n",
      "Training loss: 0.08144612610340118\n",
      "Training loss: 0.0821399837732315\n",
      "Training loss: 0.08552949130535126\n",
      "Training loss: 0.07957208156585693\n",
      "Training loss: 0.07625031471252441\n",
      "Training loss: 0.08058879524469376\n",
      "Training loss: 0.07784176617860794\n",
      "Training loss: 0.06919916719198227\n",
      "Training loss: 0.07719632238149643\n",
      "Training loss: 0.07636187225580215\n",
      "Training loss: 0.07578203082084656\n",
      "Training loss: 0.07790368050336838\n",
      "Training loss: 0.08590856194496155\n",
      "Training loss: 0.08438802510499954\n",
      "Training loss: 0.0823516845703125\n",
      "Training loss: 0.0800633504986763\n",
      "Training loss: 0.07682712376117706\n",
      "Training loss: 0.07915205508470535\n",
      "Training loss: 0.07610418647527695\n",
      "Training loss: 0.07837509363889694\n",
      "Training loss: 0.07111714780330658\n",
      "Training loss: 0.07849790155887604\n",
      "Training loss: 0.07355023920536041\n",
      "Training loss: 0.07849494367837906\n",
      "Training loss: 0.08100786805152893\n",
      "Training loss: 0.08259571343660355\n",
      "Training loss: 0.08426788449287415\n",
      "Training loss: 0.07629810273647308\n",
      "Training loss: 0.0789627730846405\n",
      "Training loss: 0.07967804372310638\n",
      "Training loss: 0.07685211300849915\n",
      "Training loss: 0.08182631433010101\n",
      "Training loss: 0.07906536012887955\n",
      "Training loss: 0.07128099352121353\n",
      "Training loss: 0.07545024901628494\n",
      "Training loss: 0.08968803286552429\n",
      "Training loss: 0.07477926462888718\n",
      "Training loss: 0.06953218579292297\n",
      "Training loss: 0.07280228286981583\n",
      "Training loss: 0.07504592835903168\n",
      "Training loss: 0.07414542138576508\n",
      "Training loss: 0.08086112886667252\n",
      "Training loss: 0.0729297623038292\n",
      "Training loss: 0.07300075143575668\n",
      "Training loss: 0.08742162585258484\n",
      "Training loss: 0.07767286896705627\n",
      "Training loss: 0.08338775485754013\n",
      "Training loss: 0.07164373993873596\n",
      "Training loss: 0.07683074474334717\n",
      "Training loss: 0.08089489489793777\n",
      "Training loss: 0.07308020442724228\n",
      "Training loss: 0.07433439046144485\n",
      "Training loss: 0.08443931490182877\n",
      "Training loss: 0.08348148316144943\n",
      "Training loss: 0.07748682051897049\n",
      "Training loss: 0.076411172747612\n",
      "Training loss: 0.07922841608524323\n",
      "Training loss: 0.0834503248333931\n",
      "Training loss: 0.07439903169870377\n",
      "Training loss: 0.08141225576400757\n",
      "Training loss: 0.08051200211048126\n",
      "Training loss: 0.07344549149274826\n",
      "Training loss: 0.08278346061706543\n",
      "Training loss: 0.08080292493104935\n",
      "Training loss: 0.07583107054233551\n",
      "Training loss: 0.07359614223241806\n",
      "Training loss: 0.07425838708877563\n",
      "Training loss: 0.07960078120231628\n",
      "Training loss: 0.07850969582796097\n",
      "Training loss: 0.07893367856740952\n",
      "Training loss: 0.07441093027591705\n",
      "Training loss: 0.08225066214799881\n",
      "Training loss: 0.08019129931926727\n",
      "Training loss: 0.07655323296785355\n",
      "Training loss: 0.07372141629457474\n",
      "Training loss: 0.07424929738044739\n",
      "Training loss: 0.08072055131196976\n",
      "Training loss: 0.07857178896665573\n",
      "Training loss: 0.08540698140859604\n",
      "Training loss: 0.08351263403892517\n",
      "Training loss: 0.07612647116184235\n",
      "Training loss: 0.0856163278222084\n",
      "Training loss: 0.08316992968320847\n",
      "Training loss: 0.0789656788110733\n",
      "Training loss: 0.07796673476696014\n",
      "Training loss: 0.07481992989778519\n",
      "Training loss: 0.07980311661958694\n",
      "Training loss: 0.07819900661706924\n",
      "Training loss: 0.08233615756034851\n",
      "Training loss: 0.07879433780908585\n",
      "Training loss: 0.07918014377355576\n",
      "Training loss: 0.08535870909690857\n",
      "Training loss: 0.08205181360244751\n",
      "Training loss: 0.08241050690412521\n",
      "Training loss: 0.07575283199548721\n",
      "Training loss: 0.07596614211797714\n",
      "Training loss: 0.0807189792394638\n",
      "Training loss: 0.07563287019729614\n",
      "Training loss: 0.08125852048397064\n",
      "Training loss: 0.07367656379938126\n",
      "Training loss: 0.08249218016862869\n",
      "Training loss: 0.07977859675884247\n",
      "Training loss: 0.07462145388126373\n",
      "Training loss: 0.08146483451128006\n",
      "Training loss: 0.07535340636968613\n",
      "Training loss: 0.07574748247861862\n",
      "Training loss: 0.07308285683393478\n",
      "Training loss: 0.0802755355834961\n",
      "Training loss: 0.07686130702495575\n",
      "Training loss: 0.08269809931516647\n",
      "Training loss: 0.07875090092420578\n",
      "Training loss: 0.08312501758337021\n",
      "Training loss: 0.07890048623085022\n",
      "Training loss: 0.07827234268188477\n",
      "Training loss: 0.08090516179800034\n",
      "Training loss: 0.06997502595186234\n",
      "Training loss: 0.07891782373189926\n",
      "Training loss: 0.08025481551885605\n",
      "Training loss: 0.07406740635633469\n",
      "Training loss: 0.07931527495384216\n",
      "Training loss: 0.07377203553915024\n",
      "Training loss: 0.07988601177930832\n",
      "Training loss: 0.07255171239376068\n",
      "Training loss: 0.08073720335960388\n",
      "Training loss: 0.08248542994260788\n",
      "Training loss: 0.07803533226251602\n",
      "Training loss: 0.08456116914749146\n",
      "Training loss: 0.08294667303562164\n",
      "Training loss: 0.08078954368829727\n",
      "Training loss: 0.07545458525419235\n",
      "Training loss: 0.0844339057803154\n",
      "Training loss: 0.0738992691040039\n",
      "Training loss: 0.0802718922495842\n",
      "Training loss: 0.0761621743440628\n",
      "Training loss: 0.08878349512815475\n",
      "Training loss: 0.0790654644370079\n",
      "Training loss: 0.07862945646047592\n",
      "Training loss: 0.0744972676038742\n",
      "Training loss: 0.07983566075563431\n",
      "Training loss: 0.08205710351467133\n",
      "Training loss: 0.08173570036888123\n",
      "Training loss: 0.07586514949798584\n",
      "Training loss: 0.07966317236423492\n",
      "Training loss: 0.08510389924049377\n",
      "Training loss: 0.08253951370716095\n",
      "Training loss: 0.07971559464931488\n",
      "Training loss: 0.07433083653450012\n",
      "Training loss: 0.07909958809614182\n",
      "Training loss: 0.08257303386926651\n",
      "Training loss: 0.08021936565637589\n",
      "Training loss: 0.0774788111448288\n",
      "Training loss: 0.08884462714195251\n",
      "Training loss: 0.07973035424947739\n",
      "Training loss: 0.07949309796094894\n",
      "Training loss: 0.07742767781019211\n",
      "Training loss: 0.07979261130094528\n",
      "Training loss: 0.078357994556427\n",
      "Training loss: 0.0724145844578743\n",
      "Training loss: 0.07759900391101837\n",
      "Training loss: 0.07760133594274521\n",
      "Training loss: 0.08042177557945251\n",
      "Training loss: 0.07799969613552094\n",
      "Training loss: 0.07644520699977875\n",
      "Training loss: 0.0832684338092804\n",
      "Training loss: 0.07909785211086273\n",
      "Training loss: 0.0797179713845253\n",
      "Training loss: 0.08596666157245636\n",
      "Training loss: 0.0768287181854248\n",
      "Training loss: 0.07765953242778778\n",
      "Training loss: 0.07547955214977264\n",
      "Training loss: 0.08261150866746902\n",
      "Training loss: 0.08343604952096939\n",
      "Training loss: 0.07891204953193665\n",
      "Training loss: 0.07502685487270355\n",
      "Training loss: 0.07907938957214355\n",
      "Training loss: 0.08186211436986923\n",
      "Training loss: 0.07842938601970673\n",
      "Training loss: 0.07914603501558304\n",
      "Training loss: 0.07750903815031052\n",
      "Training loss: 0.07655733078718185\n",
      "Training loss: 0.07398882508277893\n",
      "Training loss: 0.07626358419656754\n",
      "Training loss: 0.07729416340589523\n",
      "Training loss: 0.07524621486663818\n",
      "Training loss: 0.07464896142482758\n",
      "Training loss: 0.07803630828857422\n",
      "Training loss: 0.07564854621887207\n",
      "Training loss: 0.08061487227678299\n",
      "Training loss: 0.07136663049459457\n",
      "Training loss: 0.07548122853040695\n",
      "Training loss: 0.08448996394872665\n",
      "Training loss: 0.0774223729968071\n",
      "Training loss: 0.07742991298437119\n",
      "Training loss: 0.071185402572155\n",
      "Training loss: 0.07742670178413391\n",
      "Training loss: 0.07795470207929611\n",
      "Training loss: 0.08133307099342346\n",
      "Training loss: 0.07851935923099518\n",
      "Training loss: 0.07166741043329239\n",
      "Training loss: 0.07620067894458771\n",
      "Training loss: 0.07545828074216843\n",
      "Training loss: 0.08085509389638901\n",
      "Training loss: 0.07308255881071091\n",
      "Training loss: 0.07395040988922119\n",
      "Training loss: 0.07915139198303223\n",
      "Training loss: 0.075640968978405\n",
      "Training loss: 0.0827191099524498\n",
      "Training loss: 0.07852556556463242\n",
      "Training loss: 0.08336468040943146\n",
      "Training loss: 0.08580300956964493\n",
      "Training loss: 0.07964298874139786\n",
      "Training loss: 0.08115392923355103\n",
      "Training loss: 0.08253783732652664\n",
      "Training loss: 0.0884832814335823\n",
      "Training loss: 0.08290806412696838\n",
      "Training loss: 0.08126897364854813\n",
      "Training loss: 0.08680400252342224\n",
      "Training loss: 0.08618742227554321\n",
      "Training loss: 0.0845077708363533\n",
      "Training loss: 0.0742320567369461\n",
      "Training loss: 0.0856688916683197\n",
      "Training loss: 0.0815582200884819\n",
      "Training loss: 0.08351380378007889\n",
      "Training loss: 0.07734696567058563\n",
      "Training loss: 0.08085690438747406\n",
      "Training loss: 0.07931271195411682\n",
      "Training loss: 0.08347159624099731\n",
      "Training loss: 0.08129600435495377\n",
      "Training loss: 0.08543770015239716\n",
      "Training loss: 0.07977785170078278\n",
      "Training loss: 0.0813031867146492\n",
      "Training loss: 0.0787627100944519\n",
      "Training loss: 0.08119498938322067\n",
      "Training loss: 0.07860849797725677\n",
      "Training loss: 0.07647353410720825\n",
      "Training loss: 0.07897696644067764\n",
      "Training loss: 0.07846933603286743\n",
      "Training loss: 0.07254674285650253\n",
      "Training loss: 0.08201181888580322\n",
      "Training loss: 0.07671349495649338\n",
      "Training loss: 0.07693921774625778\n",
      "Training loss: 0.08709744364023209\n",
      "Training loss: 0.07222425937652588\n",
      "Training loss: 0.08403956890106201\n",
      "Training loss: 0.07838748395442963\n",
      "Training loss: 0.0744662880897522\n",
      "Training loss: 0.07699526101350784\n",
      "Training loss: 0.08070830255746841\n",
      "Training loss: 0.08295472711324692\n",
      "Training loss: 0.07748641818761826\n",
      "Training loss: 0.07773677259683609\n",
      "Training loss: 0.0875403881072998\n",
      "Training loss: 0.07612397521734238\n",
      "Training loss: 0.07735835760831833\n",
      "Training loss: 0.07735446095466614\n",
      "Training loss: 0.08710535615682602\n",
      "Training loss: 0.07118766754865646\n",
      "Training loss: 0.07925973832607269\n",
      "Training loss: 0.07122760266065598\n",
      "Training loss: 0.08085523545742035\n",
      "Training loss: 0.07571430504322052\n",
      "Training loss: 0.07370606809854507\n",
      "Training loss: 0.07313752174377441\n",
      "Training loss: 0.07685422152280807\n",
      "Training loss: 0.0773400366306305\n",
      "Training loss: 0.0731901228427887\n",
      "Training loss: 0.07893753796815872\n",
      "Training loss: 0.07311514019966125\n",
      "Training loss: 0.07526668161153793\n",
      "Training loss: 0.07598273456096649\n",
      "Training loss: 0.0817570611834526\n",
      "Training loss: 0.07556158304214478\n",
      "Training loss: 0.07421275228261948\n",
      "Training loss: 0.072126105427742\n",
      "Training loss: 0.07384639978408813\n",
      "Training loss: 0.06342100352048874\n",
      "Training loss: 0.0781530886888504\n",
      "Training loss: 0.0752599835395813\n",
      "Training loss: 0.08148030191659927\n",
      "Training loss: 0.07473712414503098\n",
      "Training loss: 0.0789557546377182\n",
      "Training loss: 0.07829509675502777\n",
      "Training loss: 0.07737011462450027\n",
      "Training loss: 0.07427165657281876\n",
      "Training loss: 0.08538676798343658\n",
      "Training loss: 0.07660495489835739\n",
      "Training loss: 0.08164678514003754\n",
      "Training loss: 0.07198852300643921\n",
      "Training loss: 0.07905307412147522\n",
      "Training loss: 0.07830184698104858\n",
      "Training loss: 0.0813809335231781\n",
      "Training loss: 0.07525692880153656\n",
      "Training loss: 0.08529022336006165\n",
      "Training loss: 0.08347923308610916\n",
      "Training loss: 0.07184049487113953\n",
      "Training loss: 0.07376541197299957\n",
      "Training loss: 0.08044510334730148\n",
      "Training loss: 0.07114839553833008\n",
      "Training loss: 0.07784577459096909\n",
      "Training loss: 0.07910832017660141\n",
      "Training loss: 0.08243785798549652\n",
      "Training loss: 0.07949338108301163\n",
      "Training loss: 0.07797208428382874\n",
      "Training loss: 0.07902862876653671\n",
      "Training loss: 0.07617735117673874\n",
      "Training loss: 0.07715612649917603\n",
      "Training loss: 0.07659099996089935\n",
      "Training loss: 0.07346545904874802\n",
      "Training loss: 0.0810452327132225\n",
      "Training loss: 0.08072781562805176\n",
      "Training loss: 0.0763809010386467\n",
      "Training loss: 0.07516591250896454\n",
      "Training loss: 0.07122166454792023\n",
      "Training loss: 0.07439059019088745\n",
      "Training loss: 0.07510502636432648\n",
      "Training loss: 0.0725482776761055\n",
      "Training loss: 0.07360472530126572\n",
      "Training loss: 0.07314222306013107\n",
      "Training loss: 0.07877743989229202\n",
      "Training loss: 0.07264887541532516\n",
      "Training loss: 0.07935275137424469\n",
      "Training loss: 0.075820192694664\n",
      "Training loss: 0.06814287602901459\n",
      "Training loss: 0.07276759296655655\n",
      "Training loss: 0.07196763157844543\n",
      "Training loss: 0.0749988704919815\n",
      "Training loss: 0.07185017317533493\n",
      "Training loss: 0.07569745928049088\n",
      "Training loss: 0.07524830102920532\n",
      "Training loss: 0.07590612024068832\n",
      "Training loss: 0.07447671890258789\n",
      "Training loss: 0.07675865292549133\n",
      "Training loss: 0.07591508328914642\n",
      "Training loss: 0.07368328422307968\n",
      "Training loss: 0.07051006704568863\n",
      "Training loss: 0.07696492969989777\n",
      "Training loss: 0.07640095055103302\n",
      "Training loss: 0.07307431101799011\n",
      "Training loss: 0.08382923156023026\n",
      "Training loss: 0.07681291550397873\n",
      "Training loss: 0.07819392532110214\n",
      "Training loss: 0.07103671133518219\n",
      "Training loss: 0.07661766558885574\n",
      "Training loss: 0.0778728500008583\n",
      "Training loss: 0.07393214851617813\n",
      "Training loss: 0.07441044598817825\n",
      "Training loss: 0.07903566211462021\n",
      "Training loss: 0.07328031957149506\n",
      "Training loss: 0.07652447372674942\n",
      "Training loss: 0.0761558786034584\n",
      "Training loss: 0.07818133383989334\n",
      "Training loss: 0.07464742660522461\n",
      "Training loss: 0.07387308776378632\n",
      "Training loss: 0.07361428439617157\n",
      "Training loss: 0.07781344652175903\n",
      "Training loss: 0.07431092858314514\n",
      "Training loss: 0.07973712682723999\n",
      "Training loss: 0.07831383496522903\n",
      "Training loss: 0.0726965144276619\n",
      "Training loss: 0.0758783295750618\n",
      "Training loss: 0.07049693167209625\n",
      "Training loss: 0.07543927431106567\n",
      "Training loss: 0.07570093125104904\n",
      "Training loss: 0.08164314180612564\n",
      "Training loss: 0.08395341038703918\n",
      "Training loss: 0.07754889130592346\n",
      "Training loss: 0.08410172909498215\n",
      "Training loss: 0.07803564518690109\n",
      "Training loss: 0.08034905791282654\n",
      "Training loss: 0.07668386399745941\n",
      "Training loss: 0.07423209398984909\n",
      "Training loss: 0.08668957650661469\n",
      "Training loss: 0.07969455420970917\n",
      "Training loss: 0.07850085943937302\n",
      "Training loss: 0.07714525610208511\n",
      "Training loss: 0.07453306764364243\n",
      "Training loss: 0.07722848653793335\n",
      "Training loss: 0.07290461659431458\n",
      "Training loss: 0.08059311658143997\n",
      "Training loss: 0.0766541063785553\n",
      "Training loss: 0.07645006477832794\n",
      "Training loss: 0.07148711383342743\n",
      "Training loss: 0.07742099463939667\n",
      "Training loss: 0.07523169368505478\n",
      "Training loss: 0.0758773684501648\n",
      "Training loss: 0.08024550974369049\n",
      "Training loss: 0.0743715912103653\n",
      "Training loss: 0.07387650012969971\n",
      "Training loss: 0.07862561196088791\n",
      "Training loss: 0.07896237075328827\n",
      "Training loss: 0.07952482998371124\n",
      "Training loss: 0.08146510273218155\n",
      "Training loss: 0.0788177028298378\n",
      "Training loss: 0.07347158342599869\n",
      "Training loss: 0.07282715290784836\n",
      "Training loss: 0.07786981016397476\n",
      "Training loss: 0.07770632207393646\n",
      "Training loss: 0.06962557137012482\n",
      "Training loss: 0.07427052408456802\n",
      "Training loss: 0.0726134330034256\n",
      "Training loss: 0.07551171630620956\n",
      "Training loss: 0.07426207512617111\n",
      "Training loss: 0.07698708772659302\n",
      "Training loss: 0.07309208810329437\n",
      "Training loss: 0.07064102590084076\n",
      "Training loss: 0.07542791962623596\n",
      "Training loss: 0.08184371143579483\n",
      "Training loss: 0.07713836431503296\n",
      "Training loss: 0.07585527002811432\n",
      "Training loss: 0.07520651072263718\n",
      "Training loss: 0.08052118122577667\n",
      "Training loss: 0.07261411100625992\n",
      "Training loss: 0.07333464920520782\n",
      "Training loss: 0.0728466734290123\n",
      "Training loss: 0.07032966613769531\n",
      "Training loss: 0.07276008278131485\n",
      "Training loss: 0.07698781043291092\n",
      "Training loss: 0.0721304640173912\n",
      "Training loss: 0.07422041893005371\n",
      "Training loss: 0.08227166533470154\n",
      "Training loss: 0.07811862230300903\n",
      "Training loss: 0.08860210329294205\n",
      "Training loss: 0.07695358991622925\n",
      "Training loss: 0.07416863739490509\n",
      "Training loss: 0.0808105543255806\n",
      "Training loss: 0.07640188187360764\n",
      "Training loss: 0.07817437499761581\n",
      "Training loss: 0.0710001289844513\n",
      "Training loss: 0.14100530743598938\n",
      "Training loss: 0.07531261444091797\n",
      "Training loss: 0.0829416811466217\n",
      "Training loss: 0.07823416590690613\n",
      "Training loss: 0.087189681828022\n",
      "Training loss: 0.08210324496030807\n",
      "Training loss: 0.07724785804748535\n",
      "Training loss: 0.0762232318520546\n",
      "Training loss: 0.08022163808345795\n",
      "Training loss: 0.0775780975818634\n",
      "Training loss: 0.08635332435369492\n",
      "Training loss: 0.07752715796232224\n",
      "Training loss: 0.07850157469511032\n",
      "Training loss: 0.0879102498292923\n",
      "Training loss: 0.07802481204271317\n",
      "Training loss: 0.07941535860300064\n",
      "Training loss: 0.07749748975038528\n",
      "Training loss: 0.07804713398218155\n",
      "Training loss: 0.07930994778871536\n",
      "Training loss: 0.08175244927406311\n",
      "Training loss: 0.07851267606019974\n",
      "Training loss: 0.07356350123882294\n",
      "Training loss: 0.07121799886226654\n",
      "Training loss: 0.07573136687278748\n",
      "Training loss: 0.0737173780798912\n",
      "Training loss: 0.07552555948495865\n",
      "Training loss: 0.0802503228187561\n",
      "Training loss: 0.07281678915023804\n",
      "Training loss: 0.073550745844841\n",
      "Training loss: 0.08455662429332733\n",
      "Training loss: 0.07773533463478088\n",
      "Training loss: 0.0773816779255867\n",
      "Training loss: 0.08022327721118927\n",
      "Training loss: 0.07909493893384933\n",
      "Training loss: 0.07855034619569778\n",
      "Training loss: 0.077126145362854\n",
      "Training loss: 0.07720434665679932\n",
      "Training loss: 0.08030165731906891\n",
      "Training loss: 0.08335379511117935\n",
      "Training loss: 0.07761046290397644\n",
      "Training loss: 0.07297442108392715\n",
      "Training loss: 0.0758846327662468\n",
      "Training loss: 0.07783563435077667\n",
      "Training loss: 0.08305131644010544\n",
      "Training loss: 0.07424937933683395\n",
      "Training loss: 0.07328244298696518\n",
      "Training loss: 0.07137323170900345\n",
      "Training loss: 0.0783471018075943\n",
      "Training loss: 0.07745903730392456\n",
      "Training loss: 0.08133794367313385\n",
      "Training loss: 0.08294040709733963\n",
      "Training loss: 0.0785786509513855\n",
      "Training loss: 0.08033706992864609\n",
      "Training loss: 0.07381201535463333\n",
      "Training loss: 0.08190842717885971\n",
      "Training loss: 0.07670135796070099\n",
      "Training loss: 0.07116071879863739\n",
      "Training loss: 0.07211173325777054\n",
      "Training loss: 0.07818637788295746\n",
      "Training loss: 0.10754288733005524\n",
      "Training loss: 0.07950150221586227\n",
      "Training loss: 0.07450228184461594\n",
      "Training loss: 0.07529407739639282\n",
      "Training loss: 0.06997033208608627\n",
      "Training loss: 0.07396632432937622\n",
      "Training loss: 0.07649975270032883\n",
      "Training loss: 0.07594965398311615\n",
      "Training loss: 0.07544056326150894\n",
      "Training loss: 0.07722220569849014\n",
      "Training loss: 0.06752205640077591\n",
      "Training loss: 0.07789172232151031\n",
      "Training loss: 0.08102606236934662\n",
      "Training loss: 0.0779009759426117\n",
      "Training loss: 0.07492339611053467\n",
      "Training loss: 0.07549452036619186\n",
      "Training loss: 0.07684707641601562\n",
      "Training loss: 0.07310253381729126\n",
      "Training loss: 0.07761825621128082\n",
      "Training loss: 0.07668370008468628\n",
      "Training loss: 0.07377453148365021\n",
      "Training loss: 0.08525402843952179\n",
      "Training loss: 0.06995019316673279\n",
      "Training loss: 0.07606518268585205\n",
      "Training loss: 0.07832171767950058\n",
      "Training loss: 0.07837127894163132\n",
      "Training loss: 0.07920292019844055\n",
      "Training loss: 0.07392209768295288\n",
      "Training loss: 0.07330597192049026\n",
      "Training loss: 0.07460367679595947\n",
      "Training loss: 0.0776166319847107\n",
      "Training loss: 0.08323188126087189\n",
      "Training loss: 0.07851368188858032\n",
      "Training loss: 0.0726216733455658\n",
      "Training loss: 0.07588141411542892\n",
      "Training loss: 0.08018752187490463\n",
      "Training loss: 0.0724434033036232\n",
      "Training loss: 0.08015469461679459\n",
      "Training loss: 0.07636818289756775\n",
      "Training loss: 0.07602964341640472\n",
      "Training loss: 0.07473780959844589\n",
      "Training loss: 0.07624198496341705\n",
      "Training loss: 0.07320646196603775\n",
      "Training loss: 0.0725930705666542\n",
      "Training loss: 0.07841651886701584\n",
      "Training loss: 0.07475340366363525\n",
      "Training loss: 0.07415373623371124\n",
      "Training loss: 0.08835369348526001\n",
      "Training loss: 0.07688271999359131\n",
      "Training loss: 0.07458759099245071\n",
      "Training loss: 0.07751429826021194\n",
      "Training loss: 0.07076549530029297\n",
      "Training loss: 0.08503971993923187\n",
      "Training loss: 0.07965195178985596\n",
      "Training loss: 0.07628017663955688\n",
      "Training loss: 0.08602824807167053\n",
      "Training loss: 0.07870865613222122\n",
      "Training loss: 0.07765346020460129\n",
      "Training loss: 0.08095593750476837\n",
      "Training loss: 0.0768790990114212\n",
      "Training loss: 0.07576663047075272\n",
      "Training loss: 0.077597476541996\n",
      "Training loss: 0.0734187662601471\n",
      "Training loss: 0.07762102782726288\n",
      "Training loss: 0.08123660087585449\n",
      "Training loss: 0.07716678082942963\n",
      "Training loss: 0.07977720350027084\n",
      "Training loss: 0.0769122913479805\n",
      "Training loss: 0.07660134136676788\n",
      "Training loss: 0.07451862096786499\n",
      "Training loss: 0.0770726427435875\n",
      "Training loss: 0.07702401280403137\n",
      "Training loss: 0.07240593433380127\n",
      "Training loss: 0.08003091812133789\n",
      "Training loss: 0.07730661332607269\n",
      "Training loss: 0.07267018407583237\n",
      "Training loss: 0.07752133160829544\n",
      "Training loss: 0.07760447263717651\n",
      "Training loss: 0.07413080334663391\n",
      "Training loss: 0.0771711990237236\n",
      "Training loss: 0.07674483209848404\n",
      "Training loss: 0.07810937613248825\n",
      "Training loss: 0.07583443075418472\n",
      "Training loss: 0.07844918966293335\n",
      "Training loss: 0.07772741466760635\n",
      "Training loss: 0.07606184482574463\n",
      "Training loss: 0.07502402365207672\n",
      "Training loss: 0.07603401690721512\n",
      "Training loss: 0.07865417003631592\n",
      "Training loss: 0.07307387888431549\n",
      "Training loss: 0.0737486407160759\n",
      "Training loss: 0.07074721902608871\n",
      "Training loss: 0.07338326424360275\n",
      "Training loss: 0.07313799858093262\n",
      "Training loss: 0.07492942363023758\n",
      "Training loss: 0.07356546074151993\n",
      "Training loss: 0.08057752996683121\n",
      "Training loss: 0.07439620792865753\n",
      "Training loss: 0.07849305868148804\n",
      "Training loss: 0.0728616714477539\n",
      "Training loss: 0.07462885230779648\n",
      "Training loss: 0.07576049119234085\n",
      "Training loss: 0.08274364471435547\n",
      "Training loss: 0.07377667725086212\n",
      "Training loss: 0.0833878442645073\n",
      "Training loss: 0.07677009701728821\n",
      "Training loss: 0.06891842931509018\n",
      "Training loss: 0.07222288101911545\n",
      "Training loss: 0.08081980794668198\n",
      "Training loss: 0.07728736847639084\n",
      "Training loss: 0.09365980327129364\n",
      "Training loss: 0.07723983377218246\n",
      "Training loss: 0.07042861729860306\n",
      "Training loss: 0.0699680894613266\n",
      "Training loss: 0.07325790077447891\n",
      "Training loss: 0.07647082209587097\n",
      "Training loss: 0.07484897971153259\n",
      "Training loss: 0.0715213418006897\n",
      "Training loss: 0.07369736582040787\n",
      "Training loss: 0.07811755686998367\n",
      "Training loss: 0.07573229819536209\n",
      "Training loss: 0.08170449733734131\n",
      "Training loss: 0.0725363940000534\n",
      "Training loss: 0.07300623506307602\n",
      "Training loss: 0.07663005590438843\n",
      "Training loss: 0.07481931895017624\n",
      "Training loss: 0.07487046718597412\n",
      "Training loss: 0.07235290855169296\n",
      "Training loss: 0.08010471612215042\n",
      "Training loss: 0.07594048976898193\n",
      "Training loss: 0.07740669697523117\n",
      "Training loss: 0.07374223321676254\n",
      "Training loss: 0.07211468368768692\n",
      "Training loss: 0.0756780207157135\n",
      "Training loss: 0.07692930102348328\n",
      "Training loss: 0.07607559114694595\n",
      "Training loss: 0.07521091401576996\n",
      "Training loss: 0.0815151110291481\n",
      "Training loss: 0.07408978044986725\n",
      "Training loss: 0.07360636442899704\n",
      "Training loss: 0.07013743370771408\n",
      "Training loss: 0.07681503891944885\n",
      "Training loss: 0.07751969248056412\n",
      "Training loss: 0.0777197927236557\n",
      "Training loss: 0.07324662804603577\n",
      "Training loss: 0.07639045268297195\n",
      "Training loss: 0.07794482260942459\n",
      "Training loss: 0.07406218349933624\n",
      "Training loss: 0.07234648615121841\n",
      "Training loss: 0.07022060453891754\n",
      "Training loss: 0.07018611580133438\n",
      "Training loss: 0.07150576263666153\n",
      "Training loss: 0.07576548308134079\n",
      "Training loss: 0.07474026083946228\n",
      "Training loss: 0.07457372546195984\n",
      "Training loss: 0.08297890424728394\n",
      "Training loss: 0.07179179787635803\n",
      "Training loss: 0.07602312415838242\n",
      "Training loss: 0.07359790056943893\n",
      "Training loss: 0.0767754465341568\n",
      "Training loss: 0.06760922074317932\n",
      "Training loss: 0.06891082972288132\n",
      "Training loss: 0.07163488864898682\n",
      "Training loss: 0.07317457348108292\n",
      "Training loss: 0.06668603420257568\n",
      "Training loss: 0.07948578894138336\n",
      "Training loss: 0.07822931557893753\n",
      "Training loss: 0.07406836748123169\n",
      "Training loss: 0.07551799714565277\n",
      "Training loss: 0.06921077519655228\n",
      "Training loss: 0.07250683754682541\n",
      "Training loss: 0.0692826509475708\n",
      "Training loss: 0.06919321417808533\n",
      "Training loss: 0.07530459016561508\n",
      "Training loss: 0.08152015507221222\n",
      "Training loss: 0.07739004492759705\n",
      "Training loss: 0.0662536546587944\n",
      "Training loss: 0.07645457983016968\n",
      "Training loss: 0.0760151743888855\n",
      "Training loss: 0.07128803431987762\n",
      "Training loss: 0.06818730384111404\n",
      "Training loss: 0.07223211228847504\n",
      "Training loss: 0.06903914362192154\n",
      "Training loss: 0.07191717624664307\n",
      "Training loss: 0.06940079480409622\n",
      "Training loss: 0.07463303953409195\n",
      "Training loss: 0.06785861402750015\n",
      "Training loss: 0.07548071444034576\n",
      "Training loss: 0.07492060214281082\n",
      "Training loss: 0.07574354857206345\n",
      "Training loss: 0.06589552760124207\n",
      "Training loss: 0.07051164656877518\n",
      "Training loss: 0.06866901367902756\n",
      "Training loss: 0.06974378228187561\n",
      "Training loss: 0.07064682245254517\n",
      "Training loss: 0.07393792271614075\n",
      "Training loss: 0.06965585798025131\n",
      "Training loss: 0.07028891891241074\n",
      "Training loss: 0.06942964345216751\n",
      "Training loss: 0.06859453022480011\n",
      "Training loss: 0.06681032478809357\n",
      "Training loss: 0.07150030881166458\n",
      "Training loss: 0.06657400727272034\n",
      "Training loss: 0.07461653649806976\n",
      "Training loss: 0.06916297972202301\n",
      "Training loss: 0.07000483572483063\n",
      "Training loss: 0.07224640995264053\n",
      "Training loss: 0.07458821684122086\n",
      "Training loss: 0.07535642385482788\n",
      "Training loss: 0.06718914210796356\n",
      "Training loss: 0.07450195401906967\n",
      "Training loss: 0.0708426684141159\n",
      "Training loss: 0.07086264342069626\n",
      "Training loss: 0.07288952171802521\n",
      "Training loss: 0.07187765836715698\n",
      "Training loss: 0.07424551993608475\n",
      "Training loss: 0.07450980693101883\n",
      "Training loss: 0.06626436114311218\n",
      "Training loss: 0.07488522678613663\n",
      "Training loss: 0.07325661927461624\n",
      "Training loss: 0.07982248812913895\n",
      "Training loss: 0.07693946361541748\n",
      "Training loss: 0.07132405787706375\n",
      "Training loss: 0.07274634391069412\n",
      "Training loss: 0.06946878135204315\n",
      "Training loss: 0.08077691495418549\n",
      "Training loss: 0.07082768529653549\n",
      "Training loss: 0.07328194379806519\n",
      "Training loss: 0.07078734785318375\n",
      "Training loss: 0.0687931478023529\n",
      "Training loss: 0.06918992847204208\n",
      "Training loss: 0.06550153344869614\n",
      "Training loss: 0.07486448436975479\n",
      "Training loss: 0.06951579451560974\n",
      "Training loss: 0.07421580702066422\n",
      "Training loss: 0.06834318488836288\n",
      "Training loss: 0.07335405796766281\n",
      "Training loss: 0.08069214224815369\n",
      "Training loss: 0.06871549785137177\n",
      "Training loss: 0.06953547149896622\n",
      "Training loss: 0.07068672776222229\n",
      "Training loss: 0.06989715993404388\n",
      "Training loss: 0.07044193148612976\n",
      "Training loss: 0.07017200440168381\n",
      "Training loss: 0.07297855615615845\n",
      "Training loss: 0.06787388771772385\n",
      "Training loss: 0.07226301729679108\n",
      "Training loss: 0.07365686446428299\n",
      "Training loss: 0.07133495062589645\n",
      "Training loss: 0.07766211777925491\n",
      "Training loss: 0.07056208699941635\n",
      "Training loss: 0.0653558000922203\n",
      "Training loss: 0.0755523219704628\n",
      "Training loss: 0.07336627691984177\n",
      "Training loss: 0.07338441908359528\n",
      "Training loss: 0.0744614452123642\n",
      "Training loss: 0.07268503308296204\n",
      "Training loss: 0.07465481758117676\n",
      "Training loss: 0.07161056995391846\n",
      "Training loss: 0.07577648013830185\n",
      "Training loss: 0.06725778430700302\n",
      "Training loss: 0.06976106762886047\n",
      "Training loss: 0.07228302955627441\n",
      "Training loss: 0.07640118151903152\n",
      "Training loss: 0.07309367507696152\n",
      "Training loss: 0.07414810359477997\n",
      "Training loss: 0.07720787823200226\n",
      "Training loss: 0.07544715702533722\n",
      "Training loss: 0.0820780023932457\n",
      "Training loss: 0.07467959821224213\n",
      "Training loss: 0.07220063358545303\n",
      "Training loss: 0.0722397118806839\n",
      "Training loss: 0.07658032327890396\n",
      "Training loss: 0.07474873960018158\n",
      "Training loss: 0.07415905594825745\n",
      "Training loss: 0.07118087261915207\n",
      "Training loss: 0.07886648923158646\n",
      "Training loss: 0.06938108056783676\n",
      "Training loss: 0.06989095360040665\n",
      "Training loss: 0.07789487391710281\n",
      "Training loss: 0.07013607025146484\n",
      "Training loss: 0.06761564314365387\n",
      "Training loss: 0.07111644744873047\n",
      "Training loss: 0.07604621350765228\n",
      "Training loss: 0.06953755766153336\n",
      "Training loss: 0.07274653762578964\n",
      "Training loss: 0.08153656125068665\n",
      "Training loss: 0.06745851784944534\n",
      "Training loss: 0.07007157057523727\n",
      "Training loss: 0.06994518637657166\n",
      "Training loss: 0.07042306661605835\n",
      "Training loss: 0.06684345006942749\n",
      "Training loss: 0.07219695299863815\n",
      "Training loss: 0.06968221068382263\n",
      "Training loss: 0.07374409586191177\n",
      "Training loss: 0.06463107466697693\n",
      "Training loss: 0.0680413767695427\n",
      "Training loss: 0.06819342821836472\n",
      "Training loss: 0.06740469485521317\n",
      "Training loss: 0.07055987417697906\n",
      "Training loss: 0.06938119977712631\n",
      "Training loss: 0.0710631012916565\n",
      "Training loss: 0.06694713234901428\n",
      "Training loss: 0.07551173865795135\n",
      "Training loss: 0.0813465416431427\n",
      "Training loss: 0.08339469134807587\n",
      "Training loss: 0.06568396836519241\n",
      "Training loss: 0.06900648027658463\n",
      "Training loss: 0.07096125930547714\n",
      "Training loss: 0.072920061647892\n",
      "Training loss: 0.07337149232625961\n",
      "Training loss: 0.08063069730997086\n",
      "Training loss: 0.07431702315807343\n",
      "Training loss: 0.07443387061357498\n",
      "Training loss: 0.08259091526269913\n",
      "Training loss: 0.0786106288433075\n",
      "Training loss: 0.07453251630067825\n",
      "Training loss: 0.07609480619430542\n",
      "Training loss: 0.07144194841384888\n",
      "Training loss: 0.07561753690242767\n",
      "Training loss: 0.07519526779651642\n",
      "Training loss: 0.07487814128398895\n",
      "Training loss: 0.07173200696706772\n",
      "Training loss: 0.07774186134338379\n",
      "Training loss: 0.07336606830358505\n",
      "Training loss: 0.07258980721235275\n",
      "Training loss: 0.06735079735517502\n",
      "Training loss: 0.06975497305393219\n",
      "Training loss: 0.07482605427503586\n",
      "Training loss: 0.07138946652412415\n",
      "Training loss: 0.06972033530473709\n",
      "Training loss: 0.07640685886144638\n",
      "Training loss: 0.0716460645198822\n",
      "Training loss: 0.07657225430011749\n",
      "Training loss: 0.07078518718481064\n",
      "Training loss: 0.06946910917758942\n",
      "Training loss: 0.07413700222969055\n",
      "Training loss: 0.06837508827447891\n",
      "Training loss: 0.06885302811861038\n",
      "Training loss: 0.06679408252239227\n",
      "Training loss: 0.06941258162260056\n",
      "Training loss: 0.0654149055480957\n",
      "Training loss: 0.07271166890859604\n",
      "Training loss: 0.07545717060565948\n",
      "Training loss: 0.07270213216543198\n",
      "Training loss: 0.07135047763586044\n",
      "Training loss: 0.0775766521692276\n",
      "Training loss: 0.07068869471549988\n",
      "Training loss: 0.07764890789985657\n",
      "Training loss: 0.07710126042366028\n",
      "Training loss: 0.07466448098421097\n",
      "Training loss: 0.07386016100645065\n",
      "Training loss: 0.07988360524177551\n",
      "Training loss: 0.06958173960447311\n",
      "Training loss: 0.07342248409986496\n",
      "Training loss: 0.07259464263916016\n",
      "Training loss: 0.07691100239753723\n",
      "Training loss: 0.07722345739603043\n",
      "Training loss: 0.07338828593492508\n",
      "Training loss: 0.07457482814788818\n",
      "Training loss: 0.07177971303462982\n",
      "Training loss: 0.07487687468528748\n",
      "Training loss: 0.08113274723291397\n",
      "Training loss: 0.08301997184753418\n",
      "Training loss: 0.07812249660491943\n",
      "Training loss: 0.0748189240694046\n",
      "Training loss: 0.07365123927593231\n",
      "Training loss: 0.07618606835603714\n",
      "Training loss: 0.0790206715464592\n",
      "Training loss: 0.07421207427978516\n",
      "Training loss: 0.07590501010417938\n",
      "Training loss: 0.07839846611022949\n",
      "Training loss: 0.0762142688035965\n",
      "Training loss: 0.07322867959737778\n",
      "Training loss: 0.07353997975587845\n",
      "Training loss: 0.07512281090021133\n",
      "Training loss: 0.07018303871154785\n",
      "Training loss: 0.07191677391529083\n",
      "Training loss: 0.07257883995771408\n",
      "Training loss: 0.0713476687669754\n",
      "Training loss: 0.07354577630758286\n",
      "Training loss: 0.07020853459835052\n",
      "Training loss: 0.07446455210447311\n",
      "Training loss: 0.07558444887399673\n",
      "Training loss: 0.08018038421869278\n",
      "Training loss: 0.07214184105396271\n",
      "Training loss: 0.06982136517763138\n",
      "Training loss: 0.07204746454954147\n",
      "Training loss: 0.07590405642986298\n",
      "Training loss: 0.07650359719991684\n",
      "Training loss: 0.06758558750152588\n",
      "Training loss: 0.07626170665025711\n",
      "Training loss: 0.07677362114191055\n",
      "Training loss: 0.07105650752782822\n",
      "Training loss: 0.0845746323466301\n",
      "Training loss: 0.07626166939735413\n",
      "Training loss: 0.07632433623075485\n",
      "Training loss: 0.08349963277578354\n",
      "Training loss: 0.0762089267373085\n",
      "Training loss: 0.07951165735721588\n",
      "Training loss: 0.07249073684215546\n",
      "Training loss: 0.07632272690534592\n",
      "Training loss: 0.07415822893381119\n",
      "Training loss: 0.0823567658662796\n",
      "Training loss: 0.07636327296495438\n",
      "Training loss: 0.07176148891448975\n",
      "Training loss: 0.07317021489143372\n",
      "Training loss: 0.07491646707057953\n",
      "Training loss: 0.07738377898931503\n",
      "Training loss: 0.07634604722261429\n",
      "Training loss: 0.07795154303312302\n",
      "Training loss: 0.07381894439458847\n",
      "Training loss: 0.07147757709026337\n",
      "Training loss: 0.07744228094816208\n",
      "Training loss: 0.0767751932144165\n",
      "Training loss: 0.07739592343568802\n",
      "Training loss: 0.07532897591590881\n",
      "Training loss: 0.07555599510669708\n",
      "Training loss: 0.07341210544109344\n",
      "Training loss: 0.07792229950428009\n",
      "Training loss: 0.07962805032730103\n",
      "Training loss: 0.06933130323886871\n",
      "Training loss: 0.06893964111804962\n",
      "Training loss: 0.07424940913915634\n",
      "Training loss: 0.07170664519071579\n",
      "Training loss: 0.0656106099486351\n",
      "Training loss: 0.07169222086668015\n",
      "Training loss: 0.08331721276044846\n",
      "Training loss: 0.06622301042079926\n",
      "Training loss: 0.07656707614660263\n",
      "Training loss: 0.07180815935134888\n",
      "Training loss: 0.0720156580209732\n",
      "Training loss: 0.08132610470056534\n",
      "Training loss: 0.07440612465143204\n",
      "Training loss: 0.07416675239801407\n",
      "Training loss: 0.075762078166008\n",
      "Training loss: 0.07636565715074539\n",
      "Training loss: 0.08229934424161911\n",
      "Training loss: 0.07043728977441788\n",
      "Training loss: 0.0731075182557106\n",
      "Training loss: 0.07183534651994705\n",
      "Training loss: 0.07194457203149796\n",
      "Training loss: 0.07267840951681137\n",
      "Training loss: 0.07658687233924866\n",
      "Training loss: 0.07096593827009201\n",
      "Training loss: 0.07211089879274368\n",
      "Training loss: 0.07708855718374252\n",
      "Training loss: 0.07527095824480057\n",
      "Training loss: 0.07354806363582611\n",
      "Training loss: 0.08232077956199646\n",
      "Training loss: 0.07554794102907181\n",
      "Training loss: 0.10650146752595901\n",
      "Training loss: 0.0732085108757019\n",
      "Training loss: 0.0747266337275505\n",
      "Training loss: 0.07445140182971954\n",
      "Training loss: 0.07371807843446732\n",
      "Training loss: 0.07843178510665894\n",
      "Training loss: 0.07630987465381622\n",
      "Training loss: 0.07567273080348969\n",
      "Training loss: 0.07762246578931808\n",
      "Training loss: 0.07271100580692291\n",
      "Training loss: 0.07463567703962326\n",
      "Training loss: 0.07148892432451248\n",
      "Training loss: 0.07671419531106949\n",
      "Training loss: 0.07806914299726486\n",
      "Training loss: 0.076088547706604\n",
      "Training loss: 0.07844875007867813\n",
      "Training loss: 0.0698835700750351\n",
      "Training loss: 0.06955961883068085\n",
      "Training loss: 0.07724828273057938\n",
      "Training loss: 0.07879006862640381\n",
      "Training loss: 0.07792960107326508\n",
      "Training loss: 0.07228422909975052\n",
      "Training loss: 0.07374745607376099\n",
      "Training loss: 0.07157029211521149\n",
      "Training loss: 0.07813657075166702\n",
      "Training loss: 0.07374345511198044\n",
      "Training loss: 0.08134403824806213\n",
      "Training loss: 0.07624557614326477\n",
      "Training loss: 0.07072842866182327\n",
      "Training loss: 0.07056685537099838\n",
      "Training loss: 0.07637779414653778\n",
      "Training loss: 0.07324826717376709\n",
      "Training loss: 0.07552041113376617\n",
      "Training loss: 0.07606063038110733\n",
      "Training loss: 0.07419752329587936\n",
      "Training loss: 0.07292217016220093\n",
      "Training loss: 0.06986242532730103\n",
      "Training loss: 0.0734882801771164\n",
      "Training loss: 0.07456319779157639\n",
      "Training loss: 0.07606743276119232\n",
      "Training loss: 0.07074000686407089\n",
      "Training loss: 0.0740821585059166\n",
      "Training loss: 0.0745682418346405\n",
      "Training loss: 0.07492375373840332\n",
      "Training loss: 0.07219361513853073\n",
      "Training loss: 0.0726475939154625\n",
      "Training loss: 0.07744996249675751\n",
      "Training loss: 0.07318063080310822\n",
      "Training loss: 0.07129965722560883\n",
      "Training loss: 0.06886246800422668\n",
      "Training loss: 0.0765816792845726\n",
      "Training loss: 0.07640909403562546\n",
      "Training loss: 0.07389260828495026\n",
      "Training loss: 0.06628662347793579\n",
      "Training loss: 0.07604164630174637\n",
      "Training loss: 0.07481314986944199\n",
      "Training loss: 0.0628034919500351\n",
      "Training loss: 0.07697323709726334\n",
      "Training loss: 0.06932105869054794\n",
      "Training loss: 0.07416324317455292\n",
      "Training loss: 0.07500752061605453\n",
      "Training loss: 0.07703163474798203\n",
      "Training loss: 0.07964356243610382\n",
      "Training loss: 0.06881415098905563\n",
      "Training loss: 0.07933148741722107\n",
      "Training loss: 0.07400570809841156\n",
      "Training loss: 0.07944781333208084\n",
      "Training loss: 0.0804581418633461\n",
      "Training loss: 0.08254609256982803\n",
      "Training loss: 0.07769011706113815\n",
      "Training loss: 0.07801863551139832\n",
      "Training loss: 0.07781176269054413\n",
      "Training loss: 0.07360436022281647\n",
      "Training loss: 0.0772503986954689\n",
      "Training loss: 0.073531873524189\n",
      "Training loss: 0.07154742628335953\n",
      "Training loss: 0.07252073287963867\n",
      "Training loss: 0.07312607765197754\n",
      "Training loss: 0.07172627002000809\n",
      "Training loss: 0.07285041362047195\n",
      "Training loss: 0.07269389182329178\n",
      "Training loss: 0.07761647552251816\n",
      "Training loss: 0.0771709457039833\n",
      "Training loss: 0.07496730238199234\n",
      "Training loss: 0.07237161695957184\n",
      "Training loss: 0.06960001587867737\n",
      "Training loss: 0.07427975535392761\n",
      "Training loss: 0.07260844856500626\n",
      "Training loss: 0.07453376054763794\n",
      "Training loss: 0.07679218798875809\n",
      "Training loss: 0.07668954879045486\n",
      "Training loss: 0.07213495671749115\n",
      "Training loss: 0.07241922616958618\n",
      "Training loss: 0.06938498467206955\n",
      "Training loss: 0.07534710317850113\n",
      "Training loss: 0.06986686587333679\n",
      "Training loss: 0.07050967961549759\n",
      "Training loss: 0.0773671343922615\n",
      "Training loss: 0.07722919434309006\n",
      "Training loss: 0.07214300334453583\n",
      "Training loss: 0.07652056217193604\n",
      "Training loss: 0.08080508559942245\n",
      "Training loss: 0.07411620020866394\n",
      "Training loss: 0.07475277036428452\n",
      "Training loss: 0.07555984705686569\n",
      "Training loss: 0.07287125289440155\n",
      "Training loss: 0.07054384052753448\n",
      "Training loss: 0.07392716407775879\n",
      "Training loss: 0.07439439743757248\n",
      "Training loss: 0.07458314299583435\n",
      "Training loss: 0.06815657764673233\n",
      "Training loss: 0.07406115531921387\n",
      "Training loss: 0.07510300725698471\n",
      "Training loss: 0.06901464611291885\n",
      "Training loss: 0.07129521667957306\n",
      "Training loss: 0.07907011359930038\n",
      "Training loss: 0.06738104671239853\n",
      "Training loss: 0.06956775486469269\n",
      "Training loss: 0.07526074349880219\n",
      "Training loss: 0.07470305263996124\n",
      "Training loss: 0.06794940680265427\n",
      "Training loss: 0.07401122897863388\n",
      "Training loss: 0.07185041904449463\n",
      "Training loss: 0.07626639306545258\n",
      "Training loss: 0.07472974061965942\n",
      "Training loss: 0.0700555369257927\n",
      "Training loss: 0.07260031253099442\n",
      "Training loss: 0.0767197459936142\n",
      "Training loss: 0.08016736805438995\n",
      "Training loss: 0.07278826087713242\n",
      "Training loss: 0.07141400128602982\n",
      "Training loss: 0.07295528799295425\n",
      "Training loss: 0.07351844012737274\n",
      "Training loss: 0.06927434355020523\n",
      "Training loss: 0.06499920785427094\n",
      "Training loss: 0.07795626670122147\n",
      "Training loss: 0.07539180666208267\n",
      "Training loss: 0.0713760107755661\n",
      "Training loss: 0.06863575428724289\n",
      "Training loss: 0.0800836831331253\n",
      "Training loss: 0.07282105833292007\n",
      "Training loss: 0.06715214997529984\n",
      "Training loss: 0.07396960258483887\n",
      "Training loss: 0.06682989001274109\n",
      "Training loss: 0.07580612599849701\n",
      "Training loss: 0.06965935230255127\n",
      "Training loss: 0.074024997651577\n",
      "Training loss: 0.0662287175655365\n",
      "Training loss: 0.07822579890489578\n",
      "Training loss: 0.06989341974258423\n",
      "Training loss: 0.07100056111812592\n",
      "Training loss: 0.07415509223937988\n",
      "Training loss: 0.07178904861211777\n",
      "Training loss: 0.07415556907653809\n",
      "Training loss: 0.06876927614212036\n",
      "Training loss: 0.0721537247300148\n",
      "Training loss: 0.07112760096788406\n",
      "Training loss: 0.07464263588190079\n",
      "Training loss: 0.07877352088689804\n",
      "Training loss: 0.07239309698343277\n",
      "Training loss: 0.0699993297457695\n",
      "Training loss: 0.0731782391667366\n",
      "Training loss: 0.0727684497833252\n",
      "Training loss: 0.07303550839424133\n",
      "Training loss: 0.07533270120620728\n",
      "Training loss: 0.07239087671041489\n",
      "Training loss: 0.06858493387699127\n",
      "Training loss: 0.07026946544647217\n",
      "Training loss: 0.07409792393445969\n",
      "Training loss: 0.07002583146095276\n",
      "Training loss: 0.06490909308195114\n",
      "Training loss: 0.06938895583152771\n",
      "Training loss: 0.07846114039421082\n",
      "Training loss: 0.0704440176486969\n",
      "Training loss: 0.07233504951000214\n",
      "Training loss: 0.0713815838098526\n",
      "Training loss: 0.07231951504945755\n",
      "Training loss: 0.07166635990142822\n",
      "Training loss: 0.08672461658716202\n",
      "Training loss: 0.0777832418680191\n",
      "Training loss: 0.07451579719781876\n",
      "Training loss: 0.07073735445737839\n",
      "Training loss: 0.068722203373909\n",
      "Training loss: 0.07028241455554962\n",
      "Training loss: 0.06995490193367004\n",
      "Training loss: 0.07401121407747269\n",
      "Training loss: 0.07247457653284073\n",
      "Training loss: 0.07770795375108719\n",
      "Training loss: 0.07160292565822601\n",
      "Training loss: 0.07214805483818054\n",
      "Training loss: 0.07002313435077667\n",
      "Training loss: 0.0717613473534584\n",
      "Training loss: 0.07818180322647095\n",
      "Training loss: 0.07965405285358429\n",
      "Training loss: 0.07406053692102432\n",
      "Training loss: 0.07101922482252121\n",
      "Training loss: 0.07810269296169281\n",
      "Training loss: 0.07521600276231766\n",
      "Training loss: 0.06999798119068146\n",
      "Training loss: 0.0782427117228508\n",
      "Training loss: 0.07429186254739761\n",
      "Training loss: 0.07449314743280411\n",
      "Training loss: 0.07574529200792313\n",
      "Training loss: 0.0710853561758995\n",
      "Training loss: 0.07543788105249405\n",
      "Training loss: 0.07524015009403229\n",
      "Training loss: 0.07459091395139694\n",
      "Training loss: 0.08076217770576477\n",
      "Training loss: 0.07141601294279099\n",
      "Training loss: 0.08072223514318466\n",
      "Training loss: 0.07253801822662354\n",
      "Training loss: 0.0783267468214035\n",
      "Training loss: 0.07097367197275162\n",
      "Training loss: 0.07700247317552567\n",
      "Training loss: 0.07538580894470215\n",
      "Training loss: 0.07378841191530228\n",
      "Training loss: 0.07450267672538757\n",
      "Training loss: 0.0757419764995575\n",
      "Training loss: 0.06998657435178757\n",
      "Training loss: 0.06659611314535141\n",
      "Training loss: 0.07254105061292648\n",
      "Training loss: 0.0737544521689415\n",
      "Training loss: 0.07524633407592773\n",
      "Training loss: 0.07758725434541702\n",
      "Training loss: 0.0732390359044075\n",
      "Training loss: 0.0725768432021141\n",
      "Training loss: 0.07338918745517731\n",
      "Training loss: 0.07524716109037399\n",
      "Training loss: 0.06997448951005936\n",
      "Training loss: 0.07141449302434921\n",
      "Training loss: 0.06660835444927216\n",
      "Training loss: 0.07026269286870956\n",
      "Training loss: 0.07465999573469162\n",
      "Training loss: 0.07148399204015732\n",
      "Training loss: 0.07152727246284485\n",
      "Training loss: 0.07052084803581238\n",
      "Training loss: 0.07425478845834732\n",
      "Training loss: 0.07184592634439468\n",
      "Training loss: 0.07358073443174362\n",
      "Training loss: 0.06867252290248871\n",
      "Training loss: 0.0621439553797245\n",
      "Training loss: 0.0783027932047844\n",
      "Training loss: 0.07239736616611481\n",
      "Training loss: 0.06885460764169693\n",
      "Training loss: 0.07543368637561798\n",
      "Training loss: 0.0694146528840065\n",
      "Training loss: 0.06975238770246506\n",
      "Training loss: 0.07579377293586731\n",
      "Training loss: 0.07336118817329407\n",
      "Training loss: 0.07011294364929199\n",
      "Training loss: 0.06838349252939224\n",
      "Training loss: 0.07300729304552078\n",
      "Training loss: 0.07092616707086563\n",
      "Training loss: 0.06453549116849899\n",
      "Training loss: 0.07391258329153061\n",
      "Training loss: 0.07090156525373459\n",
      "Training loss: 0.0689588338136673\n",
      "Training loss: 0.06702087819576263\n",
      "Training loss: 0.07094268500804901\n",
      "Training loss: 0.07193782180547714\n",
      "Training loss: 0.07077442109584808\n",
      "Training loss: 0.0718153789639473\n",
      "Training loss: 0.07281172275543213\n",
      "Training loss: 0.07181346416473389\n",
      "Training loss: 0.0763324722647667\n",
      "Training loss: 0.07376149296760559\n",
      "Training loss: 0.07487811893224716\n",
      "Training loss: 0.07860445976257324\n",
      "Training loss: 0.0763143002986908\n",
      "Training loss: 0.07893277704715729\n",
      "Training loss: 0.07095760107040405\n",
      "Training loss: 0.07546036690473557\n",
      "Training loss: 0.07227683067321777\n",
      "Training loss: 0.07552865892648697\n",
      "Training loss: 0.0760834664106369\n",
      "Training loss: 0.08586251735687256\n",
      "Training loss: 0.07814989238977432\n",
      "Training loss: 0.08114293962717056\n",
      "Training loss: 0.07728511840105057\n",
      "Training loss: 0.07528232038021088\n",
      "Training loss: 0.08114972710609436\n",
      "Training loss: 0.07434061914682388\n",
      "Training loss: 0.08194471150636673\n",
      "Training loss: 0.07741451263427734\n",
      "Training loss: 0.07707880437374115\n",
      "Training loss: 0.07630027085542679\n",
      "Training loss: 0.08236348628997803\n",
      "Training loss: 0.07238360494375229\n",
      "Training loss: 0.07033192366361618\n",
      "Training loss: 0.07716981321573257\n",
      "Training loss: 0.07910789549350739\n",
      "Training loss: 0.07766688615083694\n",
      "Training loss: 0.07559159398078918\n",
      "Training loss: 0.07131683081388474\n",
      "Training loss: 0.07439449429512024\n",
      "Training loss: 0.07689184695482254\n",
      "Training loss: 0.08035052567720413\n",
      "Training loss: 0.07531445473432541\n",
      "Training loss: 0.06587500125169754\n",
      "Training loss: 0.07260565459728241\n",
      "Training loss: 0.07515570521354675\n",
      "Training loss: 0.07467988133430481\n",
      "Training loss: 0.07695886492729187\n",
      "Training loss: 0.07631181180477142\n",
      "Training loss: 0.0728180930018425\n",
      "Training loss: 0.07132802903652191\n",
      "Training loss: 0.0697765201330185\n",
      "Training loss: 0.07254903018474579\n",
      "Training loss: 0.07333443313837051\n",
      "Training loss: 0.07058047503232956\n",
      "Training loss: 0.07221200317144394\n",
      "Training loss: 0.07292093336582184\n",
      "Training loss: 0.07524136453866959\n",
      "Training loss: 0.07151253521442413\n",
      "Training loss: 0.0753287672996521\n",
      "Training loss: 0.07090272754430771\n",
      "Training loss: 0.07067318260669708\n",
      "Training loss: 0.07426900416612625\n",
      "Training loss: 0.07137900590896606\n",
      "Training loss: 0.07989350706338882\n",
      "Training loss: 0.07326200604438782\n",
      "Training loss: 0.07739096879959106\n",
      "Training loss: 0.07384561747312546\n",
      "Training loss: 0.07764212787151337\n",
      "Training loss: 0.07237708568572998\n",
      "Training loss: 0.07772364467382431\n",
      "Training loss: 0.07927186787128448\n",
      "Training loss: 0.07233987003564835\n",
      "Training loss: 0.0758524090051651\n",
      "Training loss: 0.08186015486717224\n",
      "Training loss: 0.08092493563890457\n",
      "Training loss: 0.07536300271749496\n",
      "Training loss: 0.07480520009994507\n",
      "Training loss: 0.07742840051651001\n",
      "Training loss: 0.08569124341011047\n",
      "Training loss: 0.07679654657840729\n",
      "Training loss: 0.07915584743022919\n",
      "Training loss: 0.08064642548561096\n",
      "Training loss: 0.0735386312007904\n",
      "Training loss: 0.07988564670085907\n",
      "Training loss: 0.07774217426776886\n",
      "Training loss: 0.07473398745059967\n",
      "Training loss: 0.07694166153669357\n",
      "Training loss: 0.0729823112487793\n",
      "Training loss: 0.07481064647436142\n",
      "Training loss: 0.07505784183740616\n",
      "Training loss: 0.0743546336889267\n",
      "Training loss: 0.07177361845970154\n",
      "Training loss: 0.07535972446203232\n",
      "Training loss: 0.07804833352565765\n",
      "Training loss: 0.07862525433301926\n",
      "Training loss: 0.0694524273276329\n",
      "Training loss: 0.07442235946655273\n",
      "Training loss: 0.0859723910689354\n",
      "Training loss: 0.07596530765295029\n",
      "Training loss: 0.07432214170694351\n",
      "Training loss: 0.07559672743082047\n",
      "Training loss: 0.07405567914247513\n",
      "Training loss: 0.07057378441095352\n",
      "Training loss: 0.07061437517404556\n",
      "Training loss: 0.07600852102041245\n",
      "Training loss: 0.07152758538722992\n",
      "Training loss: 0.08113828301429749\n",
      "Training loss: 0.0697663351893425\n",
      "Training loss: 0.07401520758867264\n",
      "Training loss: 0.07395700365304947\n",
      "Training loss: 0.07042572647333145\n",
      "Training loss: 0.07511695474386215\n",
      "Training loss: 0.07278050482273102\n",
      "Training loss: 0.07314014434814453\n",
      "Training loss: 0.07481881231069565\n",
      "Training loss: 0.0738118514418602\n",
      "Training loss: 0.07403061538934708\n",
      "Training loss: 0.07016421109437943\n",
      "Training loss: 0.07975038141012192\n",
      "Training loss: 0.06914814561605453\n",
      "Training loss: 0.069429412484169\n",
      "Training loss: 0.07271876186132431\n",
      "Training loss: 0.0799270048737526\n",
      "Training loss: 0.07145415246486664\n",
      "Training loss: 0.06982025504112244\n",
      "Training loss: 0.07028762996196747\n",
      "Training loss: 0.06641698628664017\n",
      "Training loss: 0.0759122371673584\n",
      "Training loss: 0.07129082828760147\n",
      "Training loss: 0.06710784137248993\n",
      "Training loss: 0.07672774791717529\n",
      "Training loss: 0.07867015898227692\n",
      "Training loss: 0.07300584018230438\n",
      "Training loss: 0.0674007311463356\n",
      "Training loss: 0.0767911970615387\n",
      "Training loss: 0.07626625150442123\n",
      "Training loss: 0.07394596189260483\n",
      "Training loss: 0.06527402251958847\n",
      "Training loss: 0.07426217198371887\n",
      "Training loss: 0.06059904024004936\n",
      "Training loss: 0.08074339479207993\n",
      "Training loss: 0.0718357190489769\n",
      "Training loss: 0.06832898408174515\n",
      "Training loss: 0.06906551867723465\n",
      "Training loss: 0.07549896836280823\n",
      "Training loss: 0.07563656568527222\n",
      "Training loss: 0.07811595499515533\n",
      "Training loss: 0.07577180862426758\n",
      "Training loss: 0.08162032067775726\n",
      "Training loss: 0.07363272458314896\n",
      "Training loss: 0.07520381361246109\n",
      "Training loss: 0.07851279526948929\n",
      "Training loss: 0.070945143699646\n",
      "Training loss: 0.07099206000566483\n",
      "Training loss: 0.06956162303686142\n",
      "Training loss: 0.07523946464061737\n",
      "Training loss: 0.07007909566164017\n",
      "Training loss: 0.06831509619951248\n",
      "Training loss: 0.06577727198600769\n",
      "Training loss: 0.07016339153051376\n",
      "Training loss: 0.07533427327871323\n",
      "Training loss: 0.07840537279844284\n",
      "Training loss: 0.066322460770607\n",
      "Training loss: 0.0695728287100792\n",
      "Training loss: 0.0742875263094902\n",
      "Training loss: 0.07169214636087418\n",
      "Training loss: 0.07173436135053635\n",
      "Training loss: 0.07251955568790436\n",
      "Training loss: 0.07385428249835968\n",
      "Training loss: 0.06534995138645172\n",
      "Training loss: 0.07489602267742157\n",
      "Training loss: 0.07397417724132538\n",
      "Training loss: 0.0687786117196083\n",
      "Training loss: 0.07428567111492157\n",
      "Training loss: 0.08011211454868317\n",
      "Training loss: 0.0716777816414833\n",
      "Training loss: 0.0678982138633728\n",
      "Training loss: 0.0726967379450798\n",
      "Training loss: 0.07489079982042313\n",
      "Training loss: 0.07539395987987518\n",
      "Training loss: 0.07425231486558914\n",
      "Training loss: 0.07480604946613312\n",
      "Training loss: 0.07242856174707413\n",
      "Training loss: 0.07971721887588501\n",
      "Training loss: 0.07340209186077118\n",
      "Training loss: 0.07619094103574753\n",
      "Training loss: 0.07244198769330978\n",
      "Training loss: 0.07072093337774277\n",
      "Training loss: 0.07674892246723175\n",
      "Training loss: 0.07411278039216995\n",
      "Training loss: 0.0819120928645134\n",
      "Training loss: 0.07867952436208725\n",
      "Training loss: 0.07348370552062988\n",
      "Training loss: 0.0697588101029396\n",
      "Training loss: 0.07158073037862778\n",
      "Training loss: 0.07441303133964539\n",
      "Training loss: 0.07449202239513397\n",
      "Training loss: 0.07072534412145615\n",
      "Training loss: 0.07921162247657776\n",
      "Training loss: 0.07152059674263\n",
      "Training loss: 0.07135774940252304\n",
      "Training loss: 0.06931371986865997\n",
      "Training loss: 0.08097758889198303\n",
      "Training loss: 0.07038189470767975\n",
      "Training loss: 0.07409937679767609\n",
      "Training loss: 0.08159012347459793\n",
      "Training loss: 0.07711280882358551\n",
      "Training loss: 0.07050371915102005\n",
      "Training loss: 0.06731215119361877\n",
      "Training loss: 0.06602322310209274\n",
      "Training loss: 0.07090010493993759\n",
      "Training loss: 0.07112809270620346\n",
      "Training loss: 0.07118824124336243\n",
      "Training loss: 0.07689927518367767\n",
      "Training loss: 0.07418758422136307\n",
      "Training loss: 0.07761704176664352\n",
      "Training loss: 0.0712537169456482\n",
      "Training loss: 0.07144038379192352\n",
      "Training loss: 0.07631466537714005\n",
      "Training loss: 0.07111523300409317\n",
      "Training loss: 0.07990817725658417\n",
      "Training loss: 0.07040046900510788\n",
      "Training loss: 0.0775781199336052\n",
      "Training loss: 0.07688846439123154\n",
      "Training loss: 0.07808113843202591\n",
      "Training loss: 0.07172558456659317\n",
      "Training loss: 0.06492208689451218\n",
      "Training loss: 0.07140101492404938\n",
      "Training loss: 0.07128873467445374\n",
      "Training loss: 0.08028288930654526\n",
      "Training loss: 0.07123956829309464\n",
      "Training loss: 0.0733788013458252\n",
      "Training loss: 0.07343558222055435\n",
      "Training loss: 0.07748212665319443\n",
      "Training loss: 0.06979744881391525\n",
      "Training loss: 0.07083337754011154\n",
      "Training loss: 0.0755147710442543\n",
      "Training loss: 0.06896688044071198\n",
      "Training loss: 0.07671456784009933\n",
      "Training loss: 0.0708884596824646\n",
      "Training loss: 0.0735640749335289\n",
      "Training loss: 0.0685528889298439\n",
      "Training loss: 0.07220488786697388\n",
      "Training loss: 0.07326383888721466\n",
      "Training loss: 0.06839711219072342\n",
      "Training loss: 0.08004524558782578\n",
      "Training loss: 0.0772780179977417\n",
      "Training loss: 0.07400532811880112\n",
      "Training loss: 0.07365439832210541\n",
      "Training loss: 0.07237987220287323\n",
      "Training loss: 0.0720154270529747\n",
      "Training loss: 0.07148441672325134\n",
      "Training loss: 0.07178990542888641\n",
      "Training loss: 0.07421957701444626\n",
      "Training loss: 0.07339993119239807\n",
      "Training loss: 0.0782376229763031\n",
      "Training loss: 0.07068519294261932\n",
      "Training loss: 0.07934986799955368\n",
      "Training loss: 0.06607958674430847\n",
      "Training loss: 0.06904470175504684\n",
      "Training loss: 0.08148516714572906\n",
      "Training loss: 0.07619195431470871\n",
      "Training loss: 0.06892816722393036\n",
      "Training loss: 0.07872018218040466\n",
      "Training loss: 0.068534255027771\n",
      "Training loss: 0.06768075376749039\n",
      "Training loss: 0.07127315551042557\n",
      "Training loss: 0.07345583289861679\n",
      "Training loss: 0.0774175375699997\n",
      "Training loss: 0.0684463307261467\n",
      "Training loss: 0.07396982610225677\n",
      "Training loss: 0.07904329150915146\n",
      "Training loss: 0.0723019540309906\n",
      "Training loss: 0.06875293701887131\n",
      "Training loss: 0.06783832609653473\n",
      "Training loss: 0.08405888825654984\n",
      "Training loss: 0.07131359726190567\n",
      "Training loss: 0.07265529036521912\n",
      "Training loss: 0.07503172755241394\n",
      "Training loss: 0.07189149409532547\n",
      "Training loss: 0.06437751650810242\n",
      "Training loss: 0.06883355230093002\n",
      "Training loss: 0.07369343191385269\n",
      "Training loss: 0.07333874702453613\n",
      "Training loss: 0.07450590282678604\n",
      "Training loss: 0.06869865953922272\n",
      "Training loss: 0.07037433981895447\n",
      "Training loss: 0.07190907746553421\n",
      "Training loss: 0.06869639456272125\n",
      "Training loss: 0.07476489990949631\n",
      "Training loss: 0.07149551808834076\n",
      "Training loss: 0.06867081671953201\n",
      "Training loss: 0.06258784979581833\n",
      "Training loss: 0.07327812165021896\n",
      "Training loss: 0.0733151063323021\n",
      "Training loss: 0.07375790923833847\n",
      "Training loss: 0.07067324221134186\n",
      "Training loss: 0.07725769281387329\n",
      "Training loss: 0.06961528956890106\n",
      "Training loss: 0.07162690907716751\n",
      "Training loss: 0.06905248761177063\n",
      "Training loss: 0.06806188076734543\n",
      "Training loss: 0.06886899471282959\n",
      "Training loss: 0.06529618054628372\n",
      "Training loss: 0.06987863779067993\n",
      "Training loss: 0.06978817284107208\n",
      "Training loss: 0.07179684191942215\n",
      "Training loss: 0.06721094250679016\n",
      "Training loss: 0.07767961919307709\n",
      "Training loss: 0.06654296070337296\n",
      "Training loss: 0.06832368671894073\n",
      "Training loss: 0.07406159490346909\n",
      "Training loss: 0.06861336529254913\n",
      "Training loss: 0.06913295388221741\n",
      "Training loss: 0.06998354196548462\n",
      "Training loss: 0.07186728715896606\n",
      "Training loss: 0.07006030529737473\n",
      "Training loss: 0.06415683031082153\n",
      "Training loss: 0.06898865848779678\n",
      "Training loss: 0.07038024812936783\n",
      "Training loss: 0.07071235775947571\n",
      "Training loss: 0.06809297204017639\n",
      "Training loss: 0.07043258845806122\n",
      "Training loss: 0.07235682010650635\n",
      "Training loss: 0.07167837768793106\n",
      "Training loss: 0.06453971564769745\n",
      "Training loss: 0.07035970687866211\n",
      "Training loss: 0.07597339153289795\n",
      "Training loss: 0.06829790771007538\n",
      "Training loss: 0.06474803388118744\n",
      "Training loss: 0.07298391312360764\n",
      "Training loss: 0.0725206509232521\n",
      "Training loss: 0.07082702219486237\n",
      "Training loss: 0.07249198108911514\n",
      "Training loss: 0.06677499413490295\n",
      "Training loss: 0.06998880207538605\n",
      "Training loss: 0.07204721868038177\n",
      "Training loss: 0.07542205601930618\n",
      "Training loss: 0.07328024506568909\n",
      "Training loss: 0.07940918207168579\n",
      "Training loss: 0.07481924444437027\n",
      "Training loss: 0.07329606264829636\n",
      "Training loss: 0.07257214933633804\n",
      "Training loss: 0.07790682464838028\n",
      "Training loss: 0.07377342134714127\n",
      "Training loss: 0.07513730227947235\n",
      "Training loss: 0.07510024309158325\n",
      "Training loss: 0.07135893404483795\n",
      "Training loss: 0.07579885423183441\n",
      "Training loss: 0.07115641236305237\n",
      "Training loss: 0.0734299048781395\n",
      "Training loss: 0.07111262530088425\n",
      "Training loss: 0.07818707078695297\n",
      "Training loss: 0.07762846350669861\n",
      "Training loss: 0.0703580304980278\n",
      "Training loss: 0.07482472062110901\n",
      "Training loss: 0.07569229602813721\n",
      "Training loss: 0.07758546620607376\n",
      "Training loss: 0.07181883603334427\n",
      "Training loss: 0.0741228461265564\n",
      "Training loss: 0.07077266275882721\n",
      "Training loss: 0.07312750816345215\n",
      "Training loss: 0.07108382135629654\n",
      "Training loss: 0.06858441978693008\n",
      "Training loss: 0.06957922875881195\n",
      "Training loss: 0.07878204435110092\n",
      "Training loss: 0.06894414126873016\n",
      "Training loss: 0.06546561419963837\n",
      "Training loss: 0.07127094268798828\n",
      "Training loss: 0.07569048553705215\n",
      "Training loss: 0.07920520752668381\n",
      "Training loss: 0.07495082169771194\n",
      "Training loss: 0.0733652263879776\n",
      "Training loss: 0.07252752035856247\n",
      "Training loss: 0.07716289907693863\n",
      "Training loss: 0.07856708019971848\n",
      "Training loss: 0.07266504317522049\n",
      "Training loss: 0.07565769553184509\n",
      "Training loss: 0.0712101012468338\n",
      "Training loss: 0.07848631590604782\n",
      "Training loss: 0.07321992516517639\n",
      "Training loss: 0.06487961858510971\n",
      "Training loss: 0.07175598293542862\n",
      "Training loss: 0.07154419273138046\n",
      "Training loss: 0.07397277653217316\n",
      "Training loss: 0.0687899962067604\n",
      "Training loss: 0.0703352764248848\n",
      "Training loss: 0.08102608472108841\n",
      "Training loss: 0.073276087641716\n",
      "Training loss: 0.07436918467283249\n",
      "Training loss: 0.07328686118125916\n",
      "Training loss: 0.08158119022846222\n",
      "Training loss: 0.07477139681577682\n",
      "Training loss: 0.07117702811956406\n",
      "Training loss: 0.06593258678913116\n",
      "Training loss: 0.07695033401250839\n",
      "Training loss: 0.07235094159841537\n",
      "Training loss: 0.07591000199317932\n",
      "Training loss: 0.07481561601161957\n",
      "Training loss: 0.0717129111289978\n",
      "Training loss: 0.0645529255270958\n",
      "Training loss: 0.0722891092300415\n",
      "Training loss: 0.06766383349895477\n",
      "Training loss: 0.06690817326307297\n",
      "Training loss: 0.06647299975156784\n",
      "Training loss: 0.06979019939899445\n",
      "Training loss: 0.07248526811599731\n",
      "Training loss: 0.07494112104177475\n",
      "Training loss: 0.07128635048866272\n",
      "Training loss: 0.07590890675783157\n",
      "Training loss: 0.07247409969568253\n",
      "Training loss: 0.07927240431308746\n",
      "Training loss: 0.07170633226633072\n",
      "Training loss: 0.06895943731069565\n",
      "Training loss: 0.08035118132829666\n",
      "Training loss: 0.07042628526687622\n",
      "Training loss: 0.06949714571237564\n",
      "Training loss: 0.07002183049917221\n",
      "Training loss: 0.06909094005823135\n",
      "Training loss: 0.06655348837375641\n",
      "Training loss: 0.07186838239431381\n",
      "Training loss: 0.06905856728553772\n",
      "Training loss: 0.07178239524364471\n",
      "Training loss: 0.07028333842754364\n",
      "Training loss: 0.07585560530424118\n",
      "Training loss: 0.07045596837997437\n",
      "Training loss: 0.0694577768445015\n",
      "Training loss: 0.06765180826187134\n",
      "Training loss: 0.07977072149515152\n",
      "Training loss: 0.06730133295059204\n",
      "Training loss: 0.07105257362127304\n",
      "Training loss: 0.07260012626647949\n",
      "Training loss: 0.07193878293037415\n",
      "Training loss: 0.07036193460226059\n",
      "Training loss: 0.06906364858150482\n",
      "Training loss: 0.06857722997665405\n",
      "Training loss: 0.07669541239738464\n",
      "Training loss: 0.08252892643213272\n",
      "Training loss: 0.06762287020683289\n",
      "Training loss: 0.07124565541744232\n",
      "Training loss: 0.07364022731781006\n",
      "Training loss: 0.07468504458665848\n",
      "Training loss: 0.07193712890148163\n",
      "Training loss: 0.06966087222099304\n",
      "Training loss: 0.08030804991722107\n",
      "Training loss: 0.07608679682016373\n",
      "Training loss: 0.07218673825263977\n",
      "Training loss: 0.07723479717969894\n",
      "Training loss: 0.07721368968486786\n",
      "Training loss: 0.06751414388418198\n",
      "Training loss: 0.07351472228765488\n",
      "Training loss: 0.07439140230417252\n",
      "Training loss: 0.07684563100337982\n",
      "Training loss: 0.07191398739814758\n",
      "Training loss: 0.07362987101078033\n",
      "Training loss: 0.07463342696428299\n",
      "Training loss: 0.06921633332967758\n",
      "Training loss: 0.07976900786161423\n",
      "Training loss: 0.06800835579633713\n",
      "Training loss: 0.06948797404766083\n",
      "Training loss: 0.07084840536117554\n",
      "Training loss: 0.0712076872587204\n",
      "Training loss: 0.07475399971008301\n",
      "Training loss: 0.07093609124422073\n",
      "Training loss: 0.07470088452100754\n",
      "Training loss: 0.06725858896970749\n",
      "Training loss: 0.0775926411151886\n",
      "Training loss: 0.07378585636615753\n",
      "Training loss: 0.07352187484502792\n",
      "Training loss: 0.07074163109064102\n",
      "Training loss: 0.06716247648000717\n",
      "Training loss: 0.07048271596431732\n",
      "Training loss: 0.07526746392250061\n",
      "Training loss: 0.07663382589817047\n",
      "Training loss: 0.07123076915740967\n",
      "Training loss: 0.07255937904119492\n",
      "Training loss: 0.06995677202939987\n",
      "Training loss: 0.08117660880088806\n",
      "Training loss: 0.0718645229935646\n",
      "Training loss: 0.07658331096172333\n",
      "Training loss: 0.06848728656768799\n",
      "Training loss: 0.07321389764547348\n",
      "Training loss: 0.07233703136444092\n",
      "Training loss: 0.074518583714962\n",
      "Training loss: 0.06977833062410355\n",
      "Training loss: 0.06419187784194946\n",
      "Training loss: 0.07806479185819626\n",
      "Training loss: 0.07210594415664673\n",
      "Training loss: 0.07641960680484772\n",
      "Training loss: 0.06632202863693237\n",
      "Training loss: 0.07091612368822098\n",
      "Training loss: 0.07183398306369781\n",
      "Training loss: 0.07259268313646317\n",
      "Training loss: 0.07088667154312134\n",
      "Training loss: 0.0717398151755333\n",
      "Training loss: 0.07264947146177292\n",
      "Training loss: 0.07009486854076385\n",
      "Training loss: 0.07779162377119064\n",
      "Training loss: 0.07021699100732803\n",
      "Training loss: 0.07093249261379242\n",
      "Training loss: 0.07309373468160629\n",
      "Training loss: 0.06902133673429489\n",
      "Training loss: 0.06473612040281296\n",
      "Training loss: 0.07407384365797043\n",
      "Training loss: 0.06964786350727081\n",
      "Training loss: 0.07150199264287949\n",
      "Training loss: 0.07729288190603256\n",
      "Training loss: 0.07076573371887207\n",
      "Training loss: 0.06916233152151108\n",
      "Training loss: 0.06818646192550659\n",
      "Training loss: 0.07695827633142471\n",
      "Training loss: 0.07284875214099884\n",
      "Training loss: 0.06853173673152924\n",
      "Training loss: 0.0750555470585823\n",
      "Training loss: 0.07119446247816086\n",
      "Training loss: 0.07223601639270782\n",
      "Training loss: 0.06651898473501205\n",
      "Training loss: 0.0648844838142395\n",
      "Training loss: 0.06818587332963943\n",
      "Training loss: 0.06770404428243637\n",
      "Training loss: 0.07006584852933884\n",
      "Training loss: 0.06912336498498917\n",
      "Training loss: 0.07334280014038086\n",
      "Training loss: 0.0725700631737709\n",
      "Training loss: 0.07326044142246246\n",
      "Training loss: 0.0720934197306633\n",
      "Training loss: 0.07192526757717133\n",
      "Training loss: 0.06645048409700394\n",
      "Training loss: 0.07362909615039825\n",
      "Training loss: 0.06927413493394852\n",
      "Training loss: 0.07568268477916718\n",
      "Training loss: 0.07050611078739166\n",
      "Training loss: 0.06840560585260391\n",
      "Training loss: 0.08028315007686615\n",
      "Training loss: 0.06810621172189713\n",
      "Training loss: 0.06742716580629349\n",
      "Training loss: 0.07384572923183441\n",
      "Training loss: 0.07094966620206833\n",
      "Training loss: 0.06607899069786072\n",
      "Training loss: 0.07359135895967484\n",
      "Training loss: 0.06966594606637955\n",
      "Training loss: 0.07801128178834915\n",
      "Training loss: 0.06708020716905594\n",
      "Training loss: 0.06866003572940826\n",
      "Training loss: 0.06754857301712036\n",
      "Training loss: 0.06756173819303513\n",
      "Training loss: 0.07390885055065155\n",
      "Training loss: 0.07174850255250931\n",
      "Training loss: 0.06541291624307632\n",
      "Training loss: 0.0640445202589035\n",
      "Training loss: 0.06594986468553543\n",
      "Training loss: 0.07064072787761688\n",
      "Training loss: 0.06768254190683365\n",
      "Training loss: 0.06591972708702087\n",
      "Training loss: 0.06436973065137863\n",
      "Training loss: 0.06665897369384766\n",
      "Training loss: 0.06596304476261139\n",
      "Training loss: 0.06869994848966599\n",
      "Training loss: 0.06789465993642807\n",
      "Training loss: 0.06958827376365662\n",
      "Training loss: 0.06720223277807236\n",
      "Training loss: 0.06836763769388199\n",
      "Training loss: 0.06762233376502991\n",
      "Training loss: 0.06322585046291351\n",
      "Training loss: 0.07135627418756485\n",
      "Training loss: 0.06978829950094223\n",
      "Training loss: 0.06524762511253357\n",
      "Training loss: 0.07133732736110687\n",
      "Training loss: 0.06497031450271606\n",
      "Training loss: 0.07235153764486313\n",
      "Training loss: 0.0651867687702179\n",
      "Training loss: 0.07581005990505219\n",
      "Training loss: 0.07295973598957062\n",
      "Training loss: 0.07049206644296646\n",
      "Training loss: 0.06605105102062225\n",
      "Training loss: 0.06795483827590942\n",
      "Training loss: 0.07255781441926956\n",
      "Training loss: 0.07105765491724014\n",
      "Training loss: 0.0690692737698555\n",
      "Training loss: 0.06753037869930267\n",
      "Training loss: 0.07795514166355133\n",
      "Training loss: 0.07269812375307083\n",
      "Training loss: 0.06559670716524124\n",
      "Training loss: 0.072048619389534\n",
      "Training loss: 0.07310792058706284\n",
      "Training loss: 0.06740555912256241\n",
      "Training loss: 0.06786137819290161\n",
      "Training loss: 0.07202273607254028\n",
      "Training loss: 0.06737660616636276\n",
      "Training loss: 0.07226568460464478\n",
      "Training loss: 0.0735974907875061\n",
      "Training loss: 0.07385582476854324\n",
      "Training loss: 0.07211309671401978\n",
      "Training loss: 0.0691797137260437\n",
      "Training loss: 0.0688568726181984\n",
      "Training loss: 0.07423805445432663\n",
      "Training loss: 0.06860172748565674\n",
      "Training loss: 0.07537141442298889\n",
      "Training loss: 0.07193365693092346\n",
      "Training loss: 0.06947439163923264\n",
      "Training loss: 0.08482785522937775\n",
      "Training loss: 0.07481108605861664\n",
      "Training loss: 0.07318238168954849\n",
      "Training loss: 0.07589007169008255\n",
      "Training loss: 0.07029924541711807\n",
      "Training loss: 0.06929337233304977\n",
      "Training loss: 0.07027418911457062\n",
      "Training loss: 0.075435109436512\n",
      "Training loss: 0.07692418247461319\n",
      "Training loss: 0.06966669112443924\n",
      "Training loss: 0.07058876007795334\n",
      "Training loss: 0.06963669508695602\n",
      "Training loss: 0.06763149797916412\n",
      "Training loss: 0.06875592470169067\n",
      "Training loss: 0.07183799892663956\n",
      "Training loss: 0.07424396276473999\n",
      "Training loss: 0.07192143052816391\n",
      "Training loss: 0.06862447410821915\n",
      "Training loss: 0.0675697922706604\n",
      "Training loss: 0.07369926571846008\n",
      "Training loss: 0.07461366057395935\n",
      "Training loss: 0.06832839548587799\n",
      "Training loss: 0.06811100244522095\n",
      "Training loss: 0.0752861425280571\n",
      "Training loss: 0.06570877879858017\n",
      "Training loss: 0.07364525645971298\n",
      "Training loss: 0.06765023618936539\n",
      "Training loss: 0.07020727545022964\n",
      "Training loss: 0.066321961581707\n",
      "Training loss: 0.0698237195611\n",
      "Training loss: 0.06686782091856003\n",
      "Training loss: 0.07418377697467804\n",
      "Training loss: 0.07074373960494995\n",
      "Training loss: 0.06826106458902359\n",
      "Training loss: 0.0731421709060669\n",
      "Training loss: 0.07411478459835052\n",
      "Training loss: 0.06629320234060287\n",
      "Training loss: 0.07501297444105148\n",
      "Training loss: 0.06928813457489014\n",
      "Training loss: 0.0696508139371872\n",
      "Training loss: 0.07405896484851837\n",
      "Training loss: 0.07401295751333237\n",
      "Training loss: 0.06750353425741196\n",
      "Training loss: 0.07530791312456131\n",
      "Training loss: 0.07231180369853973\n",
      "Training loss: 0.06790424138307571\n",
      "Training loss: 0.06916537880897522\n",
      "Training loss: 0.07244053483009338\n",
      "Training loss: 0.0732295885682106\n",
      "Training loss: 0.06853365898132324\n",
      "Training loss: 0.0683550089597702\n",
      "Training loss: 0.07186663150787354\n",
      "Training loss: 0.06767594814300537\n",
      "Training loss: 0.06876595318317413\n",
      "Training loss: 0.07461763173341751\n",
      "Training loss: 0.06529160588979721\n",
      "Training loss: 0.07015623897314072\n",
      "Training loss: 0.06686452776193619\n",
      "Training loss: 0.06904944032430649\n",
      "Training loss: 0.07175827771425247\n",
      "Training loss: 0.07489433139562607\n",
      "Training loss: 0.06809880584478378\n",
      "Training loss: 0.06993478536605835\n",
      "Training loss: 0.06618915498256683\n",
      "Training loss: 0.07218817621469498\n",
      "Training loss: 0.06588908284902573\n",
      "Training loss: 0.07645318657159805\n",
      "Training loss: 0.06362481415271759\n",
      "Training loss: 0.06531482189893723\n",
      "Training loss: 0.06843260675668716\n",
      "Training loss: 0.06696151942014694\n",
      "Training loss: 0.0695376768708229\n",
      "Training loss: 0.07817694544792175\n",
      "Training loss: 0.06864682585000992\n",
      "Training loss: 0.06607415527105331\n",
      "Training loss: 0.07277090847492218\n",
      "Training loss: 0.0663386732339859\n",
      "Training loss: 0.06736896187067032\n",
      "Training loss: 0.06820802390575409\n",
      "Training loss: 0.07063416391611099\n",
      "Training loss: 0.07522586733102798\n",
      "Training loss: 0.07180653512477875\n",
      "Training loss: 0.06980717182159424\n",
      "Training loss: 0.07272219657897949\n",
      "Training loss: 0.06963486224412918\n",
      "Training loss: 0.07124762237071991\n",
      "Training loss: 0.06597909331321716\n",
      "Training loss: 0.07106634974479675\n",
      "Training loss: 0.06850417703390121\n",
      "Training loss: 0.0673794373869896\n",
      "Training loss: 0.07018012553453445\n",
      "Training loss: 0.06915299594402313\n",
      "Training loss: 0.07013185322284698\n",
      "Training loss: 0.0711265280842781\n",
      "Training loss: 0.0726943165063858\n",
      "Training loss: 0.07416868954896927\n",
      "Training loss: 0.07215894758701324\n",
      "Training loss: 0.06522151827812195\n",
      "Training loss: 0.06624734401702881\n",
      "Training loss: 0.07193076610565186\n",
      "Training loss: 0.06868019700050354\n",
      "Training loss: 0.07283549755811691\n",
      "Training loss: 0.0675908699631691\n",
      "Training loss: 0.07032407075166702\n",
      "Training loss: 0.07351432740688324\n",
      "Training loss: 0.06636364012956619\n",
      "Training loss: 0.073603056371212\n",
      "Training loss: 0.06698945164680481\n",
      "Training loss: 0.07038473337888718\n",
      "Training loss: 0.06297828257083893\n",
      "Training loss: 0.06834421306848526\n",
      "Training loss: 0.0659930408000946\n",
      "Training loss: 0.07123884558677673\n",
      "Training loss: 0.07222548872232437\n",
      "Training loss: 0.06590818613767624\n",
      "Training loss: 0.06478432565927505\n",
      "Training loss: 0.0663721114397049\n",
      "Training loss: 0.06587524712085724\n",
      "Training loss: 0.0685659646987915\n",
      "Training loss: 0.06760380417108536\n",
      "Training loss: 0.06769963353872299\n",
      "Training loss: 0.06683729588985443\n",
      "Training loss: 0.0737733542919159\n",
      "Training loss: 0.07027503848075867\n",
      "Training loss: 0.07278626412153244\n",
      "Training loss: 0.06683944910764694\n",
      "Training loss: 0.07248903065919876\n",
      "Training loss: 0.0665760412812233\n",
      "Training loss: 0.06648008525371552\n",
      "Training loss: 0.07242671400308609\n",
      "Training loss: 0.07347939908504486\n",
      "Training loss: 0.07313960790634155\n",
      "Training loss: 0.0675601214170456\n",
      "Training loss: 0.06940597295761108\n",
      "Training loss: 0.07263535261154175\n",
      "Training loss: 0.06807147711515427\n",
      "Training loss: 0.07458407431840897\n",
      "Training loss: 0.07194946706295013\n",
      "Training loss: 0.07548338174819946\n",
      "Training loss: 0.07075842469930649\n",
      "Training loss: 0.0722878947854042\n",
      "Training loss: 0.07566509395837784\n",
      "Training loss: 0.06668708473443985\n",
      "Training loss: 0.0674440935254097\n",
      "Training loss: 0.06276926398277283\n",
      "Training loss: 0.06977622956037521\n",
      "Training loss: 0.07342653721570969\n",
      "Training loss: 0.07612171024084091\n",
      "Training loss: 0.06582832336425781\n",
      "Training loss: 0.06354621052742004\n",
      "Training loss: 0.06647054851055145\n",
      "Training loss: 0.07585456967353821\n",
      "Training loss: 0.06751888990402222\n",
      "Training loss: 0.07037484645843506\n",
      "Training loss: 0.07120176404714584\n",
      "Training loss: 0.06803496181964874\n",
      "Training loss: 0.06787622720003128\n",
      "Training loss: 0.06596937775611877\n",
      "Training loss: 0.0721033364534378\n",
      "Training loss: 0.06770928204059601\n",
      "Training loss: 0.0703783929347992\n",
      "Training loss: 0.07129472494125366\n",
      "Training loss: 0.07167048752307892\n",
      "Training loss: 0.06792274117469788\n",
      "Training loss: 0.0701737105846405\n",
      "Training loss: 0.06635847687721252\n",
      "Training loss: 0.06842802464962006\n",
      "Training loss: 0.07010737806558609\n",
      "Training loss: 0.06888765096664429\n",
      "Training loss: 0.06988361477851868\n",
      "Training loss: 0.06877747923135757\n",
      "Training loss: 0.0669093206524849\n",
      "Training loss: 0.07387975603342056\n",
      "Training loss: 0.06790660321712494\n",
      "Training loss: 0.07018407434225082\n",
      "Training loss: 0.07222237437963486\n",
      "Training loss: 0.06964235007762909\n",
      "Training loss: 0.0676872730255127\n",
      "Training loss: 0.0629146471619606\n",
      "Training loss: 0.06734141707420349\n",
      "Training loss: 0.07101000845432281\n",
      "Training loss: 0.07309044152498245\n",
      "Training loss: 0.06989257782697678\n",
      "Training loss: 0.06805336475372314\n",
      "Training loss: 0.061046089977025986\n",
      "Training loss: 0.0663272812962532\n",
      "Training loss: 0.0787181556224823\n",
      "Training loss: 0.0669688731431961\n",
      "Training loss: 0.06365525722503662\n",
      "Training loss: 0.06748558580875397\n",
      "Training loss: 0.06234052777290344\n",
      "Training loss: 0.06932482123374939\n",
      "Training loss: 0.07656703144311905\n",
      "Training loss: 0.06529765576124191\n",
      "Training loss: 0.06289792060852051\n",
      "Training loss: 0.07023191452026367\n",
      "Training loss: 0.07478336244821548\n",
      "Training loss: 0.0669076070189476\n",
      "Training loss: 0.07324856519699097\n",
      "Training loss: 0.07033839076757431\n",
      "Training loss: 0.07337569445371628\n",
      "Training loss: 0.06754684448242188\n",
      "Training loss: 0.06595002859830856\n",
      "Training loss: 0.07048235833644867\n",
      "Training loss: 0.0672551840543747\n",
      "Training loss: 0.07223846763372421\n",
      "Training loss: 0.06979665905237198\n",
      "Training loss: 0.0692405104637146\n",
      "Training loss: 0.07203121483325958\n",
      "Training loss: 0.07271846383810043\n",
      "Training loss: 0.0719766691327095\n",
      "Training loss: 0.07171715050935745\n",
      "Training loss: 0.0782429426908493\n",
      "Training loss: 0.07548628747463226\n",
      "Training loss: 0.07135765254497528\n",
      "Training loss: 0.07212965190410614\n",
      "Training loss: 0.0677846148610115\n",
      "Training loss: 0.07898414134979248\n",
      "Training loss: 0.07762639969587326\n",
      "Training loss: 0.06948131322860718\n",
      "Training loss: 0.07121875137090683\n",
      "Training loss: 0.07443755865097046\n",
      "Training loss: 0.07440391182899475\n",
      "Training loss: 0.0757865458726883\n",
      "Training loss: 0.07536312937736511\n",
      "Training loss: 0.07323905825614929\n",
      "Training loss: 0.0777704045176506\n",
      "Training loss: 0.07477599382400513\n",
      "Training loss: 0.06877922266721725\n",
      "Training loss: 0.0689241960644722\n",
      "Training loss: 0.07467880100011826\n",
      "Training loss: 0.07316870987415314\n",
      "Training loss: 0.06897971779108047\n",
      "Training loss: 0.07140644639730453\n",
      "Training loss: 0.07409719377756119\n",
      "Training loss: 0.07139816135168076\n",
      "Training loss: 0.07150627672672272\n",
      "Training loss: 0.08162887394428253\n",
      "Training loss: 0.07032222300767899\n",
      "Training loss: 0.07620589435100555\n",
      "Training loss: 0.07295271754264832\n",
      "Training loss: 0.07184736430644989\n",
      "Training loss: 0.07753818482160568\n",
      "Training loss: 0.0785626471042633\n",
      "Training loss: 0.07290191203355789\n",
      "Training loss: 0.07723920792341232\n",
      "Training loss: 0.08170533925294876\n",
      "Training loss: 0.0779687836766243\n",
      "Training loss: 0.07509011775255203\n",
      "Training loss: 0.07469762116670609\n",
      "Training loss: 0.0673416331410408\n",
      "Training loss: 0.0708957314491272\n",
      "Training loss: 0.07226589322090149\n",
      "Training loss: 0.06963741034269333\n",
      "Training loss: 0.07006943970918655\n",
      "Training loss: 0.075761578977108\n",
      "Training loss: 0.07341182231903076\n",
      "Training loss: 0.07208520919084549\n",
      "Training loss: 0.07086572051048279\n",
      "Training loss: 0.07445169240236282\n",
      "Training loss: 0.07842046022415161\n",
      "Training loss: 0.06994128972291946\n",
      "Training loss: 0.07462956011295319\n",
      "Training loss: 0.07785698771476746\n",
      "Training loss: 0.0697847381234169\n",
      "Training loss: 0.06804951280355453\n",
      "Training loss: 0.07787981629371643\n",
      "Training loss: 0.07439398020505905\n",
      "Training loss: 0.0709320604801178\n",
      "Training loss: 0.07742197066545486\n",
      "Training loss: 0.0681750476360321\n",
      "Training loss: 0.072187140583992\n",
      "Training loss: 0.07672283798456192\n",
      "Training loss: 0.0756048783659935\n",
      "Training loss: 0.07932101935148239\n",
      "Training loss: 0.07467322796583176\n",
      "Training loss: 0.0744771659374237\n",
      "Training loss: 0.06733181327581406\n",
      "Training loss: 0.06922054290771484\n",
      "Training loss: 0.07218992710113525\n",
      "Training loss: 0.07363889366388321\n",
      "Training loss: 0.06738708913326263\n",
      "Training loss: 0.06945764273405075\n",
      "Training loss: 0.07032141834497452\n",
      "Training loss: 0.06689777970314026\n",
      "Training loss: 0.07106614112854004\n",
      "Training loss: 0.07027124613523483\n",
      "Training loss: 0.0693240761756897\n",
      "Training loss: 0.06644999235868454\n",
      "Training loss: 0.0640236884355545\n",
      "Training loss: 0.06699301302433014\n",
      "Training loss: 0.06425578147172928\n",
      "Training loss: 0.07115377485752106\n",
      "Training loss: 0.0687747597694397\n",
      "Training loss: 0.07508709281682968\n",
      "Training loss: 0.06877166032791138\n",
      "Training loss: 0.06733372062444687\n",
      "Training loss: 0.07143794000148773\n",
      "Training loss: 0.07198958843946457\n",
      "Training loss: 0.06576118618249893\n",
      "Training loss: 0.07112659513950348\n",
      "Training loss: 0.06801077723503113\n",
      "Training loss: 0.06827101111412048\n",
      "Training loss: 0.06704020500183105\n",
      "Training loss: 0.06724348664283752\n",
      "Training loss: 0.07223916798830032\n",
      "Training loss: 0.06151998043060303\n",
      "Training loss: 0.07137175649404526\n",
      "Training loss: 0.0740947425365448\n",
      "Training loss: 0.06679485738277435\n",
      "Training loss: 0.0751836895942688\n",
      "Training loss: 0.07058876752853394\n",
      "Training loss: 0.06893107295036316\n",
      "Training loss: 0.07264954596757889\n",
      "Training loss: 0.07403794676065445\n",
      "Training loss: 0.0728798508644104\n",
      "Training loss: 0.07394268363714218\n",
      "Training loss: 0.08028236031532288\n",
      "Training loss: 0.07486476004123688\n",
      "Training loss: 0.07398335635662079\n",
      "Training loss: 0.07398111373186111\n",
      "Training loss: 0.08783169090747833\n",
      "Training loss: 0.08154377341270447\n",
      "Training loss: 0.08008228987455368\n",
      "Training loss: 0.0767093077301979\n",
      "Training loss: 0.07804551720619202\n",
      "Training loss: 0.07467201352119446\n",
      "Training loss: 0.07896426320075989\n",
      "Training loss: 0.07580993324518204\n",
      "Training loss: 0.0745912417769432\n",
      "Training loss: 0.0732530951499939\n",
      "Training loss: 0.07775279134511948\n",
      "Training loss: 0.07992938160896301\n",
      "Training loss: 0.07346967607736588\n",
      "Training loss: 0.07341606169939041\n",
      "Training loss: 0.07595653831958771\n",
      "Training loss: 0.073267363011837\n",
      "Training loss: 0.07074803858995438\n",
      "Training loss: 0.07096561789512634\n",
      "Training loss: 0.071159727871418\n",
      "Training loss: 0.07077348232269287\n",
      "Training loss: 0.0790417343378067\n",
      "Training loss: 0.07119922339916229\n",
      "Training loss: 0.06978844851255417\n",
      "Training loss: 0.07170919328927994\n",
      "Training loss: 0.07097339630126953\n",
      "Training loss: 0.07286345958709717\n",
      "Training loss: 0.07134456932544708\n",
      "Training loss: 0.07053525000810623\n",
      "Training loss: 0.06968823075294495\n",
      "Training loss: 0.07149221003055573\n",
      "Training loss: 0.07204921543598175\n",
      "Training loss: 0.07216671854257584\n",
      "Training loss: 0.07551951706409454\n",
      "Training loss: 0.06437879800796509\n",
      "Training loss: 0.06717899441719055\n",
      "Training loss: 0.07058899104595184\n",
      "Training loss: 0.06719059497117996\n",
      "Training loss: 0.06560207903385162\n",
      "Training loss: 0.07130958884954453\n",
      "Training loss: 0.07215025275945663\n",
      "Training loss: 0.07340945303440094\n",
      "Training loss: 0.07489217072725296\n",
      "Training loss: 0.06281926482915878\n",
      "Training loss: 0.0732598677277565\n",
      "Training loss: 0.06942974776029587\n",
      "Training loss: 0.0671372339129448\n",
      "Training loss: 0.07422137260437012\n",
      "Training loss: 0.06387841701507568\n",
      "Training loss: 0.07012858986854553\n",
      "Training loss: 0.07131636887788773\n",
      "Training loss: 0.07209814339876175\n",
      "Training loss: 0.06936651468276978\n",
      "Training loss: 0.06417465209960938\n",
      "Training loss: 0.06927239149808884\n",
      "Training loss: 0.07744234800338745\n",
      "Training loss: 0.07277827709913254\n",
      "Training loss: 0.06962850689888\n",
      "Training loss: 0.07213643938302994\n",
      "Training loss: 0.06760158389806747\n",
      "Training loss: 0.07991013675928116\n",
      "Training loss: 0.07764433324337006\n",
      "Training loss: 0.07658080011606216\n",
      "Training loss: 0.07510107010602951\n",
      "Training loss: 0.06950495392084122\n",
      "Training loss: 0.0706857442855835\n",
      "Training loss: 0.06874842941761017\n",
      "Training loss: 0.07562540471553802\n",
      "Training loss: 0.06989861279726028\n",
      "Training loss: 0.07483907043933868\n",
      "Training loss: 0.08236513286828995\n",
      "Training loss: 0.07508382201194763\n",
      "Training loss: 0.07049298286437988\n",
      "Training loss: 0.06766004860401154\n",
      "Training loss: 0.07211638987064362\n",
      "Training loss: 0.07077571749687195\n",
      "Training loss: 0.07669773697853088\n",
      "Training loss: 0.07067174464464188\n",
      "Training loss: 0.07032216340303421\n",
      "Training loss: 0.07452229410409927\n",
      "Training loss: 0.06776048988103867\n",
      "Training loss: 0.06955072283744812\n",
      "Training loss: 0.06954454630613327\n",
      "Training loss: 0.07201429456472397\n",
      "Training loss: 0.07421819865703583\n",
      "Training loss: 0.07223054766654968\n",
      "Training loss: 0.07274969667196274\n",
      "Training loss: 0.06581790000200272\n",
      "Training loss: 0.07096787542104721\n",
      "Training loss: 0.06700663268566132\n",
      "Training loss: 0.06416948139667511\n",
      "Training loss: 0.07340476661920547\n",
      "Training loss: 0.06861802190542221\n",
      "Training loss: 0.07865278422832489\n",
      "Training loss: 0.06906053423881531\n",
      "Training loss: 0.07278748601675034\n",
      "Training loss: 0.06657202541828156\n",
      "Training loss: 0.06900571286678314\n",
      "Training loss: 0.06648019701242447\n",
      "Training loss: 0.06770867109298706\n",
      "Training loss: 0.07126516848802567\n",
      "Training loss: 0.07475476711988449\n",
      "Training loss: 0.06329017877578735\n",
      "Training loss: 0.0718221515417099\n",
      "Training loss: 0.06432432681322098\n",
      "Training loss: 0.07055328786373138\n",
      "Training loss: 0.07352938503026962\n",
      "Training loss: 0.07102857530117035\n",
      "Training loss: 0.06744677573442459\n",
      "Training loss: 0.06948939710855484\n",
      "Training loss: 0.06695474684238434\n",
      "Training loss: 0.07706809788942337\n",
      "Training loss: 0.07472418248653412\n",
      "Training loss: 0.07043668627738953\n",
      "Training loss: 0.06387828290462494\n",
      "Training loss: 0.0677502229809761\n",
      "Training loss: 0.06537263095378876\n",
      "Training loss: 0.06729557365179062\n",
      "Training loss: 0.06904393434524536\n",
      "Training loss: 0.06756124645471573\n",
      "Training loss: 0.06776776909828186\n",
      "Training loss: 0.06967933475971222\n",
      "Training loss: 0.06876057386398315\n",
      "Training loss: 0.07373899221420288\n",
      "Training loss: 0.06764557957649231\n",
      "Training loss: 0.06576035916805267\n",
      "Training loss: 0.06827990710735321\n",
      "Training loss: 0.07153553515672684\n",
      "Training loss: 0.07163318991661072\n",
      "Training loss: 0.06650520116090775\n",
      "Training loss: 0.08858350664377213\n",
      "Training loss: 0.0683143213391304\n",
      "Training loss: 0.06747733801603317\n",
      "Training loss: 0.06698403507471085\n",
      "Training loss: 0.0689549669623375\n",
      "Training loss: 0.07275012880563736\n",
      "Training loss: 0.06803122907876968\n",
      "Training loss: 0.06758657842874527\n",
      "Training loss: 0.06631467491388321\n",
      "Training loss: 0.06496867537498474\n",
      "Training loss: 0.06061207503080368\n",
      "Training loss: 0.08059000968933105\n",
      "Training loss: 0.06829026341438293\n",
      "Training loss: 0.06981958448886871\n",
      "Training loss: 0.06789959222078323\n",
      "Training loss: 0.072486512362957\n",
      "Training loss: 0.06693746894598007\n",
      "Training loss: 0.07124576717615128\n",
      "Training loss: 0.07412688434123993\n",
      "Training loss: 0.06869415938854218\n",
      "Training loss: 0.0708165094256401\n",
      "Training loss: 0.07513563334941864\n",
      "Training loss: 0.06340857595205307\n",
      "Training loss: 0.06466414034366608\n",
      "Training loss: 0.06744054704904556\n",
      "Training loss: 0.06474927812814713\n",
      "Training loss: 0.07102354615926743\n",
      "Training loss: 0.06582745909690857\n",
      "Training loss: 0.06773347407579422\n",
      "Training loss: 0.0653257891535759\n",
      "Training loss: 0.07412347942590714\n",
      "Training loss: 0.06515194475650787\n",
      "Training loss: 0.07327636331319809\n",
      "Training loss: 0.06883157789707184\n",
      "Training loss: 0.07751554250717163\n",
      "Training loss: 0.06701375544071198\n",
      "Training loss: 0.0680122971534729\n",
      "Training loss: 0.06991558521986008\n",
      "Training loss: 0.0710454136133194\n",
      "Training loss: 0.07611408829689026\n",
      "Training loss: 0.07103770971298218\n",
      "Training loss: 0.07150638848543167\n",
      "Training loss: 0.06509818136692047\n",
      "Training loss: 0.06683347374200821\n",
      "Training loss: 0.0663451999425888\n",
      "Training loss: 0.07038549333810806\n",
      "Training loss: 0.07261252403259277\n",
      "Training loss: 0.06449443846940994\n",
      "Training loss: 0.07018430531024933\n",
      "Training loss: 0.06409874558448792\n",
      "Training loss: 0.06143905967473984\n",
      "Training loss: 0.07210514694452286\n",
      "Training loss: 0.06893067061901093\n",
      "Training loss: 0.07065929472446442\n",
      "Training loss: 0.06591571867465973\n",
      "Training loss: 0.06582092493772507\n",
      "Training loss: 0.0655154213309288\n",
      "Training loss: 0.06603116542100906\n",
      "Training loss: 0.060918956995010376\n",
      "Training loss: 0.07062016427516937\n",
      "Training loss: 0.0664340928196907\n",
      "Training loss: 0.0640420913696289\n",
      "Training loss: 0.06949873268604279\n",
      "Training loss: 0.06515457481145859\n",
      "Training loss: 0.06621402502059937\n",
      "Training loss: 0.0652213990688324\n",
      "Training loss: 0.06617389619350433\n",
      "Training loss: 0.06436247378587723\n",
      "Training loss: 0.06176191195845604\n",
      "Training loss: 0.06690961122512817\n",
      "Training loss: 0.0775991678237915\n",
      "Training loss: 0.0649004653096199\n",
      "Training loss: 0.0735391229391098\n",
      "Training loss: 0.06255024671554565\n",
      "Training loss: 0.07024995982646942\n",
      "Training loss: 0.06872575730085373\n",
      "Training loss: 0.06807173788547516\n",
      "Training loss: 0.0683283880352974\n",
      "Training loss: 0.06816951185464859\n",
      "Training loss: 0.06891752779483795\n",
      "Training loss: 0.07398594170808792\n",
      "Training loss: 0.07314971834421158\n",
      "Training loss: 0.06772630661725998\n",
      "Training loss: 0.07453538477420807\n",
      "Training loss: 0.07198821753263474\n",
      "Training loss: 0.06923257559537888\n",
      "Training loss: 0.069789819419384\n",
      "Training loss: 0.06524069607257843\n",
      "Training loss: 0.07057063281536102\n",
      "Training loss: 0.07326830923557281\n",
      "Training loss: 0.06792894750833511\n",
      "Training loss: 0.07592164725065231\n",
      "Training loss: 0.0719442144036293\n",
      "Training loss: 0.06506221741437912\n",
      "Training loss: 0.06875680387020111\n",
      "Training loss: 0.06281014531850815\n",
      "Training loss: 0.06832552701234818\n",
      "Training loss: 0.06907135248184204\n",
      "Training loss: 0.07150034606456757\n",
      "Training loss: 0.07085627317428589\n",
      "Training loss: 0.06702346354722977\n",
      "Training loss: 0.06884104758501053\n",
      "Training loss: 0.0688125267624855\n",
      "Training loss: 0.07581303268671036\n",
      "Training loss: 0.06564566493034363\n",
      "Training loss: 0.06519366055727005\n",
      "Training loss: 0.06858610361814499\n",
      "Training loss: 0.06657939404249191\n",
      "Training loss: 0.06789065897464752\n",
      "Training loss: 0.06988154351711273\n",
      "Training loss: 0.06729909777641296\n",
      "Training loss: 0.06438826769590378\n",
      "Training loss: 0.06577928364276886\n",
      "Training loss: 0.06672347337007523\n",
      "Training loss: 0.07193906605243683\n",
      "Training loss: 0.07072668522596359\n",
      "Training loss: 0.06753368675708771\n",
      "Training loss: 0.07516945153474808\n",
      "Training loss: 0.06707718223333359\n",
      "Training loss: 0.06609344482421875\n",
      "Training loss: 0.07105770707130432\n",
      "Training loss: 0.07202190905809402\n",
      "Training loss: 0.06511036306619644\n",
      "Training loss: 0.06958115845918655\n",
      "Training loss: 0.07265394181013107\n",
      "Training loss: 0.06512570381164551\n",
      "Training loss: 0.07482505589723587\n",
      "Training loss: 0.06840598583221436\n",
      "Training loss: 0.0747310072183609\n",
      "Training loss: 0.07097623497247696\n",
      "Training loss: 0.06730137765407562\n",
      "Training loss: 0.06844332069158554\n",
      "Training loss: 0.07642339915037155\n",
      "Training loss: 0.07397498935461044\n",
      "Training loss: 0.06776456534862518\n",
      "Training loss: 0.07262290269136429\n",
      "Training loss: 0.06543265283107758\n",
      "Training loss: 0.07071980088949203\n",
      "Training loss: 0.07045488059520721\n",
      "Training loss: 0.071167953312397\n",
      "Training loss: 0.07204707711935043\n",
      "Training loss: 0.06920718401670456\n",
      "Training loss: 0.06545918434858322\n",
      "Training loss: 0.07038112729787827\n",
      "Training loss: 0.06405403465032578\n",
      "Training loss: 0.06677433848381042\n",
      "Training loss: 0.06920318305492401\n",
      "Training loss: 0.06859046220779419\n",
      "Training loss: 0.07676561176776886\n",
      "Training loss: 0.06756911426782608\n",
      "Training loss: 0.0715467631816864\n",
      "Training loss: 0.06441055983304977\n",
      "Training loss: 0.06194473057985306\n",
      "Training loss: 0.06427641957998276\n",
      "Training loss: 0.06563065201044083\n",
      "Training loss: 0.07325658947229385\n",
      "Training loss: 0.0698525533080101\n",
      "Training loss: 0.061373062431812286\n",
      "Training loss: 0.0660988837480545\n",
      "Training loss: 0.0630388930439949\n",
      "Training loss: 0.07014265656471252\n",
      "Training loss: 0.06751947849988937\n",
      "Training loss: 0.06601739674806595\n",
      "Training loss: 0.0646371990442276\n",
      "Training loss: 0.06812074780464172\n",
      "Training loss: 0.06666344404220581\n",
      "Training loss: 0.060820069164037704\n",
      "Training loss: 0.06637942045927048\n",
      "Training loss: 0.071429543197155\n",
      "Training loss: 0.07325384765863419\n",
      "Training loss: 0.06555523723363876\n",
      "Training loss: 0.06593571603298187\n",
      "Training loss: 0.0726163387298584\n",
      "Training loss: 0.06611495465040207\n",
      "Training loss: 0.06558506935834885\n",
      "Training loss: 0.06902852654457092\n",
      "Training loss: 0.06741832941770554\n",
      "Training loss: 0.06603893637657166\n",
      "Training loss: 0.07105886936187744\n",
      "Training loss: 0.06783322244882584\n",
      "Training loss: 0.06735190749168396\n",
      "Training loss: 0.07223764806985855\n",
      "Training loss: 0.06001347303390503\n",
      "Training loss: 0.06545683741569519\n",
      "Training loss: 0.06617461144924164\n",
      "Training loss: 0.06744717806577682\n",
      "Training loss: 0.068549744784832\n",
      "Training loss: 0.0710635557770729\n",
      "Training loss: 0.06496550887823105\n",
      "Training loss: 0.0643492192029953\n",
      "Training loss: 0.06402917951345444\n",
      "Training loss: 0.0656762346625328\n",
      "Training loss: 0.06398183107376099\n",
      "Training loss: 0.06501497328281403\n",
      "Training loss: 0.06686732172966003\n",
      "Training loss: 0.06876952201128006\n",
      "Training loss: 0.06533320248126984\n",
      "Training loss: 0.07099819928407669\n",
      "Training loss: 0.06616291403770447\n",
      "Training loss: 0.06844203919172287\n",
      "Training loss: 0.06682471185922623\n",
      "Training loss: 0.065983846783638\n",
      "Training loss: 0.06449843943119049\n",
      "Training loss: 0.06562148779630661\n",
      "Training loss: 0.06555372476577759\n",
      "Training loss: 0.06277458369731903\n",
      "Training loss: 0.06280780583620071\n",
      "Training loss: 0.06785810738801956\n",
      "Training loss: 0.06343985348939896\n",
      "Training loss: 0.0671892836689949\n",
      "Training loss: 0.07021045684814453\n",
      "Training loss: 0.07129461318254471\n",
      "Training loss: 0.06142695993185043\n",
      "Training loss: 0.0704972967505455\n",
      "Training loss: 0.06831774115562439\n",
      "Training loss: 0.07470021396875381\n",
      "Training loss: 0.06578708440065384\n",
      "Training loss: 0.06207539513707161\n",
      "Training loss: 0.06111731007695198\n",
      "Training loss: 0.06896873563528061\n",
      "Training loss: 0.07182172685861588\n",
      "Training loss: 0.06789541244506836\n",
      "Training loss: 0.06763230264186859\n",
      "Training loss: 0.0712592750787735\n",
      "Training loss: 0.061170339584350586\n",
      "Training loss: 0.06355315446853638\n",
      "Training loss: 0.06813161820173264\n",
      "Training loss: 0.07452642172574997\n",
      "Training loss: 0.07148020714521408\n",
      "Training loss: 0.06066405400633812\n",
      "Training loss: 0.0631226971745491\n",
      "Training loss: 0.06416690349578857\n",
      "Training loss: 0.06638719886541367\n",
      "Training loss: 0.0655844584107399\n",
      "Training loss: 0.06753750145435333\n",
      "Training loss: 0.06548663973808289\n",
      "Training loss: 0.06390169262886047\n",
      "Training loss: 0.07130857557058334\n",
      "Training loss: 0.06429006159305573\n",
      "Training loss: 0.06581857800483704\n",
      "Training loss: 0.06917939335107803\n",
      "Training loss: 0.07017889618873596\n",
      "Training loss: 0.06847274303436279\n",
      "Training loss: 0.06517624109983444\n",
      "Training loss: 0.06739409267902374\n",
      "Training loss: 0.07376240193843842\n",
      "Training loss: 0.07927927374839783\n",
      "Training loss: 0.07192140072584152\n",
      "Training loss: 0.07655345648527145\n",
      "Training loss: 0.07267898321151733\n",
      "Training loss: 0.09353861957788467\n",
      "Training loss: 0.07005657255649567\n",
      "Training loss: 0.06991621851921082\n",
      "Training loss: 0.07351013273000717\n",
      "Training loss: 0.06909574568271637\n",
      "Training loss: 0.08313862234354019\n",
      "Training loss: 0.07233542203903198\n",
      "Training loss: 0.07703062891960144\n",
      "Training loss: 0.07688794285058975\n",
      "Training loss: 0.07548702508211136\n",
      "Training loss: 0.07815348356962204\n",
      "Training loss: 0.07089892774820328\n",
      "Training loss: 0.07146485149860382\n",
      "Training loss: 0.06677110493183136\n",
      "Training loss: 0.07077578455209732\n",
      "Training loss: 0.06968845427036285\n",
      "Training loss: 0.07386388629674911\n",
      "Training loss: 0.07144878804683685\n",
      "Training loss: 0.07353305071592331\n",
      "Training loss: 0.0683935359120369\n",
      "Training loss: 0.07253400981426239\n",
      "Training loss: 0.07076941430568695\n",
      "Training loss: 0.06377245485782623\n",
      "Training loss: 0.07088427990674973\n",
      "Training loss: 0.07150840759277344\n",
      "Training loss: 0.06636610627174377\n",
      "Training loss: 0.06589073687791824\n",
      "Training loss: 0.0713142678141594\n",
      "Training loss: 0.07077370584011078\n",
      "Training loss: 0.06431923806667328\n",
      "Training loss: 0.06630814075469971\n",
      "Training loss: 0.06589767336845398\n",
      "Training loss: 0.05954409018158913\n",
      "Training loss: 0.07025718688964844\n",
      "Training loss: 0.06574536859989166\n",
      "Training loss: 0.06407269090414047\n",
      "Training loss: 0.06480415910482407\n",
      "Training loss: 0.06923821568489075\n",
      "Training loss: 0.06695764511823654\n",
      "Training loss: 0.06483377516269684\n",
      "Training loss: 0.06340342015028\n",
      "Training loss: 0.0645485445857048\n",
      "Training loss: 0.0656256154179573\n",
      "Training loss: 0.06692676991224289\n",
      "Training loss: 0.06932280212640762\n",
      "Training loss: 0.06727730482816696\n",
      "Training loss: 0.06869368255138397\n",
      "Training loss: 0.07158777117729187\n",
      "Training loss: 0.06728435307741165\n",
      "Training loss: 0.07150916755199432\n",
      "Training loss: 0.06909936666488647\n",
      "Training loss: 0.06773389875888824\n",
      "Training loss: 0.067790687084198\n",
      "Training loss: 0.07141873240470886\n",
      "Training loss: 0.06826005131006241\n",
      "Training loss: 0.06581699103116989\n",
      "Training loss: 0.0699022114276886\n",
      "Training loss: 0.07295367866754532\n",
      "Training loss: 0.06511317938566208\n",
      "Training loss: 0.06339941918849945\n",
      "Training loss: 0.07119867205619812\n",
      "Training loss: 0.06528493016958237\n",
      "Training loss: 0.06791354715824127\n",
      "Training loss: 0.0661420077085495\n",
      "Training loss: 0.06879734992980957\n",
      "Training loss: 0.06850817799568176\n",
      "Training loss: 0.0687808096408844\n",
      "Training loss: 0.06988480687141418\n",
      "Training loss: 0.0645824447274208\n",
      "Training loss: 0.07121206820011139\n",
      "Training loss: 0.06922982633113861\n",
      "Training loss: 0.06442944705486298\n",
      "Training loss: 0.07543322443962097\n",
      "Training loss: 0.07086361199617386\n",
      "Training loss: 0.0678890272974968\n",
      "Training loss: 0.06878820806741714\n",
      "Training loss: 0.06938641518354416\n",
      "Training loss: 0.06542391330003738\n",
      "Training loss: 0.06017407774925232\n",
      "Training loss: 0.07121486961841583\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    log_ps = model(X_train_tensor)\n",
    "    loss = criterion(log_ps, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Training loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-nqCC3cq1Er"
   },
   "outputs": [],
   "source": [
    "y_pred = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhdNTZUfq1CW"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4WzvYcFcq078",
    "outputId": "087a9b8b-d650-46d2-9f52-e257cad277b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 2, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 186,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnNrAcF0q05G"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M1IEVG8nq0wg",
    "outputId": "239cb6b5-63cb-4607-d690-f1b66ac875f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696815718157181"
      ]
     },
     "execution_count": 188,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_train_tensor.to('cpu')),np.array( y_pred.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Gri5NaaRq0nP",
    "outputId": "ee525531-96f7-4e01-d0ca-ff9e9010a9a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test_np = preprocessor.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5td2c04bq0ka"
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.from_numpy(X_test_np).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48EaHSIYq0b0"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(model(X_test_tensor),axis=1)\n",
    "y_pred = y_pred.to('cpu').numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyLfpcEqq0ZM"
   },
   "outputs": [],
   "source": [
    "output_nn = pd.DataFrame({'id':test_df.id.values,'outage_duration':y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Dw-Q4sLDuVV"
   },
   "source": [
    "### 5.3 Neural Network - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgMX0vGJDh9u"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1330, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, 256)\n",
    "        self.fc6 = nn.Linear(256, 256)\n",
    "        self.fc7 = nn.Linear(256, 128)\n",
    "        self.fc8 = nn.Linear(128, 128)\n",
    "        self.fc9 = nn.Linear(128, 50)\n",
    "        self.fc10 = nn.Linear(50, 10)\n",
    "        self.fc11 = nn.Linear(10, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened        \n",
    "        x = self.dropout(F.leaky_relu(self.fc1(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc2(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc3(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc4(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc5(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc6(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc7(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc8(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc9(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc10(x)))\n",
    "        x = F.log_softmax(self.fc7(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJG1v-N5Dh91"
   },
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfNi5DVYErzF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VuWIwfsFauz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_datasets(batch_size):\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    dataset = TensorDataset( X_train_tensor, y_train_tensor )\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_final)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    return train_loader,  valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "MR831SEhEruv",
    "outputId": "07d0f5ef-2e88-418f-f940-072a175bf79f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e0417464f118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorchtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# to track the training loss as the model trains\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\PHD\\pytorchtools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(train_loader, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nK4TDXGLDh99"
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train_tensor =  torch.Tensor(train_final.outage_duration.values).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "b9R_mhEUDh9_",
    "outputId": "8e45d594-bef7-471b-c57c-ff772adc72b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training loss: 0.1354953646659851\n",
      "Training loss: 0.14191880822181702\n",
      "Training loss: 0.1379302740097046\n",
      "Training loss: 0.1288977414369583\n",
      "Training loss: 0.13673637807369232\n",
      "Training loss: 0.13814379274845123\n",
      "Training loss: 0.13252395391464233\n",
      "Training loss: 0.13412542641162872\n",
      "Training loss: 0.1537761241197586\n",
      "Training loss: 0.13998520374298096\n",
      "Training loss: 0.13475528359413147\n",
      "Training loss: 0.14020457863807678\n",
      "Training loss: 0.13688607513904572\n",
      "Training loss: 0.14135347306728363\n",
      "Training loss: 0.1372460573911667\n",
      "Training loss: 0.13814012706279755\n",
      "Training loss: 0.1364450603723526\n",
      "Training loss: 0.14482347667217255\n",
      "Training loss: 0.1350596398115158\n",
      "Training loss: 0.13199393451213837\n",
      "Training loss: 0.1300664097070694\n",
      "Training loss: 0.13455703854560852\n",
      "Training loss: 0.14048106968402863\n",
      "Training loss: 0.12928713858127594\n",
      "Training loss: 0.13232630491256714\n",
      "Training loss: 0.13475997745990753\n",
      "Training loss: 0.13141265511512756\n",
      "Training loss: 0.15862718224525452\n",
      "Training loss: 0.13904887437820435\n",
      "Training loss: 0.13356728851795197\n",
      "Training loss: 0.13874833285808563\n",
      "Training loss: 0.13147325813770294\n",
      "Training loss: 0.1334504932165146\n",
      "Training loss: 0.13461799919605255\n",
      "Training loss: 0.13095888495445251\n",
      "Training loss: 0.1357051283121109\n",
      "Training loss: 0.12946422398090363\n",
      "Training loss: 0.13522222638130188\n",
      "Training loss: 0.13005681335926056\n",
      "Training loss: 0.12851937115192413\n",
      "Training loss: 0.1393972635269165\n",
      "Training loss: 0.12586429715156555\n",
      "Training loss: 0.13493278622627258\n",
      "Training loss: 0.13472986221313477\n",
      "Training loss: 0.14480061829090118\n",
      "Training loss: 0.13161800801753998\n",
      "Training loss: 0.1315358728170395\n",
      "Training loss: 0.12835043668746948\n",
      "Training loss: 0.12778626382350922\n",
      "Training loss: 0.13147254288196564\n",
      "Training loss: 0.13099609315395355\n",
      "Training loss: 0.13068881630897522\n",
      "Training loss: 0.13616971671581268\n",
      "Training loss: 0.13634036481380463\n",
      "Training loss: 0.13408784568309784\n",
      "Training loss: 0.1423463523387909\n",
      "Training loss: 0.14730305969715118\n",
      "Training loss: 0.14197172224521637\n",
      "Training loss: 0.13556234538555145\n",
      "Training loss: 0.14508499205112457\n",
      "Training loss: 0.13306567072868347\n",
      "Training loss: 0.13423247635364532\n",
      "Training loss: 0.1385420858860016\n",
      "Training loss: 0.13013865053653717\n",
      "Training loss: 0.13797110319137573\n",
      "Training loss: 0.14553101360797882\n",
      "Training loss: 0.13591334223747253\n",
      "Training loss: 0.13510456681251526\n",
      "Training loss: 0.14494390785694122\n",
      "Training loss: 0.13778960704803467\n",
      "Training loss: 0.1338006556034088\n",
      "Training loss: 0.13385659456253052\n",
      "Training loss: 0.1302381008863449\n",
      "Training loss: 0.13486535847187042\n",
      "Training loss: 0.13526375591754913\n",
      "Training loss: 0.13407258689403534\n",
      "Training loss: 0.13948020339012146\n",
      "Training loss: 0.13799451291561127\n",
      "Training loss: 0.1264033019542694\n",
      "Training loss: 0.1337764710187912\n",
      "Training loss: 0.14087414741516113\n",
      "Training loss: 0.1347460001707077\n",
      "Training loss: 0.12550385296344757\n",
      "Training loss: 0.130305215716362\n",
      "Training loss: 0.12773846089839935\n",
      "Training loss: 0.13028542697429657\n",
      "Training loss: 0.12504325807094574\n",
      "Training loss: 0.1383732259273529\n",
      "Training loss: 0.13300564885139465\n",
      "Training loss: 0.13044312596321106\n",
      "Training loss: 0.14195924997329712\n",
      "Training loss: 0.12771199643611908\n",
      "Training loss: 0.13519492745399475\n",
      "Training loss: 0.12786231935024261\n",
      "Training loss: 0.1365930438041687\n",
      "Training loss: 0.134329691529274\n",
      "Training loss: 0.13777683675289154\n",
      "Training loss: 0.12803910672664642\n",
      "Training loss: 0.13877522945404053\n",
      "Training loss: 0.13059474527835846\n",
      "Training loss: 0.1358024775981903\n",
      "Training loss: 0.13175101578235626\n",
      "Training loss: 0.1329725980758667\n",
      "Training loss: 0.12829747796058655\n",
      "Training loss: 0.13226532936096191\n",
      "Training loss: 0.13043493032455444\n",
      "Training loss: 0.13081800937652588\n",
      "Training loss: 0.14406085014343262\n",
      "Training loss: 0.13484260439872742\n",
      "Training loss: 0.13332706689834595\n",
      "Training loss: 0.1360761970281601\n",
      "Training loss: 0.13543270528316498\n",
      "Training loss: 0.1270529329776764\n",
      "Training loss: 0.1296338587999344\n",
      "Training loss: 0.13401320576667786\n",
      "Training loss: 0.12840773165225983\n",
      "Training loss: 0.12578284740447998\n",
      "Training loss: 0.12856584787368774\n",
      "Training loss: 0.13187648355960846\n",
      "Training loss: 0.12399190664291382\n",
      "Training loss: 0.12505410611629486\n",
      "Training loss: 0.1251591145992279\n",
      "Training loss: 0.14083899557590485\n",
      "Training loss: 0.12608209252357483\n",
      "Training loss: 0.13645043969154358\n",
      "Training loss: 0.1443122774362564\n",
      "Training loss: 0.1330590397119522\n",
      "Training loss: 0.1308247148990631\n",
      "Training loss: 0.13287587463855743\n",
      "Training loss: 0.13095803558826447\n",
      "Training loss: 0.13457423448562622\n",
      "Training loss: 0.13134202361106873\n",
      "Training loss: 0.13273024559020996\n",
      "Training loss: 0.1328064650297165\n",
      "Training loss: 0.1309683471918106\n",
      "Training loss: 0.13141071796417236\n",
      "Training loss: 0.13134291768074036\n",
      "Training loss: 0.13411536812782288\n",
      "Training loss: 0.14340975880622864\n",
      "Training loss: 0.1319563090801239\n",
      "Training loss: 0.14234808087348938\n",
      "Training loss: 0.1350516378879547\n",
      "Training loss: 0.1289324015378952\n",
      "Training loss: 0.1352728307247162\n",
      "Training loss: 0.1253218650817871\n",
      "Training loss: 0.13658994436264038\n",
      "Training loss: 0.12867820262908936\n",
      "Training loss: 0.1263294816017151\n",
      "Training loss: 0.13218143582344055\n",
      "Training loss: 0.12655311822891235\n",
      "Training loss: 0.1351432502269745\n",
      "Training loss: 0.1374858021736145\n",
      "Training loss: 0.1329174041748047\n",
      "Training loss: 0.12747368216514587\n",
      "Training loss: 0.13223831355571747\n",
      "Training loss: 0.1323518604040146\n",
      "Training loss: 0.12931609153747559\n",
      "Training loss: 0.1388850212097168\n",
      "Training loss: 0.134249746799469\n",
      "Training loss: 0.12690193951129913\n",
      "Training loss: 0.1322948932647705\n",
      "Training loss: 0.12191078811883926\n",
      "Training loss: 0.1316967010498047\n",
      "Training loss: 0.1322583705186844\n",
      "Training loss: 0.12824594974517822\n",
      "Training loss: 0.12948094308376312\n",
      "Training loss: 0.1322159320116043\n",
      "Training loss: 0.12410853803157806\n",
      "Training loss: 0.1340150237083435\n",
      "Training loss: 0.12722277641296387\n",
      "Training loss: 0.13769176602363586\n",
      "Training loss: 0.14052756130695343\n",
      "Training loss: 0.1441652625799179\n",
      "Training loss: 0.12749645113945007\n",
      "Training loss: 0.1309853047132492\n",
      "Training loss: 0.1287357360124588\n",
      "Training loss: 0.1403302252292633\n",
      "Training loss: 0.1418638527393341\n",
      "Training loss: 0.12669172883033752\n",
      "Training loss: 0.13629601895809174\n",
      "Training loss: 0.13099493086338043\n",
      "Training loss: 0.1406463384628296\n",
      "Training loss: 0.12969361245632172\n",
      "Training loss: 0.12524071335792542\n",
      "Training loss: 0.13568015396595\n",
      "Training loss: 0.12761282920837402\n",
      "Training loss: 0.13525564968585968\n",
      "Training loss: 0.13139788806438446\n",
      "Training loss: 0.1359291523694992\n",
      "Training loss: 0.12897077202796936\n",
      "Training loss: 0.13418112695217133\n",
      "Training loss: 0.13154320418834686\n",
      "Training loss: 0.13145378232002258\n",
      "Training loss: 0.12322840094566345\n",
      "Training loss: 0.126747727394104\n",
      "Training loss: 0.12482821196317673\n",
      "Training loss: 0.1289401650428772\n",
      "Training loss: 0.12945976853370667\n",
      "Training loss: 0.12423698604106903\n",
      "Training loss: 0.13188914954662323\n",
      "Training loss: 0.12376049160957336\n",
      "Training loss: 0.1273242086172104\n",
      "Training loss: 0.13912706077098846\n",
      "Training loss: 0.1305389255285263\n",
      "Training loss: 0.1219739019870758\n",
      "Training loss: 0.13579556345939636\n",
      "Training loss: 0.12758053839206696\n",
      "Training loss: 0.12144917994737625\n",
      "Training loss: 0.12621787190437317\n",
      "Training loss: 0.12431930005550385\n",
      "Training loss: 0.12383750081062317\n",
      "Training loss: 0.1254260092973709\n",
      "Training loss: 0.12344539910554886\n",
      "Training loss: 0.13261812925338745\n",
      "Training loss: 0.12817126512527466\n",
      "Training loss: 0.12632013857364655\n",
      "Training loss: 0.12648190557956696\n",
      "Training loss: 0.12510167062282562\n",
      "Training loss: 0.1212957352399826\n",
      "Training loss: 0.12466171383857727\n",
      "Training loss: 0.13398058712482452\n",
      "Training loss: 0.13823333382606506\n",
      "Training loss: 0.128540500998497\n",
      "Training loss: 0.12046784907579422\n",
      "Training loss: 0.13260024785995483\n",
      "Training loss: 0.12898695468902588\n",
      "Training loss: 0.13099415600299835\n",
      "Training loss: 0.12851379811763763\n",
      "Training loss: 0.12329208850860596\n",
      "Training loss: 0.12974005937576294\n",
      "Training loss: 0.12250536680221558\n",
      "Training loss: 0.12225805222988129\n",
      "Training loss: 0.12574824690818787\n",
      "Training loss: 0.12277578562498093\n",
      "Training loss: 0.1269412636756897\n",
      "Training loss: 0.1328282356262207\n",
      "Training loss: 0.13622771203517914\n",
      "Training loss: 0.1422649770975113\n",
      "Training loss: 0.12834255397319794\n",
      "Training loss: 0.13733404874801636\n",
      "Training loss: 0.13702981173992157\n",
      "Training loss: 0.13549336791038513\n",
      "Training loss: 0.1376493126153946\n",
      "Training loss: 0.1253310889005661\n",
      "Training loss: 0.12418147176504135\n",
      "Training loss: 0.12757249176502228\n",
      "Training loss: 0.1358708292245865\n",
      "Training loss: 0.12981656193733215\n",
      "Training loss: 0.12765280902385712\n",
      "Training loss: 0.12481185048818588\n",
      "Training loss: 0.12464411556720734\n",
      "Training loss: 0.127983957529068\n",
      "Training loss: 0.12680980563163757\n",
      "Training loss: 0.12779642641544342\n",
      "Training loss: 0.1320854276418686\n",
      "Training loss: 0.12111306190490723\n",
      "Training loss: 0.13182079792022705\n",
      "Training loss: 0.13380193710327148\n",
      "Training loss: 0.1327056884765625\n",
      "Training loss: 0.12493301182985306\n",
      "Training loss: 0.13936983048915863\n",
      "Training loss: 0.1260179877281189\n",
      "Training loss: 0.13329732418060303\n",
      "Training loss: 0.144559845328331\n",
      "Training loss: 0.1328640580177307\n",
      "Training loss: 0.13029474020004272\n",
      "Training loss: 0.12465193122625351\n",
      "Training loss: 0.1308135688304901\n",
      "Training loss: 0.1360280066728592\n",
      "Training loss: 0.12394596636295319\n",
      "Training loss: 0.12409847229719162\n",
      "Training loss: 0.1282331794500351\n",
      "Training loss: 0.14356794953346252\n",
      "Training loss: 0.1317272186279297\n",
      "Training loss: 0.1306706815958023\n",
      "Training loss: 0.13052992522716522\n",
      "Training loss: 0.1505628377199173\n",
      "Training loss: 0.12945470213890076\n",
      "Training loss: 0.12867595255374908\n",
      "Training loss: 0.13114479184150696\n",
      "Training loss: 0.12229806929826736\n",
      "Training loss: 0.1294160932302475\n",
      "Training loss: 0.12473157793283463\n",
      "Training loss: 0.12970302999019623\n",
      "Training loss: 0.12989170849323273\n",
      "Training loss: 0.12465105205774307\n",
      "Training loss: 0.13252729177474976\n",
      "Training loss: 0.13625088334083557\n",
      "Training loss: 0.1244245171546936\n",
      "Training loss: 0.12870241701602936\n",
      "Training loss: 0.13035057485103607\n",
      "Training loss: 0.12611670792102814\n",
      "Training loss: 0.12960760295391083\n",
      "Training loss: 0.12813453376293182\n",
      "Training loss: 0.14631083607673645\n",
      "Training loss: 0.12244654446840286\n",
      "Training loss: 0.12471389025449753\n",
      "Training loss: 0.1241217851638794\n",
      "Training loss: 0.1262597292661667\n",
      "Training loss: 0.1286548376083374\n",
      "Training loss: 0.1254120022058487\n",
      "Training loss: 0.12574459612369537\n",
      "Training loss: 0.12297677248716354\n",
      "Training loss: 0.13350534439086914\n",
      "Training loss: 0.12657848000526428\n",
      "Training loss: 0.13076351583003998\n",
      "Training loss: 0.12232588976621628\n",
      "Training loss: 0.11795409023761749\n",
      "Training loss: 0.13994711637496948\n",
      "Training loss: 0.12601397931575775\n",
      "Training loss: 0.12077344954013824\n",
      "Training loss: 0.1334751397371292\n",
      "Training loss: 0.12917427718639374\n",
      "Training loss: 0.12547385692596436\n",
      "Training loss: 0.12023912370204926\n",
      "Training loss: 0.1226881667971611\n",
      "Training loss: 0.12851153314113617\n",
      "Training loss: 0.11860105395317078\n",
      "Training loss: 0.12620986998081207\n",
      "Training loss: 0.139929860830307\n",
      "Training loss: 0.13055899739265442\n",
      "Training loss: 0.13771027326583862\n",
      "Training loss: 0.12417437136173248\n",
      "Training loss: 0.12198244035243988\n",
      "Training loss: 0.11976224184036255\n",
      "Training loss: 0.12315592169761658\n",
      "Training loss: 0.13564875721931458\n",
      "Training loss: 0.12801842391490936\n",
      "Training loss: 0.12744179368019104\n",
      "Training loss: 0.12985287606716156\n",
      "Training loss: 0.1295531988143921\n",
      "Training loss: 0.12813901901245117\n",
      "Training loss: 0.12624491751194\n",
      "Training loss: 0.12464886158704758\n",
      "Training loss: 0.11855652183294296\n",
      "Training loss: 0.1174175888299942\n",
      "Training loss: 0.12726618349552155\n",
      "Training loss: 0.12549619376659393\n",
      "Training loss: 0.12658415734767914\n",
      "Training loss: 0.11514683067798615\n",
      "Training loss: 0.12743230164051056\n",
      "Training loss: 0.12668494880199432\n",
      "Training loss: 0.12362882494926453\n",
      "Training loss: 0.12232242524623871\n",
      "Training loss: 0.12403728067874908\n",
      "Training loss: 0.12554407119750977\n",
      "Training loss: 0.11978168040513992\n",
      "Training loss: 0.1220986396074295\n",
      "Training loss: 0.12529443204402924\n",
      "Training loss: 0.12255280464887619\n",
      "Training loss: 0.12163840234279633\n",
      "Training loss: 0.1423317790031433\n",
      "Training loss: 0.12684482336044312\n",
      "Training loss: 0.14607015252113342\n",
      "Training loss: 0.1273842602968216\n",
      "Training loss: 0.12723349034786224\n",
      "Training loss: 0.12601912021636963\n",
      "Training loss: 0.1305098682641983\n",
      "Training loss: 0.12412460148334503\n",
      "Training loss: 0.1220116838812828\n",
      "Training loss: 0.12726502120494843\n",
      "Training loss: 0.1512029767036438\n",
      "Training loss: 0.12514643371105194\n",
      "Training loss: 0.12469056248664856\n",
      "Training loss: 0.13105957210063934\n",
      "Training loss: 0.12774460017681122\n",
      "Training loss: 0.13426697254180908\n",
      "Training loss: 0.12758557498455048\n",
      "Training loss: 0.13665229082107544\n",
      "Training loss: 0.17757511138916016\n",
      "Training loss: 0.13191434741020203\n",
      "Training loss: 0.13508811593055725\n",
      "Training loss: 0.1337895542383194\n",
      "Training loss: 0.1339111179113388\n",
      "Training loss: 0.12849825620651245\n",
      "Training loss: 0.15213105082511902\n",
      "Training loss: 0.1336691528558731\n",
      "Training loss: 0.130751371383667\n",
      "Training loss: 0.1328459233045578\n",
      "Training loss: 0.12996578216552734\n",
      "Training loss: 0.12754781544208527\n",
      "Training loss: 0.12670356035232544\n",
      "Training loss: 0.13151220977306366\n",
      "Training loss: 0.13233479857444763\n",
      "Training loss: 0.1262146383523941\n",
      "Training loss: 0.13018031418323517\n",
      "Training loss: 0.13096842169761658\n",
      "Training loss: 0.13195061683654785\n",
      "Training loss: 0.12597593665122986\n",
      "Training loss: 0.13007721304893494\n",
      "Training loss: 0.13139091432094574\n",
      "Training loss: 0.12267737835645676\n",
      "Training loss: 0.12361989915370941\n",
      "Training loss: 0.13090357184410095\n",
      "Training loss: 0.1297535002231598\n",
      "Training loss: 0.12903675436973572\n",
      "Training loss: 0.1370801031589508\n",
      "Training loss: 0.13768500089645386\n",
      "Training loss: 0.1298416256904602\n",
      "Training loss: 0.13400863111019135\n",
      "Training loss: 0.1298636496067047\n",
      "Training loss: 0.1354759931564331\n",
      "Training loss: 0.14760948717594147\n",
      "Training loss: 0.13315314054489136\n",
      "Training loss: 0.1266825795173645\n",
      "Training loss: 0.13494513928890228\n",
      "Training loss: 0.12714987993240356\n",
      "Training loss: 0.13488775491714478\n",
      "Training loss: 0.1299814134836197\n",
      "Training loss: 0.12373130023479462\n",
      "Training loss: 0.12468580156564713\n",
      "Training loss: 0.12524840235710144\n",
      "Training loss: 0.12463869154453278\n",
      "Training loss: 0.12830998003482819\n",
      "Training loss: 0.1234780102968216\n",
      "Training loss: 0.13121014833450317\n",
      "Training loss: 0.12820878624916077\n",
      "Training loss: 0.13901092112064362\n",
      "Training loss: 0.1352720558643341\n",
      "Training loss: 0.12992171943187714\n",
      "Training loss: 0.1314367651939392\n",
      "Training loss: 0.12213342636823654\n",
      "Training loss: 0.12284429371356964\n",
      "Training loss: 0.12885235249996185\n",
      "Training loss: 0.1283196061849594\n",
      "Training loss: 0.1267194151878357\n",
      "Training loss: 0.12645992636680603\n",
      "Training loss: 0.12589755654335022\n",
      "Training loss: 0.12929178774356842\n",
      "Training loss: 0.12339600920677185\n",
      "Training loss: 0.12397923320531845\n",
      "Training loss: 0.1196959912776947\n",
      "Training loss: 0.12400863319635391\n",
      "Training loss: 0.12970876693725586\n",
      "Training loss: 0.12084803730249405\n",
      "Training loss: 0.12869538366794586\n",
      "Training loss: 0.12130996584892273\n",
      "Training loss: 0.12223994731903076\n",
      "Training loss: 0.12473580241203308\n",
      "Training loss: 0.12391655147075653\n",
      "Training loss: 0.12326104938983917\n",
      "Training loss: 0.11535289138555527\n",
      "Training loss: 0.12718504667282104\n",
      "Training loss: 0.12539736926555634\n",
      "Training loss: 0.11859536916017532\n",
      "Training loss: 0.12373583763837814\n",
      "Training loss: 0.11935319006443024\n",
      "Training loss: 0.11981320381164551\n",
      "Training loss: 0.12284330278635025\n",
      "Training loss: 0.127278670668602\n",
      "Training loss: 0.12177566438913345\n",
      "Training loss: 0.12939660251140594\n",
      "Training loss: 0.11498215794563293\n",
      "Training loss: 0.12713037431240082\n",
      "Training loss: 0.12010148167610168\n",
      "Training loss: 0.12174037843942642\n",
      "Training loss: 0.1290115863084793\n",
      "Training loss: 0.13045676052570343\n",
      "Training loss: 0.12156148254871368\n",
      "Training loss: 0.12646253407001495\n",
      "Training loss: 0.12451019138097763\n",
      "Training loss: 0.13372015953063965\n",
      "Training loss: 0.12357552349567413\n",
      "Training loss: 0.12852327525615692\n",
      "Training loss: 0.12131726741790771\n",
      "Training loss: 0.1262872964143753\n",
      "Training loss: 0.12646667659282684\n",
      "Training loss: 0.12712298333644867\n",
      "Training loss: 0.12864693999290466\n",
      "Training loss: 0.12879794836044312\n",
      "Training loss: 0.12899789214134216\n",
      "Training loss: 0.1333044171333313\n",
      "Training loss: 0.13263344764709473\n",
      "Training loss: 0.12644898891448975\n",
      "Training loss: 0.1148427352309227\n",
      "Training loss: 0.12086660414934158\n",
      "Training loss: 0.13550913333892822\n",
      "Training loss: 0.11521556973457336\n",
      "Training loss: 0.1309053599834442\n",
      "Training loss: 0.125202938914299\n",
      "Training loss: 0.122235968708992\n",
      "Training loss: 0.12673959136009216\n",
      "Training loss: 0.12215389311313629\n",
      "Training loss: 0.12680071592330933\n",
      "Training loss: 0.1229015365242958\n",
      "Training loss: 0.12079177796840668\n",
      "Training loss: 0.12547168135643005\n",
      "Training loss: 0.12855474650859833\n",
      "Training loss: 0.13055719435214996\n",
      "Training loss: 0.12393561750650406\n",
      "Training loss: 0.1264255791902542\n",
      "Training loss: 0.12412257492542267\n",
      "Training loss: 0.11897594481706619\n",
      "Training loss: 0.12732970714569092\n",
      "Training loss: 0.14621330797672272\n",
      "Training loss: 0.12603245675563812\n",
      "Training loss: 0.12491638958454132\n",
      "Training loss: 0.12440560013055801\n",
      "Training loss: 0.12323158979415894\n",
      "Training loss: 0.12194578349590302\n",
      "Training loss: 0.12314897030591965\n",
      "Training loss: 0.12165208905935287\n",
      "Training loss: 0.12148257344961166\n",
      "Training loss: 0.13450650870800018\n",
      "Training loss: 0.12473054975271225\n",
      "Training loss: 0.13794437050819397\n",
      "Training loss: 0.13437947630882263\n",
      "Training loss: 0.12355735152959824\n",
      "Training loss: 0.13237901031970978\n",
      "Training loss: 0.12531349062919617\n",
      "Training loss: 0.1302105188369751\n",
      "Training loss: 0.12581372261047363\n",
      "Training loss: 0.12045000493526459\n",
      "Training loss: 0.12415630370378494\n",
      "Training loss: 0.12745846807956696\n",
      "Training loss: 0.12608923017978668\n",
      "Training loss: 0.12054101377725601\n",
      "Training loss: 0.12587149441242218\n",
      "Training loss: 0.13374072313308716\n",
      "Training loss: 0.12100524455308914\n",
      "Training loss: 0.12747187912464142\n",
      "Training loss: 0.11944467574357986\n",
      "Training loss: 0.12595976889133453\n",
      "Training loss: 0.12493659555912018\n",
      "Training loss: 0.12884992361068726\n",
      "Training loss: 0.12538886070251465\n",
      "Training loss: 0.12592577934265137\n",
      "Training loss: 0.1227152943611145\n",
      "Training loss: 0.1272045224905014\n",
      "Training loss: 0.12263787537813187\n",
      "Training loss: 0.12124894559383392\n",
      "Training loss: 0.13139881193637848\n",
      "Training loss: 0.12004884332418442\n",
      "Training loss: 0.117603600025177\n",
      "Training loss: 0.1301306188106537\n",
      "Training loss: 0.12825676798820496\n",
      "Training loss: 0.1251213252544403\n",
      "Training loss: 0.12543454766273499\n",
      "Training loss: 0.13150791823863983\n",
      "Training loss: 0.1265992373228073\n",
      "Training loss: 0.1364731639623642\n",
      "Training loss: 0.12671560049057007\n",
      "Training loss: 0.13153457641601562\n",
      "Training loss: 0.12789733707904816\n",
      "Training loss: 0.12984029948711395\n",
      "Training loss: 0.1276363581418991\n",
      "Training loss: 0.13138143718242645\n",
      "Training loss: 0.12722192704677582\n",
      "Training loss: 0.12002421915531158\n",
      "Training loss: 0.125046968460083\n",
      "Training loss: 0.13712774217128754\n",
      "Training loss: 0.11911708861589432\n",
      "Training loss: 0.12610283493995667\n",
      "Training loss: 0.12107137590646744\n",
      "Training loss: 0.13127680122852325\n",
      "Training loss: 0.1274074912071228\n",
      "Training loss: 0.11958438158035278\n",
      "Training loss: 0.12069155275821686\n",
      "Training loss: 0.12472381442785263\n",
      "Training loss: 0.1629905104637146\n",
      "Training loss: 0.134743794798851\n",
      "Training loss: 0.12742123007774353\n",
      "Training loss: 0.12305144220590591\n",
      "Training loss: 0.12978766858577728\n",
      "Training loss: 0.1253286749124527\n",
      "Training loss: 0.12828347086906433\n",
      "Training loss: 0.13305692374706268\n",
      "Training loss: 0.12725085020065308\n",
      "Training loss: 0.125821053981781\n",
      "Training loss: 0.1284748911857605\n",
      "Training loss: 0.12006442993879318\n",
      "Training loss: 0.12732857465744019\n",
      "Training loss: 0.12281932681798935\n",
      "Training loss: 0.1144605278968811\n",
      "Training loss: 0.12630948424339294\n",
      "Training loss: 0.11661851406097412\n",
      "Training loss: 0.12465528398752213\n",
      "Training loss: 0.12571187317371368\n",
      "Training loss: 0.12027276307344437\n",
      "Training loss: 0.12445725500583649\n",
      "Training loss: 0.14467816054821014\n",
      "Training loss: 0.12445291876792908\n",
      "Training loss: 0.13258421421051025\n",
      "Training loss: 0.13750475645065308\n",
      "Training loss: 0.11848483979701996\n",
      "Training loss: 0.11693432182073593\n",
      "Training loss: 0.12768080830574036\n",
      "Training loss: 0.13002125918865204\n",
      "Training loss: 0.13077102601528168\n",
      "Training loss: 0.11705000698566437\n",
      "Training loss: 0.12909752130508423\n",
      "Training loss: 0.12065289169549942\n",
      "Training loss: 0.12648743391036987\n",
      "Training loss: 0.12114250659942627\n",
      "Training loss: 0.12201372534036636\n",
      "Training loss: 0.1325603574514389\n",
      "Training loss: 0.12992540001869202\n",
      "Training loss: 0.13025444746017456\n",
      "Training loss: 0.1288376748561859\n",
      "Training loss: 0.13052073121070862\n",
      "Training loss: 0.1273868978023529\n",
      "Training loss: 0.12658292055130005\n",
      "Training loss: 0.12095052748918533\n",
      "Training loss: 0.11957232654094696\n",
      "Training loss: 0.12561386823654175\n",
      "Training loss: 0.12580464780330658\n",
      "Training loss: 0.12646716833114624\n",
      "Training loss: 0.12422247231006622\n",
      "Training loss: 0.12612377107143402\n",
      "Training loss: 0.12667584419250488\n",
      "Training loss: 0.1233842521905899\n",
      "Training loss: 0.12469060719013214\n",
      "Training loss: 0.12608857452869415\n",
      "Training loss: 0.12462704628705978\n",
      "Training loss: 0.1278323084115982\n",
      "Training loss: 0.1199045181274414\n",
      "Training loss: 0.12334346026182175\n",
      "Training loss: 0.1232926994562149\n",
      "Training loss: 0.12423797696828842\n",
      "Training loss: 0.12480684369802475\n",
      "Training loss: 0.14015473425388336\n",
      "Training loss: 0.11674121022224426\n",
      "Training loss: 0.11787768453359604\n",
      "Training loss: 0.12613585591316223\n",
      "Training loss: 0.11692643910646439\n",
      "Training loss: 0.11920054256916046\n",
      "Training loss: 0.1155758798122406\n",
      "Training loss: 0.12577477097511292\n",
      "Training loss: 0.12374197691679001\n",
      "Training loss: 0.12201064079999924\n",
      "Training loss: 0.11909610033035278\n",
      "Training loss: 0.1341809779405594\n",
      "Training loss: 0.11854685842990875\n",
      "Training loss: 0.11913822591304779\n",
      "Training loss: 0.12220966815948486\n",
      "Training loss: 0.128290593624115\n",
      "Training loss: 0.13156446814537048\n",
      "Training loss: 0.1185787245631218\n",
      "Training loss: 0.12793631851673126\n",
      "Training loss: 0.11665400862693787\n",
      "Training loss: 0.12414766848087311\n",
      "Training loss: 0.12845081090927124\n",
      "Training loss: 0.11998657882213593\n",
      "Training loss: 0.12354308366775513\n",
      "Training loss: 0.12516389787197113\n",
      "Training loss: 0.11804666370153427\n",
      "Training loss: 0.13345935940742493\n",
      "Training loss: 0.12769563496112823\n",
      "Training loss: 0.12592019140720367\n",
      "Training loss: 0.12257858365774155\n",
      "Training loss: 0.12848907709121704\n",
      "Training loss: 0.12064547836780548\n",
      "Training loss: 0.12024224549531937\n",
      "Training loss: 0.1290360391139984\n",
      "Training loss: 0.11689739674329758\n",
      "Training loss: 0.12266335636377335\n",
      "Training loss: 0.12589876353740692\n",
      "Training loss: 0.12580518424510956\n",
      "Training loss: 0.12649548053741455\n",
      "Training loss: 0.12923865020275116\n",
      "Training loss: 0.12430074065923691\n",
      "Training loss: 0.12666866183280945\n",
      "Training loss: 0.12644799053668976\n",
      "Training loss: 0.11922454088926315\n",
      "Training loss: 0.12413232773542404\n",
      "Training loss: 0.12241768836975098\n",
      "Training loss: 0.11785741150379181\n",
      "Training loss: 0.11961326748132706\n",
      "Training loss: 0.11589709669351578\n",
      "Training loss: 0.12432121485471725\n",
      "Training loss: 0.12859125435352325\n",
      "Training loss: 0.12246090918779373\n",
      "Training loss: 0.1250651776790619\n",
      "Training loss: 0.12483537942171097\n",
      "Training loss: 0.12133154273033142\n",
      "Training loss: 0.12261731177568436\n",
      "Training loss: 0.12054121494293213\n",
      "Training loss: 0.1159830242395401\n",
      "Training loss: 0.1325138360261917\n",
      "Training loss: 0.11380984634160995\n",
      "Training loss: 0.1216239333152771\n",
      "Training loss: 0.12066443264484406\n",
      "Training loss: 0.11463546007871628\n",
      "Training loss: 0.12399227172136307\n",
      "Training loss: 0.11835585534572601\n",
      "Training loss: 0.12128730863332748\n",
      "Training loss: 0.1153179481625557\n",
      "Training loss: 0.11509488523006439\n",
      "Training loss: 0.11697492003440857\n",
      "Training loss: 0.120655857026577\n",
      "Training loss: 0.11700613796710968\n",
      "Training loss: 0.1250704526901245\n",
      "Training loss: 0.11902187019586563\n",
      "Training loss: 0.12233510613441467\n",
      "Training loss: 0.11815155297517776\n",
      "Training loss: 0.12055955082178116\n",
      "Training loss: 0.11829768866300583\n",
      "Training loss: 0.11958424746990204\n",
      "Training loss: 0.11559327691793442\n",
      "Training loss: 0.11459305882453918\n",
      "Training loss: 0.11323048174381256\n",
      "Training loss: 0.11513271182775497\n",
      "Training loss: 0.12222041189670563\n",
      "Training loss: 0.12233710289001465\n",
      "Training loss: 0.11496403068304062\n",
      "Training loss: 0.1275491565465927\n",
      "Training loss: 0.12133899331092834\n",
      "Training loss: 0.12448888272047043\n",
      "Training loss: 0.12496380507946014\n",
      "Training loss: 0.12299084663391113\n",
      "Training loss: 0.11991202086210251\n",
      "Training loss: 0.12009517848491669\n",
      "Training loss: 0.11738664656877518\n",
      "Training loss: 0.124981589615345\n",
      "Training loss: 0.12598538398742676\n",
      "Training loss: 0.1377849280834198\n",
      "Training loss: 0.12728692591190338\n",
      "Training loss: 0.11861347407102585\n",
      "Training loss: 0.1159493550658226\n",
      "Training loss: 0.12747633457183838\n",
      "Training loss: 0.11436638981103897\n",
      "Training loss: 0.1398872435092926\n",
      "Training loss: 0.12088377773761749\n",
      "Training loss: 0.11798346042633057\n",
      "Training loss: 0.1200399100780487\n",
      "Training loss: 0.12151483446359634\n",
      "Training loss: 0.11932141333818436\n",
      "Training loss: 0.11938831955194473\n",
      "Training loss: 0.11934421211481094\n",
      "Training loss: 0.1262560337781906\n",
      "Training loss: 0.12704575061798096\n",
      "Training loss: 0.12993845343589783\n",
      "Training loss: 0.11776168644428253\n",
      "Training loss: 0.11554307490587234\n",
      "Training loss: 0.12197868525981903\n",
      "Training loss: 0.11876664310693741\n",
      "Training loss: 0.12707673013210297\n",
      "Training loss: 0.13149425387382507\n",
      "Training loss: 0.12050174921751022\n",
      "Training loss: 0.12157511711120605\n",
      "Training loss: 0.1264122724533081\n",
      "Training loss: 0.12298648804426193\n",
      "Training loss: 0.12232133746147156\n",
      "Training loss: 0.12102682143449783\n",
      "Training loss: 0.11536954343318939\n",
      "Training loss: 0.11469295620918274\n",
      "Training loss: 0.1212642714381218\n",
      "Training loss: 0.12521977722644806\n",
      "Training loss: 0.11907056719064713\n",
      "Training loss: 0.11931262910366058\n",
      "Training loss: 0.12263710051774979\n",
      "Training loss: 0.12605607509613037\n",
      "Training loss: 0.12043065577745438\n",
      "Training loss: 0.1186070665717125\n",
      "Training loss: 0.11624753475189209\n",
      "Training loss: 0.1191931739449501\n",
      "Training loss: 0.1212490126490593\n",
      "Training loss: 0.11299965530633926\n",
      "Training loss: 0.12023032456636429\n",
      "Training loss: 0.13196603953838348\n",
      "Training loss: 0.12089263647794724\n",
      "Training loss: 0.13109765946865082\n",
      "Training loss: 0.11548164486885071\n",
      "Training loss: 0.13711346685886383\n",
      "Training loss: 0.12112031131982803\n",
      "Training loss: 0.12836642563343048\n",
      "Training loss: 0.12540386617183685\n",
      "Training loss: 0.12037698179483414\n",
      "Training loss: 0.12176982313394547\n",
      "Training loss: 0.1263928860425949\n",
      "Training loss: 0.11161819845438004\n",
      "Training loss: 0.1174325942993164\n",
      "Training loss: 0.12526999413967133\n",
      "Training loss: 0.12170764058828354\n",
      "Training loss: 0.11715289205312729\n",
      "Training loss: 0.11407368630170822\n",
      "Training loss: 0.11216679960489273\n",
      "Training loss: 0.11805056035518646\n",
      "Training loss: 0.12650199234485626\n",
      "Training loss: 0.1197773665189743\n",
      "Training loss: 0.1148177906870842\n",
      "Training loss: 0.11874748021364212\n",
      "Training loss: 0.11967755854129791\n",
      "Training loss: 0.11985475569963455\n",
      "Training loss: 0.11571067571640015\n",
      "Training loss: 0.11596406996250153\n",
      "Training loss: 0.11704801768064499\n",
      "Training loss: 0.11483100801706314\n",
      "Training loss: 0.11691470444202423\n",
      "Training loss: 0.11733002960681915\n",
      "Training loss: 0.13110505044460297\n",
      "Training loss: 0.12237370014190674\n",
      "Training loss: 0.11912582069635391\n",
      "Training loss: 0.115972138941288\n",
      "Training loss: 0.12779469788074493\n",
      "Training loss: 0.11578924208879471\n",
      "Training loss: 0.1188177838921547\n",
      "Training loss: 0.12279750406742096\n",
      "Training loss: 0.11227846890687943\n",
      "Training loss: 0.12727724015712738\n",
      "Training loss: 0.11957445740699768\n",
      "Training loss: 0.37477046251296997\n",
      "Training loss: 0.12252829223871231\n",
      "Training loss: 0.12104920297861099\n",
      "Training loss: 0.12582939863204956\n",
      "Training loss: 0.12507323920726776\n",
      "Training loss: 0.12562444806098938\n",
      "Training loss: 0.12657681107521057\n",
      "Training loss: 0.11758014559745789\n",
      "Training loss: 0.12673337757587433\n",
      "Training loss: 0.1230427622795105\n",
      "Training loss: 0.13777922093868256\n",
      "Training loss: 0.1185954138636589\n",
      "Training loss: 0.11613352596759796\n",
      "Training loss: 0.1159011647105217\n",
      "Training loss: 0.12309184670448303\n",
      "Training loss: 0.12210056185722351\n",
      "Training loss: 0.12695510685443878\n",
      "Training loss: 0.12619717419147491\n",
      "Training loss: 0.13212482631206512\n",
      "Training loss: 0.12211576104164124\n",
      "Training loss: 0.12573598325252533\n",
      "Training loss: 0.12076721340417862\n",
      "Training loss: 0.12695170938968658\n",
      "Training loss: 0.12445530295372009\n",
      "Training loss: 0.11474572122097015\n",
      "Training loss: 0.11835405975580215\n",
      "Training loss: 0.12035709619522095\n",
      "Training loss: 0.1239953264594078\n",
      "Training loss: 0.11904069036245346\n",
      "Training loss: 0.12424994260072708\n",
      "Training loss: 0.11642907559871674\n",
      "Training loss: 0.12042068690061569\n",
      "Training loss: 0.11671332269906998\n",
      "Training loss: 0.13012316823005676\n",
      "Training loss: 0.12848098576068878\n",
      "Training loss: 0.11881155520677567\n",
      "Training loss: 0.12094569951295853\n",
      "Training loss: 0.11775001883506775\n",
      "Training loss: 0.12590350210666656\n",
      "Training loss: 0.12324018031358719\n",
      "Training loss: 0.13548988103866577\n",
      "Training loss: 0.12137625366449356\n",
      "Training loss: 0.12222525477409363\n",
      "Training loss: 0.11443830281496048\n",
      "Training loss: 0.11644051969051361\n",
      "Training loss: 0.11632406711578369\n",
      "Training loss: 0.12021811306476593\n",
      "Training loss: 0.11885558813810349\n",
      "Training loss: 0.1252182275056839\n",
      "Training loss: 0.12188086658716202\n",
      "Training loss: 0.12183462083339691\n",
      "Training loss: 0.13300853967666626\n",
      "Training loss: 0.1147274374961853\n",
      "Training loss: 0.12237237393856049\n",
      "Training loss: 0.13195160031318665\n",
      "Training loss: 0.1182243824005127\n",
      "Training loss: 0.12232376635074615\n",
      "Training loss: 0.12382082641124725\n",
      "Training loss: 0.12791062891483307\n",
      "Training loss: 0.12111960351467133\n",
      "Training loss: 0.11897213011980057\n",
      "Training loss: 0.10996951162815094\n",
      "Training loss: 0.12017624080181122\n",
      "Training loss: 0.1260797083377838\n",
      "Training loss: 0.1306280493736267\n",
      "Training loss: 0.11991418898105621\n",
      "Training loss: 0.11773397028446198\n",
      "Training loss: 0.1308237463235855\n",
      "Training loss: 0.11768535524606705\n",
      "Training loss: 0.11542820930480957\n",
      "Training loss: 0.13098682463169098\n",
      "Training loss: 0.1193562000989914\n",
      "Training loss: 0.1198417916893959\n",
      "Training loss: 0.11445243656635284\n",
      "Training loss: 0.11439039558172226\n",
      "Training loss: 0.11927428096532822\n",
      "Training loss: 0.1220831423997879\n",
      "Training loss: 0.11561134457588196\n",
      "Training loss: 0.1206013634800911\n",
      "Training loss: 0.11744385957717896\n",
      "Training loss: 0.11485922336578369\n",
      "Training loss: 0.11378761380910873\n",
      "Training loss: 0.11849711090326309\n",
      "Training loss: 0.12156936526298523\n",
      "Training loss: 0.11643272638320923\n",
      "Training loss: 0.11256934702396393\n",
      "Training loss: 0.12316489964723587\n",
      "Training loss: 0.11052311956882477\n",
      "Training loss: 0.1191079244017601\n",
      "Training loss: 0.1161099374294281\n",
      "Training loss: 0.11514807492494583\n",
      "Training loss: 0.1124645248055458\n",
      "Training loss: 0.11689533293247223\n",
      "Training loss: 0.11166282743215561\n",
      "Training loss: 0.11857065558433533\n",
      "Training loss: 0.11389029026031494\n",
      "Training loss: 0.11439114063978195\n",
      "Training loss: 0.12226926535367966\n",
      "Training loss: 0.1138630360364914\n",
      "Training loss: 0.11496985703706741\n",
      "Training loss: 0.11073452979326248\n",
      "Training loss: 0.11939479410648346\n",
      "Training loss: 0.11489886790513992\n",
      "Training loss: 0.10585840791463852\n",
      "Training loss: 0.11364918947219849\n",
      "Training loss: 0.11789478361606598\n",
      "Training loss: 0.11948935687541962\n",
      "Training loss: 0.120049849152565\n",
      "Training loss: 0.11437041312456131\n",
      "Training loss: 0.1179284080862999\n",
      "Training loss: 0.11046109348535538\n",
      "Training loss: 0.11742686480283737\n",
      "Training loss: 0.11722370237112045\n",
      "Training loss: 0.11622179299592972\n",
      "Training loss: 0.12495990842580795\n",
      "Training loss: 0.1194051131606102\n",
      "Training loss: 0.11379772424697876\n",
      "Training loss: 0.11270227283239365\n",
      "Training loss: 0.12946298718452454\n",
      "Training loss: 0.11236961930990219\n",
      "Training loss: 0.11722534894943237\n",
      "Training loss: 0.11409751325845718\n",
      "Training loss: 0.12069107592105865\n",
      "Training loss: 0.12019966542720795\n",
      "Training loss: 0.12640784680843353\n",
      "Training loss: 0.11679597944021225\n",
      "Training loss: 0.1160237193107605\n",
      "Training loss: 0.1172088161110878\n",
      "Training loss: 0.10952901095151901\n",
      "Training loss: 0.1117665246129036\n",
      "Training loss: 0.12142559885978699\n",
      "Training loss: 0.11525612324476242\n",
      "Training loss: 0.11278237402439117\n",
      "Training loss: 0.11403939127922058\n",
      "Training loss: 0.11363472789525986\n",
      "Training loss: 0.11272972822189331\n",
      "Training loss: 0.1178293228149414\n",
      "Training loss: 0.11510185152292252\n",
      "Training loss: 0.10928798466920853\n",
      "Training loss: 0.11729210615158081\n",
      "Training loss: 0.11131377518177032\n",
      "Training loss: 0.1098610982298851\n",
      "Training loss: 0.11317925900220871\n",
      "Training loss: 0.11923260241746902\n",
      "Training loss: 0.11345978826284409\n",
      "Training loss: 0.11730305850505829\n",
      "Training loss: 0.11891921609640121\n",
      "Training loss: 0.12441656738519669\n",
      "Training loss: 0.11613768339157104\n",
      "Training loss: 0.1223379597067833\n",
      "Training loss: 0.11781273782253265\n",
      "Training loss: 0.11260690540075302\n",
      "Training loss: 0.1112905889749527\n",
      "Training loss: 0.12200234830379486\n",
      "Training loss: 0.1245599314570427\n",
      "Training loss: 0.11698173731565475\n",
      "Training loss: 0.11879511922597885\n",
      "Training loss: 0.11654279381036758\n",
      "Training loss: 0.11208371073007584\n",
      "Training loss: 0.11486486345529556\n",
      "Training loss: 0.11973495036363602\n",
      "Training loss: 0.12080158293247223\n",
      "Training loss: 0.11778043210506439\n",
      "Training loss: 0.11375141143798828\n",
      "Training loss: 0.12026924639940262\n",
      "Training loss: 0.12399563193321228\n",
      "Training loss: 0.11800169199705124\n",
      "Training loss: 0.11815463751554489\n",
      "Training loss: 0.12052955478429794\n",
      "Training loss: 0.11771231889724731\n",
      "Training loss: 0.11663372069597244\n",
      "Training loss: 0.12972047924995422\n",
      "Training loss: 0.11549073457717896\n",
      "Training loss: 0.11256138980388641\n",
      "Training loss: 0.12307488173246384\n",
      "Training loss: 0.11644097417593002\n",
      "Training loss: 0.12843166291713715\n",
      "Training loss: 0.12070384621620178\n",
      "Training loss: 0.12001056969165802\n",
      "Training loss: 0.1138177141547203\n",
      "Training loss: 0.11239226162433624\n",
      "Training loss: 0.12212569266557693\n",
      "Training loss: 0.12148725986480713\n",
      "Training loss: 0.11466041207313538\n",
      "Training loss: 0.11683251708745956\n",
      "Training loss: 0.11744407564401627\n",
      "Training loss: 0.11300957202911377\n",
      "Training loss: 0.12550021708011627\n",
      "Training loss: 0.12343914061784744\n",
      "Training loss: 0.12376555800437927\n",
      "Training loss: 0.12085998803377151\n",
      "Training loss: 0.11742699146270752\n",
      "Training loss: 0.11771467328071594\n",
      "Training loss: 0.14703191816806793\n",
      "Training loss: 0.12016158550977707\n",
      "Training loss: 0.12324822694063187\n",
      "Training loss: 0.1205030009150505\n",
      "Training loss: 0.12128102034330368\n",
      "Training loss: 0.1102212443947792\n",
      "Training loss: 0.12152579426765442\n",
      "Training loss: 0.12325127422809601\n",
      "Training loss: 0.11089248210191727\n",
      "Training loss: 0.11639685183763504\n",
      "Training loss: 0.11697667092084885\n",
      "Training loss: 0.11355610191822052\n",
      "Training loss: 0.12142746895551682\n",
      "Training loss: 0.12174107134342194\n",
      "Training loss: 0.12001791596412659\n",
      "Training loss: 0.119790218770504\n",
      "Training loss: 0.121857188642025\n",
      "Training loss: 0.11503742635250092\n",
      "Training loss: 0.11320962011814117\n",
      "Training loss: 0.11715271323919296\n",
      "Training loss: 0.11023175716400146\n",
      "Training loss: 0.1139458566904068\n",
      "Training loss: 0.11537645012140274\n",
      "Training loss: 0.12254273891448975\n",
      "Training loss: 0.115426205098629\n",
      "Training loss: 0.1125127375125885\n",
      "Training loss: 0.1128004714846611\n",
      "Training loss: 0.11894484609365463\n",
      "Training loss: 0.11827578395605087\n",
      "Training loss: 0.11199459433555603\n",
      "Training loss: 0.12483465671539307\n",
      "Training loss: 0.12355022132396698\n",
      "Training loss: 0.11804697662591934\n",
      "Training loss: 0.11537586897611618\n",
      "Training loss: 0.11534470319747925\n",
      "Training loss: 0.1134132519364357\n",
      "Training loss: 0.1180049329996109\n",
      "Training loss: 0.11730274558067322\n",
      "Training loss: 0.11333884298801422\n",
      "Training loss: 0.10743686556816101\n",
      "Training loss: 0.11834313720464706\n",
      "Training loss: 0.11120184510946274\n",
      "Training loss: 0.1154080405831337\n",
      "Training loss: 0.12005089968442917\n",
      "Training loss: 0.11356887221336365\n",
      "Training loss: 0.11934222280979156\n",
      "Training loss: 0.11414436250925064\n",
      "Training loss: 0.11301308125257492\n",
      "Training loss: 0.10703180730342865\n",
      "Training loss: 0.11084504425525665\n",
      "Training loss: 0.11060744524002075\n",
      "Training loss: 0.10891501605510712\n",
      "Training loss: 0.1128622442483902\n",
      "Training loss: 0.11756224185228348\n",
      "Training loss: 0.11574825644493103\n",
      "Training loss: 0.11252015829086304\n",
      "Training loss: 0.12233608961105347\n",
      "Training loss: 0.1119229719042778\n",
      "Training loss: 0.11187140643596649\n",
      "Training loss: 0.12047459930181503\n",
      "Training loss: 0.11229313910007477\n",
      "Training loss: 0.11543445289134979\n",
      "Training loss: 0.11574933677911758\n",
      "Training loss: 0.10728742927312851\n",
      "Training loss: 0.10622560977935791\n",
      "Training loss: 0.11144766211509705\n",
      "Training loss: 0.11309614777565002\n",
      "Training loss: 0.11347928643226624\n",
      "Training loss: 0.1111556813120842\n",
      "Training loss: 0.1081836074590683\n",
      "Training loss: 0.1130213513970375\n",
      "Training loss: 0.10668560862541199\n",
      "Training loss: 0.11198535561561584\n",
      "Training loss: 0.11387158930301666\n",
      "Training loss: 0.11643781512975693\n",
      "Training loss: 0.11024938523769379\n",
      "Training loss: 0.10992194712162018\n",
      "Training loss: 0.11084996163845062\n",
      "Training loss: 0.10627075284719467\n",
      "Training loss: 0.1104709655046463\n",
      "Training loss: 0.11810670793056488\n",
      "Training loss: 0.11844179034233093\n",
      "Training loss: 0.11415039002895355\n",
      "Training loss: 0.1179719865322113\n",
      "Training loss: 0.11522813141345978\n",
      "Training loss: 0.11242888867855072\n",
      "Training loss: 0.11777792125940323\n",
      "Training loss: 0.1101190447807312\n",
      "Training loss: 0.11277680844068527\n",
      "Training loss: 0.11420920491218567\n",
      "Training loss: 0.11057937145233154\n",
      "Training loss: 0.1157032921910286\n",
      "Training loss: 0.12354543060064316\n",
      "Training loss: 0.10734996199607849\n",
      "Training loss: 0.10611685365438461\n",
      "Training loss: 0.11181320250034332\n",
      "Training loss: 0.11822754889726639\n",
      "Training loss: 0.11233674734830856\n",
      "Training loss: 0.12091437727212906\n",
      "Training loss: 0.11276276409626007\n",
      "Training loss: 0.1161690279841423\n",
      "Training loss: 0.11419320851564407\n",
      "Training loss: 0.11473672091960907\n",
      "Training loss: 0.11679016053676605\n",
      "Training loss: 0.12071940302848816\n",
      "Training loss: 0.11100977659225464\n",
      "Training loss: 0.11361270397901535\n",
      "Training loss: 0.11187586933374405\n",
      "Training loss: 0.11990996450185776\n",
      "Training loss: 0.114021435379982\n",
      "Training loss: 0.10501493513584137\n",
      "Training loss: 0.11849913746118546\n",
      "Training loss: 0.11601278930902481\n",
      "Training loss: 0.11986478418111801\n",
      "Training loss: 0.1160675436258316\n",
      "Training loss: 0.11484821885824203\n",
      "Training loss: 0.11076977103948593\n",
      "Training loss: 0.11417867243289948\n",
      "Training loss: 0.11224039644002914\n",
      "Training loss: 0.11052725464105606\n",
      "Training loss: 0.11560915410518646\n",
      "Training loss: 0.11299032717943192\n",
      "Training loss: 0.11378122121095657\n",
      "Training loss: 0.11462618410587311\n",
      "Training loss: 0.11750270426273346\n",
      "Training loss: 0.10808499902486801\n",
      "Training loss: 0.11850210279226303\n",
      "Training loss: 0.11756478250026703\n",
      "Training loss: 0.11233197152614594\n",
      "Training loss: 0.11798587441444397\n",
      "Training loss: 0.11109420657157898\n",
      "Training loss: 0.11323422938585281\n",
      "Training loss: 0.11101166158914566\n",
      "Training loss: 0.11514952033758163\n",
      "Training loss: 0.11461558938026428\n",
      "Training loss: 0.11059670895338058\n",
      "Training loss: 0.11365141719579697\n",
      "Training loss: 0.11729615926742554\n",
      "Training loss: 0.11663901060819626\n",
      "Training loss: 0.10578739643096924\n",
      "Training loss: 0.11369185894727707\n",
      "Training loss: 0.11505810171365738\n",
      "Training loss: 0.11191198229789734\n",
      "Training loss: 0.1097598522901535\n",
      "Training loss: 0.11276653409004211\n",
      "Training loss: 0.11973563581705093\n",
      "Training loss: 0.10898537188768387\n",
      "Training loss: 0.10901065915822983\n",
      "Training loss: 0.1210368424654007\n",
      "Training loss: 0.10882462561130524\n",
      "Training loss: 0.11496374756097794\n",
      "Training loss: 0.10621416568756104\n",
      "Training loss: 0.11389659345149994\n",
      "Training loss: 0.11329878121614456\n",
      "Training loss: 0.10546387732028961\n",
      "Training loss: 0.11891188472509384\n",
      "Training loss: 0.11465458571910858\n",
      "Training loss: 0.13014142215251923\n",
      "Training loss: 0.1149045005440712\n",
      "Training loss: 0.1132267490029335\n",
      "Training loss: 0.11464715749025345\n",
      "Training loss: 0.11627169698476791\n",
      "Training loss: 0.11044967174530029\n",
      "Training loss: 0.11574460566043854\n",
      "Training loss: 0.10690937936306\n",
      "Training loss: 0.10496385395526886\n",
      "Training loss: 0.11305075883865356\n",
      "Training loss: 0.1117539331316948\n",
      "Training loss: 0.12025494128465652\n",
      "Training loss: 0.11183331906795502\n",
      "Training loss: 0.10929293930530548\n",
      "Training loss: 0.10911265760660172\n",
      "Training loss: 0.11956621706485748\n",
      "Training loss: 0.10889261960983276\n",
      "Training loss: 0.11222919821739197\n",
      "Training loss: 0.10899218916893005\n",
      "Training loss: 0.13293962180614471\n",
      "Training loss: 0.10880650579929352\n",
      "Training loss: 0.1077294573187828\n",
      "Training loss: 0.11786822974681854\n",
      "Training loss: 0.11421012133359909\n",
      "Training loss: 0.11768236756324768\n",
      "Training loss: 0.1160891130566597\n",
      "Training loss: 0.10296285897493362\n",
      "Training loss: 0.11738554388284683\n",
      "Training loss: 0.11256638169288635\n",
      "Training loss: 0.1103423610329628\n",
      "Training loss: 0.11147308349609375\n",
      "Training loss: 0.11234113574028015\n",
      "Training loss: 0.10924915969371796\n",
      "Training loss: 0.11205200105905533\n",
      "Training loss: 0.10873054713010788\n",
      "Training loss: 0.11726287752389908\n",
      "Training loss: 0.11547359824180603\n",
      "Training loss: 0.11180415749549866\n",
      "Training loss: 0.11172012984752655\n",
      "Training loss: 0.11943861842155457\n",
      "Training loss: 0.11441918462514877\n",
      "Training loss: 0.11161097884178162\n",
      "Training loss: 0.11495435237884521\n",
      "Training loss: 0.11862270534038544\n",
      "Training loss: 0.12789469957351685\n",
      "Training loss: 0.11700528115034103\n",
      "Training loss: 0.11826124787330627\n",
      "Training loss: 0.11587616056203842\n",
      "Training loss: 0.11236762255430222\n",
      "Training loss: 0.12390130758285522\n",
      "Training loss: 0.10757266730070114\n",
      "Training loss: 0.11803555488586426\n",
      "Training loss: 0.11321648955345154\n",
      "Training loss: 0.11179859191179276\n",
      "Training loss: 0.11513116955757141\n",
      "Training loss: 0.11497686058282852\n",
      "Training loss: 0.11101951450109482\n",
      "Training loss: 0.11705679446458817\n",
      "Training loss: 0.11527691781520844\n",
      "Training loss: 0.11238333582878113\n",
      "Training loss: 0.11143570393323898\n",
      "Training loss: 0.11444788426160812\n",
      "Training loss: 0.10738220810890198\n",
      "Training loss: 0.11933011561632156\n",
      "Training loss: 0.11309117823839188\n",
      "Training loss: 0.1137532889842987\n",
      "Training loss: 0.11158759891986847\n",
      "Training loss: 0.11142868548631668\n",
      "Training loss: 0.11393885314464569\n",
      "Training loss: 0.13084962964057922\n",
      "Training loss: 0.11350701004266739\n",
      "Training loss: 0.11727145314216614\n",
      "Training loss: 0.11721748858690262\n",
      "Training loss: 0.11849350482225418\n",
      "Training loss: 0.10997102409601212\n",
      "Training loss: 0.11447221040725708\n",
      "Training loss: 0.10878840833902359\n",
      "Training loss: 0.11396205425262451\n",
      "Training loss: 0.11388923227787018\n",
      "Training loss: 0.11013055592775345\n",
      "Training loss: 0.12081913650035858\n",
      "Training loss: 0.12329591810703278\n",
      "Training loss: 0.13074247539043427\n",
      "Training loss: 0.12541039288043976\n",
      "Training loss: 0.11581164598464966\n",
      "Training loss: 0.11645528674125671\n",
      "Training loss: 0.11290937662124634\n",
      "Training loss: 0.11863034218549728\n",
      "Training loss: 0.12104250490665436\n",
      "Training loss: 0.12248211354017258\n",
      "Training loss: 0.12333507835865021\n",
      "Training loss: 0.11386825889348984\n",
      "Training loss: 0.11660683155059814\n",
      "Training loss: 0.10904736071825027\n",
      "Training loss: 0.1151968240737915\n",
      "Training loss: 0.12864501774311066\n",
      "Training loss: 0.1219048947095871\n",
      "Training loss: 0.12490997463464737\n",
      "Training loss: 0.11958608776330948\n",
      "Training loss: 0.12058685719966888\n",
      "Training loss: 0.11557184904813766\n",
      "Training loss: 0.11174589395523071\n",
      "Training loss: 0.12640443444252014\n",
      "Training loss: 0.11472643911838531\n",
      "Training loss: 0.11675667017698288\n",
      "Training loss: 0.1258665770292282\n",
      "Training loss: 0.11306650936603546\n",
      "Training loss: 0.11676181852817535\n",
      "Training loss: 0.12015978246927261\n",
      "Training loss: 0.11409308016300201\n",
      "Training loss: 0.11011864989995956\n",
      "Training loss: 0.11305543780326843\n",
      "Training loss: 0.11539795249700546\n",
      "Training loss: 0.11284651607275009\n",
      "Training loss: 0.12016128748655319\n",
      "Training loss: 0.1372617781162262\n",
      "Training loss: 0.11635418981313705\n",
      "Training loss: 0.11545031517744064\n",
      "Training loss: 0.11179444938898087\n",
      "Training loss: 0.11827772110700607\n",
      "Training loss: 0.1102520003914833\n",
      "Training loss: 0.1108969897031784\n",
      "Training loss: 0.10979651659727097\n",
      "Training loss: 0.11728887259960175\n",
      "Training loss: 0.11823803931474686\n",
      "Training loss: 0.1137487143278122\n",
      "Training loss: 0.1122211292386055\n",
      "Training loss: 0.11480101197957993\n",
      "Training loss: 0.10953530669212341\n",
      "Training loss: 0.11745010316371918\n",
      "Training loss: 0.10506097227334976\n",
      "Training loss: 0.10912016779184341\n",
      "Training loss: 0.1094442754983902\n",
      "Training loss: 0.11077078431844711\n",
      "Training loss: 0.11093167215585709\n",
      "Training loss: 0.10861422121524811\n",
      "Training loss: 0.11406862735748291\n",
      "Training loss: 0.11626484990119934\n",
      "Training loss: 0.11696477979421616\n",
      "Training loss: 0.10715792328119278\n",
      "Training loss: 0.11646109819412231\n",
      "Training loss: 0.11324016004800797\n",
      "Training loss: 0.11537516862154007\n",
      "Training loss: 0.11293896287679672\n",
      "Training loss: 0.11325763911008835\n",
      "Training loss: 0.11510729044675827\n",
      "Training loss: 0.10665632039308548\n",
      "Training loss: 0.11045080423355103\n",
      "Training loss: 0.11123968660831451\n",
      "Training loss: 0.11319153755903244\n",
      "Training loss: 0.11469266563653946\n",
      "Training loss: 0.12315817177295685\n",
      "Training loss: 0.11030464619398117\n",
      "Training loss: 0.11146379262208939\n",
      "Training loss: 0.11394120007753372\n",
      "Training loss: 0.11798133701086044\n",
      "Training loss: 0.1122257262468338\n",
      "Training loss: 0.11168566346168518\n",
      "Training loss: 0.10763265192508698\n",
      "Training loss: 0.11241122335195541\n",
      "Training loss: 0.1132856011390686\n",
      "Training loss: 0.11550775170326233\n",
      "Training loss: 0.11456048488616943\n",
      "Training loss: 0.11071687191724777\n",
      "Training loss: 0.1089443638920784\n",
      "Training loss: 0.11261601746082306\n",
      "Training loss: 0.11188087612390518\n",
      "Training loss: 0.1036614403128624\n",
      "Training loss: 0.11276353895664215\n",
      "Training loss: 0.10996748507022858\n",
      "Training loss: 0.10967427492141724\n",
      "Training loss: 0.10937518626451492\n",
      "Training loss: 0.11435739696025848\n",
      "Training loss: 0.11384281516075134\n",
      "Training loss: 0.11790840327739716\n",
      "Training loss: 0.10707753151655197\n",
      "Training loss: 0.11113627254962921\n",
      "Training loss: 0.10688064247369766\n",
      "Training loss: 0.11450662463903427\n",
      "Training loss: 0.1001967340707779\n",
      "Training loss: 0.11620501428842545\n",
      "Training loss: 0.11358913779258728\n",
      "Training loss: 0.10612981766462326\n",
      "Training loss: 0.10929170995950699\n",
      "Training loss: 0.1071067824959755\n",
      "Training loss: 0.11499253660440445\n",
      "Training loss: 0.11533211916685104\n",
      "Training loss: 0.10870973020792007\n",
      "Training loss: 0.10825620591640472\n",
      "Training loss: 0.10876744985580444\n",
      "Training loss: 0.1113755851984024\n",
      "Training loss: 0.1127062737941742\n",
      "Training loss: 0.11051055788993835\n",
      "Training loss: 0.11192863434553146\n",
      "Training loss: 0.11583642661571503\n",
      "Training loss: 0.10826391726732254\n",
      "Training loss: 0.1183081716299057\n",
      "Training loss: 0.11148015409708023\n",
      "Training loss: 0.10904525220394135\n",
      "Training loss: 0.11598634719848633\n",
      "Training loss: 0.1123267188668251\n",
      "Training loss: 0.10871439427137375\n",
      "Training loss: 0.11583893746137619\n",
      "Training loss: 0.10503245145082474\n",
      "Training loss: 0.11663425713777542\n",
      "Training loss: 0.11122878640890121\n",
      "Training loss: 0.10533758252859116\n",
      "Training loss: 0.10917401313781738\n",
      "Training loss: 0.11598693579435349\n",
      "Training loss: 0.11623714119195938\n",
      "Training loss: 0.10903007537126541\n",
      "Training loss: 0.11377747356891632\n",
      "Training loss: 0.11073282361030579\n",
      "Training loss: 0.1108175739645958\n",
      "Training loss: 0.11962677538394928\n",
      "Training loss: 0.10926368832588196\n",
      "Training loss: 0.11581940948963165\n",
      "Training loss: 0.10662653297185898\n",
      "Training loss: 0.11747558414936066\n",
      "Training loss: 0.12485525012016296\n",
      "Training loss: 0.11032606661319733\n",
      "Training loss: 0.10667536407709122\n",
      "Training loss: 0.11326049268245697\n",
      "Training loss: 0.10712028294801712\n",
      "Training loss: 0.11501358449459076\n",
      "Training loss: 0.1121131181716919\n",
      "Training loss: 0.10654937475919724\n",
      "Training loss: 0.10869822651147842\n",
      "Training loss: 0.10913971811532974\n",
      "Training loss: 0.11166688054800034\n",
      "Training loss: 0.11296620965003967\n",
      "Training loss: 0.10925010591745377\n",
      "Training loss: 0.1425887495279312\n",
      "Training loss: 0.11037634313106537\n",
      "Training loss: 0.11181734502315521\n",
      "Training loss: 0.11010123044252396\n",
      "Training loss: 0.11746761947870255\n",
      "Training loss: 0.11342176049947739\n",
      "Training loss: 0.10507455468177795\n",
      "Training loss: 0.11187709122896194\n",
      "Training loss: 0.11725123226642609\n",
      "Training loss: 0.11955219507217407\n",
      "Training loss: 0.11234839260578156\n",
      "Training loss: 0.10878454148769379\n",
      "Training loss: 0.11804701387882233\n",
      "Training loss: 0.11413321644067764\n",
      "Training loss: 0.11010941863059998\n",
      "Training loss: 0.1122443899512291\n",
      "Training loss: 0.10963629931211472\n",
      "Training loss: 0.11232508718967438\n",
      "Training loss: 0.1072387769818306\n",
      "Training loss: 0.11050888150930405\n",
      "Training loss: 0.11652220040559769\n",
      "Training loss: 0.11327127367258072\n",
      "Training loss: 0.11176400631666183\n",
      "Training loss: 0.11573689430952072\n",
      "Training loss: 0.10882715880870819\n",
      "Training loss: 0.11045845597982407\n",
      "Training loss: 0.11717035621404648\n",
      "Training loss: 0.10650063306093216\n",
      "Training loss: 0.10970556735992432\n",
      "Training loss: 0.10957200080156326\n",
      "Training loss: 0.11296011507511139\n",
      "Training loss: 0.11347664892673492\n",
      "Training loss: 0.11472790688276291\n",
      "Training loss: 0.11053194105625153\n",
      "Training loss: 0.11124332994222641\n",
      "Training loss: 0.10582650452852249\n",
      "Training loss: 0.22630061209201813\n",
      "Training loss: 0.10966696590185165\n",
      "Training loss: 0.11478648334741592\n",
      "Training loss: 0.10522180050611496\n",
      "Training loss: 0.11004337668418884\n",
      "Training loss: 0.11223359405994415\n",
      "Training loss: 0.12094613164663315\n",
      "Training loss: 0.10787414759397507\n",
      "Training loss: 0.11409399658441544\n",
      "Training loss: 0.10964073985815048\n",
      "Training loss: 0.10786333680152893\n",
      "Training loss: 0.11018705368041992\n",
      "Training loss: 0.11510562896728516\n",
      "Training loss: 0.10747431963682175\n",
      "Training loss: 0.11078561097383499\n",
      "Training loss: 0.1046944260597229\n",
      "Training loss: 0.10957583785057068\n",
      "Training loss: 0.11933209747076035\n",
      "Training loss: 0.1114082783460617\n",
      "Training loss: 0.1174948439002037\n",
      "Training loss: 0.1154683530330658\n",
      "Training loss: 0.1111222431063652\n",
      "Training loss: 0.1215638741850853\n",
      "Training loss: 0.11138841509819031\n",
      "Training loss: 0.11261244863271713\n",
      "Training loss: 0.11691945791244507\n",
      "Training loss: 0.11476349085569382\n",
      "Training loss: 0.11564892530441284\n",
      "Training loss: 0.1211099773645401\n",
      "Training loss: 0.12090154737234116\n",
      "Training loss: 0.11316709965467453\n",
      "Training loss: 0.12015295773744583\n",
      "Training loss: 0.1563553363084793\n",
      "Training loss: 0.12791699171066284\n",
      "Training loss: 0.11221326142549515\n",
      "Training loss: 0.11150306463241577\n",
      "Training loss: 0.12034185975790024\n",
      "Training loss: 0.11944244801998138\n",
      "Training loss: 0.11415599286556244\n",
      "Training loss: 0.1190018281340599\n",
      "Training loss: 0.11422638595104218\n",
      "Training loss: 0.11786532402038574\n",
      "Training loss: 0.12000706791877747\n",
      "Training loss: 0.12048275023698807\n",
      "Training loss: 0.1148378774523735\n",
      "Training loss: 0.12904615700244904\n",
      "Training loss: 0.10710269212722778\n",
      "Training loss: 0.11682561039924622\n",
      "Training loss: 0.10732036828994751\n",
      "Training loss: 0.10928799957036972\n",
      "Training loss: 0.11841617524623871\n",
      "Training loss: 0.1119004338979721\n",
      "Training loss: 0.10615390539169312\n",
      "Training loss: 0.1126929521560669\n",
      "Training loss: 0.11248751729726791\n",
      "Training loss: 0.1306271255016327\n",
      "Training loss: 0.11786459386348724\n",
      "Training loss: 0.1100141853094101\n",
      "Training loss: 0.1120670884847641\n",
      "Training loss: 0.11660970002412796\n",
      "Training loss: 0.11746876686811447\n",
      "Training loss: 0.11173105984926224\n",
      "Training loss: 0.10636075586080551\n",
      "Training loss: 0.11166688799858093\n",
      "Training loss: 0.10652924329042435\n",
      "Training loss: 0.11524379998445511\n",
      "Training loss: 0.1151907667517662\n",
      "Training loss: 0.11089987307786942\n",
      "Training loss: 0.10973381251096725\n",
      "Training loss: 0.13109639286994934\n",
      "Training loss: 0.11785218864679337\n",
      "Training loss: 0.10527364909648895\n",
      "Training loss: 0.11124841868877411\n",
      "Training loss: 0.11692117154598236\n",
      "Training loss: 0.11317843943834305\n",
      "Training loss: 0.11303628236055374\n",
      "Training loss: 0.11452064663171768\n",
      "Training loss: 0.10475821048021317\n",
      "Training loss: 0.11302664875984192\n",
      "Training loss: 0.11524242907762527\n",
      "Training loss: 0.10499826818704605\n",
      "Training loss: 0.112376868724823\n",
      "Training loss: 0.10991478711366653\n",
      "Training loss: 0.10787505656480789\n",
      "Training loss: 0.11762619763612747\n",
      "Training loss: 0.12237628549337387\n",
      "Training loss: 0.10992089658975601\n",
      "Training loss: 0.11791157722473145\n",
      "Training loss: 0.11747843027114868\n",
      "Training loss: 0.12272165715694427\n",
      "Training loss: 0.11191929131746292\n",
      "Training loss: 0.11335169523954391\n",
      "Training loss: 0.11237481236457825\n",
      "Training loss: 0.11213143169879913\n",
      "Training loss: 0.12100724130868912\n",
      "Training loss: 0.11294540017843246\n",
      "Training loss: 0.11071161925792694\n",
      "Training loss: 0.10871128737926483\n",
      "Training loss: 0.10616841167211533\n",
      "Training loss: 0.11298705637454987\n",
      "Training loss: 0.11742491275072098\n",
      "Training loss: 0.11048606038093567\n",
      "Training loss: 0.11320119351148605\n",
      "Training loss: 0.10807772725820541\n",
      "Training loss: 0.1255207359790802\n",
      "Training loss: 0.11047862470149994\n",
      "Training loss: 0.10552722960710526\n",
      "Training loss: 0.10737335681915283\n",
      "Training loss: 0.12325242161750793\n",
      "Training loss: 0.11150781810283661\n",
      "Training loss: 0.10706689953804016\n",
      "Training loss: 0.11663010716438293\n",
      "Training loss: 0.11622905731201172\n",
      "Training loss: 0.10476618260145187\n",
      "Training loss: 0.10572246462106705\n",
      "Training loss: 0.11346212029457092\n",
      "Training loss: 0.11196132004261017\n",
      "Training loss: 0.11774486303329468\n",
      "Training loss: 0.11014200001955032\n",
      "Training loss: 0.1064845621585846\n",
      "Training loss: 0.11822274327278137\n",
      "Training loss: 0.10615427047014236\n",
      "Training loss: 0.10554925352334976\n",
      "Training loss: 0.10446273535490036\n",
      "Training loss: 0.11850173026323318\n",
      "Training loss: 0.10891585052013397\n",
      "Training loss: 0.11157941818237305\n",
      "Training loss: 0.1130128800868988\n",
      "Training loss: 0.11272959411144257\n",
      "Training loss: 0.1135183572769165\n",
      "Training loss: 0.11697982251644135\n",
      "Training loss: 0.10232001543045044\n",
      "Training loss: 0.11455133557319641\n",
      "Training loss: 0.1188679188489914\n",
      "Training loss: 0.10760754346847534\n",
      "Training loss: 0.11329439282417297\n",
      "Training loss: 0.11584685742855072\n",
      "Training loss: 0.11017360538244247\n",
      "Training loss: 0.10885544866323471\n",
      "Training loss: 0.11646666377782822\n",
      "Training loss: 0.11276794970035553\n",
      "Training loss: 0.11661355197429657\n",
      "Training loss: 0.11139551550149918\n",
      "Training loss: 0.10544577986001968\n",
      "Training loss: 0.11289375275373459\n",
      "Training loss: 0.11148642748594284\n",
      "Training loss: 0.11617723852396011\n",
      "Training loss: 0.11485839635133743\n",
      "Training loss: 0.10990030318498611\n",
      "Training loss: 0.10826950520277023\n",
      "Training loss: 0.11712681502103806\n",
      "Training loss: 0.11395110934972763\n",
      "Training loss: 0.10931529849767685\n",
      "Training loss: 0.12325067818164825\n",
      "Training loss: 0.11546427011489868\n",
      "Training loss: 0.11780598014593124\n",
      "Training loss: 0.12018268555402756\n",
      "Training loss: 0.11564113944768906\n",
      "Training loss: 0.11575667560100555\n",
      "Training loss: 0.11115157604217529\n",
      "Training loss: 0.11832340061664581\n",
      "Training loss: 0.10435070097446442\n",
      "Training loss: 0.11274240911006927\n",
      "Training loss: 0.10921698808670044\n",
      "Training loss: 0.10936515033245087\n",
      "Training loss: 0.11023437976837158\n",
      "Training loss: 0.11016525328159332\n",
      "Training loss: 0.11417701095342636\n",
      "Training loss: 0.10554768145084381\n",
      "Training loss: 0.11543381214141846\n",
      "Training loss: 0.11104682087898254\n",
      "Training loss: 0.10410428047180176\n",
      "Training loss: 0.10572309046983719\n",
      "Training loss: 0.10427463799715042\n",
      "Training loss: 0.10998065769672394\n",
      "Training loss: 0.1071232259273529\n",
      "Training loss: 0.1075419932603836\n",
      "Training loss: 0.10480109602212906\n",
      "Training loss: 0.10620658844709396\n",
      "Training loss: 0.10785944014787674\n",
      "Training loss: 0.10403148829936981\n",
      "Training loss: 0.1115066409111023\n",
      "Training loss: 0.11661915481090546\n",
      "Training loss: 0.12177767604589462\n",
      "Training loss: 0.11000315099954605\n",
      "Training loss: 0.10945503413677216\n",
      "Training loss: 0.10617681592702866\n",
      "Training loss: 0.1119670495390892\n",
      "Training loss: 0.11313191056251526\n",
      "Training loss: 0.10634522885084152\n",
      "Training loss: 0.11073343455791473\n",
      "Training loss: 0.11512964218854904\n",
      "Training loss: 0.1054433062672615\n",
      "Training loss: 0.11074282228946686\n",
      "Training loss: 0.11953429132699966\n",
      "Training loss: 0.10939186811447144\n",
      "Training loss: 0.10844496637582779\n",
      "Training loss: 0.11460542678833008\n",
      "Training loss: 0.12739239633083344\n",
      "Training loss: 0.10463981330394745\n",
      "Training loss: 0.10767599940299988\n",
      "Training loss: 0.11396928131580353\n",
      "Training loss: 0.11689795553684235\n",
      "Training loss: 0.11635001003742218\n",
      "Training loss: 0.11321461945772171\n",
      "Training loss: 0.11047610640525818\n",
      "Training loss: 0.11581462621688843\n",
      "Training loss: 0.10927294194698334\n",
      "Training loss: 0.11074782907962799\n",
      "Training loss: 0.11215922236442566\n",
      "Training loss: 0.11272867769002914\n",
      "Training loss: 0.10927198082208633\n",
      "Training loss: 0.11076347529888153\n",
      "Training loss: 0.10985641926527023\n",
      "Training loss: 0.11874797940254211\n",
      "Training loss: 0.11853720247745514\n",
      "Training loss: 0.10867126286029816\n",
      "Training loss: 0.10861125588417053\n",
      "Training loss: 0.10711093246936798\n",
      "Training loss: 0.11105707287788391\n",
      "Training loss: 0.11952868103981018\n",
      "Training loss: 0.1129368245601654\n",
      "Training loss: 0.11075584590435028\n",
      "Training loss: 0.11081572622060776\n",
      "Training loss: 0.11230569332838058\n",
      "Training loss: 0.10095846652984619\n",
      "Training loss: 0.1032879501581192\n",
      "Training loss: 0.11080057919025421\n",
      "Training loss: 0.10609297454357147\n",
      "Training loss: 0.11249583959579468\n",
      "Training loss: 0.11259330064058304\n",
      "Training loss: 0.1094624400138855\n",
      "Training loss: 0.10358487069606781\n",
      "Training loss: 0.10387898981571198\n",
      "Training loss: 0.12314781546592712\n",
      "Training loss: 0.1170661449432373\n",
      "Training loss: 0.10426788777112961\n",
      "Training loss: 0.10676369816064835\n",
      "Training loss: 0.10945163667201996\n",
      "Training loss: 0.10508079826831818\n",
      "Training loss: 0.11015807092189789\n",
      "Training loss: 0.1134665459394455\n",
      "Training loss: 0.11502086371183395\n",
      "Training loss: 0.1143377348780632\n",
      "Training loss: 0.11120003461837769\n",
      "Training loss: 0.105891652405262\n",
      "Training loss: 0.10294505208730698\n",
      "Training loss: 0.10711848735809326\n",
      "Training loss: 0.11192625761032104\n",
      "Training loss: 0.10647924244403839\n",
      "Training loss: 0.1054614707827568\n",
      "Training loss: 0.11337082833051682\n",
      "Training loss: 0.1026063859462738\n",
      "Training loss: 0.10657363384962082\n",
      "Training loss: 0.10434819757938385\n",
      "Training loss: 0.11261335015296936\n",
      "Training loss: 0.10292685776948929\n",
      "Training loss: 0.11221436411142349\n",
      "Training loss: 0.112604521214962\n",
      "Training loss: 0.11452475190162659\n",
      "Training loss: 0.11724629253149033\n",
      "Training loss: 0.10925468802452087\n",
      "Training loss: 0.10861318558454514\n",
      "Training loss: 0.10426629334688187\n",
      "Training loss: 0.10670991986989975\n",
      "Training loss: 0.10567895323038101\n",
      "Training loss: 0.1048356369137764\n",
      "Training loss: 0.10285723209381104\n",
      "Training loss: 0.10940860956907272\n",
      "Training loss: 0.10533316433429718\n",
      "Training loss: 0.11142594367265701\n",
      "Training loss: 0.108062744140625\n",
      "Training loss: 0.106650710105896\n",
      "Training loss: 0.10289278626441956\n",
      "Training loss: 0.10657795518636703\n",
      "Training loss: 0.10194076597690582\n",
      "Training loss: 0.10331493616104126\n",
      "Training loss: 0.1031031385064125\n",
      "Training loss: 0.10215910524129868\n",
      "Training loss: 0.10461145639419556\n",
      "Training loss: 0.10965846478939056\n",
      "Training loss: 0.1029914990067482\n",
      "Training loss: 0.10233407467603683\n",
      "Training loss: 0.10085707157850266\n",
      "Training loss: 0.104122593998909\n",
      "Training loss: 0.10103610157966614\n",
      "Training loss: 0.10411415994167328\n",
      "Training loss: 0.10538538545370102\n",
      "Training loss: 0.0982792004942894\n",
      "Training loss: 0.1116265282034874\n",
      "Training loss: 0.10492237657308578\n",
      "Training loss: 0.11178795993328094\n",
      "Training loss: 0.10660597681999207\n",
      "Training loss: 0.12292805314064026\n",
      "Training loss: 0.10064799338579178\n",
      "Training loss: 0.1106010377407074\n",
      "Training loss: 0.10604743659496307\n",
      "Training loss: 0.11772412806749344\n",
      "Training loss: 0.11548564583063126\n",
      "Training loss: 0.11038163304328918\n",
      "Training loss: 0.11272918432950974\n",
      "Training loss: 0.11570693552494049\n",
      "Training loss: 0.1132647916674614\n",
      "Training loss: 0.10725829005241394\n",
      "Training loss: 0.11112431436777115\n",
      "Training loss: 0.10531304031610489\n",
      "Training loss: 0.11335400491952896\n",
      "Training loss: 0.1093020960688591\n",
      "Training loss: 0.11520412564277649\n",
      "Training loss: 0.108346126973629\n",
      "Training loss: 0.11299788951873779\n",
      "Training loss: 0.11023664474487305\n",
      "Training loss: 0.10948856920003891\n",
      "Training loss: 0.10554640740156174\n",
      "Training loss: 0.12137692421674728\n",
      "Training loss: 0.10666096210479736\n",
      "Training loss: 0.11408324539661407\n",
      "Training loss: 0.1062089130282402\n",
      "Training loss: 0.11059106141328812\n",
      "Training loss: 0.11005910485982895\n",
      "Training loss: 0.12001793086528778\n",
      "Training loss: 0.11241801083087921\n",
      "Training loss: 0.11263822764158249\n",
      "Training loss: 0.11161775141954422\n",
      "Training loss: 0.10727962851524353\n",
      "Training loss: 0.11834318935871124\n",
      "Training loss: 0.1109628826379776\n",
      "Training loss: 0.11575693637132645\n",
      "Training loss: 0.10913494229316711\n",
      "Training loss: 0.11273687332868576\n",
      "Training loss: 0.10953313112258911\n",
      "Training loss: 0.10844656825065613\n",
      "Training loss: 0.10946156829595566\n",
      "Training loss: 0.11642222106456757\n",
      "Training loss: 0.10962193459272385\n",
      "Training loss: 0.1094854325056076\n",
      "Training loss: 0.11104343831539154\n",
      "Training loss: 0.11428213864564896\n",
      "Training loss: 0.10594100505113602\n",
      "Training loss: 0.10563196241855621\n",
      "Training loss: 0.10347149521112442\n",
      "Training loss: 0.10703112930059433\n",
      "Training loss: 0.106464684009552\n",
      "Training loss: 0.11194386333227158\n",
      "Training loss: 0.10138116031885147\n",
      "Training loss: 0.10727489739656448\n",
      "Training loss: 0.10360515117645264\n",
      "Training loss: 0.10547717660665512\n",
      "Training loss: 0.10016071051359177\n",
      "Training loss: 0.1056588888168335\n",
      "Training loss: 0.10380873829126358\n",
      "Training loss: 0.10294253379106522\n",
      "Training loss: 0.10904059559106827\n",
      "Training loss: 0.10117831826210022\n",
      "Training loss: 0.10132860392332077\n",
      "Training loss: 0.10789986699819565\n",
      "Training loss: 0.11123470962047577\n",
      "Training loss: 0.10715699940919876\n",
      "Training loss: 0.10009642690420151\n",
      "Training loss: 0.09658177942037582\n",
      "Training loss: 0.11306235194206238\n",
      "Training loss: 0.10070675611495972\n",
      "Training loss: 0.09977412968873978\n",
      "Training loss: 0.09818524122238159\n",
      "Training loss: 0.10816885530948639\n",
      "Training loss: 0.09997937083244324\n",
      "Training loss: 0.09576807916164398\n",
      "Training loss: 0.1101970300078392\n",
      "Training loss: 0.10191123187541962\n",
      "Training loss: 0.10335474461317062\n",
      "Training loss: 0.10062787681818008\n",
      "Training loss: 0.09797733277082443\n",
      "Training loss: 0.10612261295318604\n",
      "Training loss: 0.10683839023113251\n",
      "Training loss: 0.10356774181127548\n",
      "Training loss: 0.10844913125038147\n",
      "Training loss: 0.10254646092653275\n",
      "Training loss: 0.10453498363494873\n",
      "Training loss: 0.10785051435232162\n",
      "Training loss: 0.10434984415769577\n",
      "Training loss: 0.10964380949735641\n",
      "Training loss: 0.10197986662387848\n",
      "Training loss: 0.11096437275409698\n",
      "Training loss: 0.10495109856128693\n",
      "Training loss: 0.10399632155895233\n",
      "Training loss: 0.09712478518486023\n",
      "Training loss: 0.10228075832128525\n",
      "Training loss: 0.1046474352478981\n",
      "Training loss: 0.10727206617593765\n",
      "Training loss: 0.11204598844051361\n",
      "Training loss: 0.11964692175388336\n",
      "Training loss: 0.1091599315404892\n",
      "Training loss: 0.10258589684963226\n",
      "Training loss: 0.11056021600961685\n",
      "Training loss: 0.1056130975484848\n",
      "Training loss: 0.10735953599214554\n",
      "Training loss: 0.11194521933794022\n",
      "Training loss: 0.10151620954275131\n",
      "Training loss: 0.10159987956285477\n",
      "Training loss: 0.10057886689901352\n",
      "Training loss: 0.10383550822734833\n",
      "Training loss: 0.10506102442741394\n",
      "Training loss: 0.10491570085287094\n",
      "Training loss: 0.1021566092967987\n",
      "Training loss: 0.11607029289007187\n",
      "Training loss: 0.1158016100525856\n",
      "Training loss: 0.1042221263051033\n",
      "Training loss: 0.10634392499923706\n",
      "Training loss: 0.10620392113924026\n",
      "Training loss: 0.11158581078052521\n",
      "Training loss: 0.0972026139497757\n",
      "Training loss: 0.11049868911504745\n",
      "Training loss: 0.10617689043283463\n",
      "Training loss: 0.11331692337989807\n",
      "Training loss: 0.1097278818488121\n",
      "Training loss: 0.1084728091955185\n",
      "Training loss: 0.10722282528877258\n",
      "Training loss: 0.10423335433006287\n",
      "Training loss: 0.10737231373786926\n",
      "Training loss: 0.10495995730161667\n",
      "Training loss: 0.10352271795272827\n",
      "Training loss: 0.1102236732840538\n",
      "Training loss: 0.10839097201824188\n",
      "Training loss: 0.10836833715438843\n",
      "Training loss: 0.10608307272195816\n",
      "Training loss: 0.11049607396125793\n",
      "Training loss: 0.1066618338227272\n",
      "Training loss: 0.10907188057899475\n",
      "Training loss: 0.1088588684797287\n",
      "Training loss: 0.10668683797121048\n",
      "Training loss: 0.11439592391252518\n",
      "Training loss: 0.10838578641414642\n",
      "Training loss: 0.1049514189362526\n",
      "Training loss: 0.10385193675756454\n",
      "Training loss: 0.10975131392478943\n",
      "Training loss: 0.11501264572143555\n",
      "Training loss: 0.10445139557123184\n",
      "Training loss: 0.10907872766256332\n",
      "Training loss: 0.10218480229377747\n",
      "Training loss: 0.10432321578264236\n",
      "Training loss: 0.1086890771985054\n",
      "Training loss: 0.10738930851221085\n",
      "Training loss: 0.11281169205904007\n",
      "Training loss: 0.10814804583787918\n",
      "Training loss: 0.09947500377893448\n",
      "Training loss: 0.11068160831928253\n",
      "Training loss: 0.12155908346176147\n",
      "Training loss: 0.1061430349946022\n",
      "Training loss: 0.10878355801105499\n",
      "Training loss: 0.10546614229679108\n",
      "Training loss: 0.10492242872714996\n",
      "Training loss: 0.10320613533258438\n",
      "Training loss: 0.10766137391328812\n",
      "Training loss: 0.1136525347828865\n",
      "Training loss: 0.11386176943778992\n",
      "Training loss: 0.10354888439178467\n",
      "Training loss: 0.11076394468545914\n",
      "Training loss: 0.11197122931480408\n",
      "Training loss: 0.10788038372993469\n",
      "Training loss: 0.10063361376523972\n",
      "Training loss: 0.10667961090803146\n",
      "Training loss: 0.1081172451376915\n",
      "Training loss: 0.1134016364812851\n",
      "Training loss: 0.11068485677242279\n",
      "Training loss: 0.11543425172567368\n",
      "Training loss: 0.10643130540847778\n",
      "Training loss: 0.10488251596689224\n",
      "Training loss: 0.10912619531154633\n",
      "Training loss: 0.11575593054294586\n",
      "Training loss: 0.11190355569124222\n",
      "Training loss: 0.10978560149669647\n",
      "Training loss: 0.11469084769487381\n",
      "Training loss: 0.11438938230276108\n",
      "Training loss: 0.11256841570138931\n",
      "Training loss: 0.11470749974250793\n",
      "Training loss: 0.10240136831998825\n",
      "Training loss: 0.107584148645401\n",
      "Training loss: 0.11494365334510803\n",
      "Training loss: 0.10972510278224945\n",
      "Training loss: 0.10538551956415176\n",
      "Training loss: 0.1076379045844078\n",
      "Training loss: 0.11381082236766815\n",
      "Training loss: 0.11114984005689621\n",
      "Training loss: 0.11096066236495972\n",
      "Training loss: 0.10704102367162704\n",
      "Training loss: 0.10847755521535873\n",
      "Training loss: 0.10650119185447693\n",
      "Training loss: 0.10612676292657852\n",
      "Training loss: 0.10956982523202896\n",
      "Training loss: 0.1088317334651947\n",
      "Training loss: 0.10657716542482376\n",
      "Training loss: 0.10592500120401382\n",
      "Training loss: 0.10101177543401718\n",
      "Training loss: 0.10744460672140121\n",
      "Training loss: 0.10732898861169815\n",
      "Training loss: 0.12154743820428848\n",
      "Training loss: 0.10591917484998703\n",
      "Training loss: 0.10684625059366226\n",
      "Training loss: 0.11652685701847076\n",
      "Training loss: 0.10741367936134338\n",
      "Training loss: 0.10574513673782349\n",
      "Training loss: 0.10310102999210358\n",
      "Training loss: 0.105924092233181\n",
      "Training loss: 0.10243312269449234\n",
      "Training loss: 0.10476062446832657\n",
      "Training loss: 0.11004983633756638\n",
      "Training loss: 0.10719787329435349\n",
      "Training loss: 0.1142384484410286\n",
      "Training loss: 0.10341189056634903\n",
      "Training loss: 0.1072617694735527\n",
      "Training loss: 0.11558812111616135\n",
      "Training loss: 0.10163193196058273\n",
      "Training loss: 0.1097252368927002\n",
      "Training loss: 0.1057085171341896\n",
      "Training loss: 0.10436129570007324\n",
      "Training loss: 0.11195207387208939\n",
      "Training loss: 0.10117526352405548\n",
      "Training loss: 0.10402681678533554\n",
      "Training loss: 0.10167037695646286\n",
      "Training loss: 0.1077800840139389\n",
      "Training loss: 0.10308606177568436\n",
      "Training loss: 0.11009163409471512\n",
      "Training loss: 0.10831544548273087\n",
      "Training loss: 0.1001788079738617\n",
      "Training loss: 0.10067880153656006\n",
      "Training loss: 0.11111439764499664\n",
      "Training loss: 0.103640116751194\n",
      "Training loss: 0.10741280019283295\n",
      "Training loss: 0.10287665575742722\n",
      "Training loss: 0.10439908504486084\n",
      "Training loss: 0.102020263671875\n",
      "Training loss: 0.10940524190664291\n",
      "Training loss: 0.1018902137875557\n",
      "Training loss: 0.109375961124897\n",
      "Training loss: 0.10132942348718643\n",
      "Training loss: 0.10010550171136856\n",
      "Training loss: 0.10347738862037659\n",
      "Training loss: 0.10781529545783997\n",
      "Training loss: 0.10814210027456284\n",
      "Training loss: 0.1033971756696701\n",
      "Training loss: 0.11097709834575653\n",
      "Training loss: 0.10604842007160187\n",
      "Training loss: 0.10209133476018906\n",
      "Training loss: 0.10685709118843079\n",
      "Training loss: 0.12832118570804596\n",
      "Training loss: 0.09963890165090561\n",
      "Training loss: 0.11822537332773209\n",
      "Training loss: 0.09899545460939407\n",
      "Training loss: 0.10099052637815475\n",
      "Training loss: 0.1041371300816536\n",
      "Training loss: 0.10591506212949753\n",
      "Training loss: 0.10551594942808151\n",
      "Training loss: 0.10558417439460754\n",
      "Training loss: 0.10684415698051453\n",
      "Training loss: 0.10188266634941101\n",
      "Training loss: 0.100584976375103\n",
      "Training loss: 0.10116668790578842\n",
      "Training loss: 0.10285362601280212\n",
      "Training loss: 0.10797750949859619\n",
      "Training loss: 0.10830902308225632\n",
      "Training loss: 0.09975580126047134\n",
      "Training loss: 0.10522770136594772\n",
      "Training loss: 0.10445195436477661\n",
      "Training loss: 0.10384207963943481\n",
      "Training loss: 0.09718053042888641\n",
      "Training loss: 0.10393617302179337\n",
      "Training loss: 0.10665781050920486\n",
      "Training loss: 0.09947417676448822\n",
      "Training loss: 0.10473781824111938\n",
      "Training loss: 0.09714806824922562\n",
      "Training loss: 0.10068276524543762\n",
      "Training loss: 0.10374251008033752\n",
      "Training loss: 0.10559810698032379\n",
      "Training loss: 0.10933724790811539\n",
      "Training loss: 0.10052158683538437\n",
      "Training loss: 0.10401361435651779\n",
      "Training loss: 0.10215950757265091\n",
      "Training loss: 0.10183341801166534\n",
      "Training loss: 0.10192815959453583\n",
      "Training loss: 0.1078028604388237\n",
      "Training loss: 0.10010407865047455\n",
      "Training loss: 0.10287395119667053\n",
      "Training loss: 0.09819374233484268\n",
      "Training loss: 0.10604816675186157\n",
      "Training loss: 0.1042180061340332\n",
      "Training loss: 0.10121282190084457\n",
      "Training loss: 0.10917218774557114\n",
      "Training loss: 0.1023898646235466\n",
      "Training loss: 0.10622994601726532\n",
      "Training loss: 0.10657405108213425\n",
      "Training loss: 0.11266425251960754\n",
      "Training loss: 0.10594706237316132\n",
      "Training loss: 0.10840179026126862\n",
      "Training loss: 0.10550139844417572\n",
      "Training loss: 0.10357698798179626\n",
      "Training loss: 0.1030697226524353\n",
      "Training loss: 0.10171588510274887\n",
      "Training loss: 0.10680627077817917\n",
      "Training loss: 0.09645398706197739\n",
      "Training loss: 0.10149865597486496\n",
      "Training loss: 0.10416995733976364\n",
      "Training loss: 0.09904434531927109\n",
      "Training loss: 0.10316213965415955\n",
      "Training loss: 0.10868804156780243\n",
      "Training loss: 0.104180708527565\n",
      "Training loss: 0.10353029519319534\n",
      "Training loss: 0.10208334773778915\n",
      "Training loss: 0.09375210106372833\n",
      "Training loss: 0.1055692657828331\n",
      "Training loss: 0.10069690644741058\n",
      "Training loss: 0.10289406776428223\n",
      "Training loss: 0.11441416293382645\n",
      "Training loss: 0.10651601105928421\n",
      "Training loss: 0.10787377506494522\n",
      "Training loss: 0.11060778051614761\n",
      "Training loss: 0.10214182734489441\n",
      "Training loss: 0.0940031036734581\n",
      "Training loss: 0.09947127103805542\n",
      "Training loss: 0.09637518227100372\n",
      "Training loss: 0.10394662618637085\n",
      "Training loss: 0.10690814256668091\n",
      "Training loss: 0.09711872041225433\n",
      "Training loss: 0.10936962068080902\n",
      "Training loss: 0.09767362475395203\n",
      "Training loss: 0.0974225178360939\n",
      "Training loss: 0.10635394603013992\n",
      "Training loss: 0.09757834672927856\n",
      "Training loss: 0.10703104734420776\n",
      "Training loss: 0.09670446068048477\n",
      "Training loss: 0.10487410426139832\n",
      "Training loss: 0.10644327849149704\n",
      "Training loss: 0.10772555321455002\n",
      "Training loss: 0.09939047694206238\n",
      "Training loss: 0.10617100447416306\n",
      "Training loss: 0.10305433720350266\n",
      "Training loss: 0.10151957720518112\n",
      "Training loss: 0.11109621077775955\n",
      "Training loss: 0.09828239679336548\n",
      "Training loss: 0.10397364944219589\n",
      "Training loss: 0.10089260339736938\n",
      "Training loss: 0.1071927547454834\n",
      "Training loss: 0.10317253321409225\n",
      "Training loss: 0.10652681440114975\n",
      "Training loss: 0.10213650017976761\n",
      "Training loss: 0.10312721878290176\n",
      "Training loss: 0.11262571811676025\n",
      "Training loss: 0.10059253126382828\n",
      "Training loss: 0.10196670144796371\n",
      "Training loss: 0.10274780541658401\n",
      "Training loss: 0.09686949104070663\n",
      "Training loss: 0.10169889777898788\n",
      "Training loss: 0.10337089747190475\n",
      "Training loss: 0.10500749200582504\n",
      "Training loss: 0.10341691970825195\n",
      "Training loss: 0.11031248420476913\n",
      "Training loss: 0.09555189311504364\n",
      "Training loss: 0.09838879853487015\n",
      "Training loss: 0.10283926874399185\n",
      "Training loss: 0.0975528359413147\n",
      "Training loss: 0.09923835843801498\n",
      "Training loss: 0.09767501056194305\n",
      "Training loss: 0.10294531285762787\n",
      "Training loss: 0.10515578091144562\n",
      "Training loss: 0.10600419342517853\n",
      "Training loss: 0.11095548421144485\n",
      "Training loss: 0.10115522891283035\n",
      "Training loss: 0.09683285653591156\n",
      "Training loss: 0.09892003238201141\n",
      "Training loss: 0.10020571947097778\n",
      "Training loss: 0.10061832517385483\n",
      "Training loss: 0.10935664176940918\n",
      "Training loss: 0.109368696808815\n",
      "Training loss: 0.10083023458719254\n",
      "Training loss: 0.10454889386892319\n",
      "Training loss: 0.10086330771446228\n",
      "Training loss: 0.10619743913412094\n",
      "Training loss: 0.10052713751792908\n",
      "Training loss: 0.0993533656001091\n",
      "Training loss: 0.10223733633756638\n",
      "Training loss: 0.09974265843629837\n",
      "Training loss: 0.10629749298095703\n",
      "Training loss: 0.10479340702295303\n",
      "Training loss: 0.09372959285974503\n",
      "Training loss: 0.09792079031467438\n",
      "Training loss: 0.09662358462810516\n",
      "Training loss: 0.09992482513189316\n",
      "Training loss: 0.10489467531442642\n",
      "Training loss: 0.10285364836454391\n",
      "Training loss: 0.10033585131168365\n",
      "Training loss: 0.10019394010305405\n",
      "Training loss: 0.09902247786521912\n",
      "Training loss: 0.11071030050516129\n",
      "Training loss: 0.10857465863227844\n",
      "Training loss: 0.09947554022073746\n",
      "Training loss: 0.09628832340240479\n",
      "Training loss: 0.10068492591381073\n",
      "Training loss: 0.10245922207832336\n",
      "Training loss: 0.10639815777540207\n",
      "Training loss: 0.09989787638187408\n",
      "Training loss: 0.10105226933956146\n",
      "Training loss: 0.104301817715168\n",
      "Training loss: 0.10608948022127151\n",
      "Training loss: 0.10367956757545471\n",
      "Training loss: 0.10293684899806976\n",
      "Training loss: 0.09900861978530884\n",
      "Training loss: 0.11176300048828125\n",
      "Training loss: 0.10311921685934067\n",
      "Training loss: 0.10289628803730011\n",
      "Training loss: 0.10610436648130417\n",
      "Training loss: 0.107109434902668\n",
      "Training loss: 0.1035296842455864\n",
      "Training loss: 0.10737396776676178\n",
      "Training loss: 0.10321245342493057\n",
      "Training loss: 0.11281196773052216\n",
      "Training loss: 0.10349185764789581\n",
      "Training loss: 0.1076308861374855\n",
      "Training loss: 0.12032971531152725\n",
      "Training loss: 0.10791492462158203\n",
      "Training loss: 0.10091504454612732\n",
      "Training loss: 0.11073122173547745\n",
      "Training loss: 0.10371023416519165\n",
      "Training loss: 0.10685508698225021\n",
      "Training loss: 0.10354044288396835\n",
      "Training loss: 0.10756325721740723\n",
      "Training loss: 0.1046544536948204\n",
      "Training loss: 0.10247945040464401\n",
      "Training loss: 0.10033678263425827\n",
      "Training loss: 0.10483750700950623\n",
      "Training loss: 0.10808627307415009\n",
      "Training loss: 0.1048477366566658\n",
      "Training loss: 0.09310219436883926\n",
      "Training loss: 0.10835834592580795\n",
      "Training loss: 0.11111265420913696\n",
      "Training loss: 0.10088536888360977\n",
      "Training loss: 0.108228400349617\n",
      "Training loss: 0.09806093573570251\n",
      "Training loss: 0.10419711470603943\n",
      "Training loss: 0.10540512949228287\n",
      "Training loss: 0.10496712476015091\n",
      "Training loss: 0.10844486206769943\n",
      "Training loss: 0.10215947777032852\n",
      "Training loss: 0.10407791286706924\n",
      "Training loss: 0.10471455752849579\n",
      "Training loss: 0.0977267473936081\n",
      "Training loss: 0.10350412130355835\n",
      "Training loss: 0.10585813969373703\n",
      "Training loss: 0.10152366012334824\n",
      "Training loss: 0.10069591552019119\n",
      "Training loss: 0.10547749698162079\n",
      "Training loss: 0.0995352640748024\n",
      "Training loss: 0.11267158389091492\n",
      "Training loss: 0.09825460612773895\n",
      "Training loss: 0.09907550364732742\n",
      "Training loss: 0.10421387106180191\n",
      "Training loss: 0.10149475187063217\n",
      "Training loss: 0.09983879327774048\n",
      "Training loss: 0.10105665773153305\n",
      "Training loss: 0.10629048198461533\n",
      "Training loss: 0.104219950735569\n",
      "Training loss: 0.10540134459733963\n",
      "Training loss: 0.10139977931976318\n",
      "Training loss: 0.09829413890838623\n",
      "Training loss: 0.1006602868437767\n",
      "Training loss: 0.10436096787452698\n",
      "Training loss: 0.10336975008249283\n",
      "Training loss: 0.10317965596914291\n",
      "Training loss: 0.10680270195007324\n",
      "Training loss: 0.09628752619028091\n",
      "Training loss: 0.1151510700583458\n",
      "Training loss: 0.10281599313020706\n",
      "Training loss: 0.09566167742013931\n",
      "Training loss: 0.10148527473211288\n",
      "Training loss: 0.09775938838720322\n",
      "Training loss: 0.10268089920282364\n",
      "Training loss: 0.09962359815835953\n",
      "Training loss: 0.09915807098150253\n",
      "Training loss: 0.10213900357484818\n",
      "Training loss: 0.11464386433362961\n",
      "Training loss: 0.10183654725551605\n",
      "Training loss: 0.1075214073061943\n",
      "Training loss: 0.10229647159576416\n",
      "Training loss: 0.1043405681848526\n",
      "Training loss: 0.10482744872570038\n",
      "Training loss: 0.10081364214420319\n",
      "Training loss: 0.10266973078250885\n",
      "Training loss: 0.10089913755655289\n",
      "Training loss: 0.10280518233776093\n",
      "Training loss: 0.1025720089673996\n",
      "Training loss: 0.1009567603468895\n",
      "Training loss: 0.09706489741802216\n",
      "Training loss: 0.09815666079521179\n",
      "Training loss: 0.09979717433452606\n",
      "Training loss: 0.094316765666008\n",
      "Training loss: 0.10420002043247223\n",
      "Training loss: 0.10456547141075134\n",
      "Training loss: 0.09788031131029129\n",
      "Training loss: 0.09796103090047836\n",
      "Training loss: 0.10936172306537628\n",
      "Training loss: 0.10235387831926346\n",
      "Training loss: 0.1019270122051239\n",
      "Training loss: 0.10783349722623825\n",
      "Training loss: 0.10066097974777222\n",
      "Training loss: 0.1037992388010025\n",
      "Training loss: 0.10247258841991425\n",
      "Training loss: 0.10010800510644913\n",
      "Training loss: 0.09831371158361435\n",
      "Training loss: 0.10236478596925735\n",
      "Training loss: 0.10537823289632797\n",
      "Training loss: 0.09374954551458359\n",
      "Training loss: 0.10428081452846527\n",
      "Training loss: 0.09897875785827637\n",
      "Training loss: 0.09373821318149567\n",
      "Training loss: 0.10834331810474396\n",
      "Training loss: 0.10457485169172287\n",
      "Training loss: 0.09965917468070984\n",
      "Training loss: 0.0990828424692154\n",
      "Training loss: 0.10122212022542953\n",
      "Training loss: 0.1094176322221756\n",
      "Training loss: 0.10615924000740051\n",
      "Training loss: 0.0985257551074028\n",
      "Training loss: 0.10477534681558609\n",
      "Training loss: 0.09754326939582825\n",
      "Training loss: 0.10764363408088684\n",
      "Training loss: 0.10185876488685608\n",
      "Training loss: 0.09899038821458817\n",
      "Training loss: 0.09700566530227661\n",
      "Training loss: 0.09996046125888824\n",
      "Training loss: 0.11255822330713272\n",
      "Training loss: 0.09803465008735657\n",
      "Training loss: 0.10689392685890198\n",
      "Training loss: 0.10247015953063965\n",
      "Training loss: 0.10123512148857117\n",
      "Training loss: 0.10392913967370987\n",
      "Training loss: 0.09951376914978027\n",
      "Training loss: 0.09771204739809036\n",
      "Training loss: 0.10385900735855103\n",
      "Training loss: 0.10289604961872101\n",
      "Training loss: 0.0967913344502449\n",
      "Training loss: 0.09866771101951599\n",
      "Training loss: 0.10300653427839279\n",
      "Training loss: 0.10149849951267242\n",
      "Training loss: 0.101832814514637\n",
      "Training loss: 0.10160073637962341\n",
      "Training loss: 0.10030202567577362\n",
      "Training loss: 0.09978295117616653\n",
      "Training loss: 0.10054074227809906\n",
      "Training loss: 0.10321690142154694\n",
      "Training loss: 0.10500191152095795\n",
      "Training loss: 0.1053706556558609\n",
      "Training loss: 0.0996285080909729\n",
      "Training loss: 0.10189136117696762\n",
      "Training loss: 0.10535262525081635\n",
      "Training loss: 0.10468249768018723\n",
      "Training loss: 0.10161761194467545\n",
      "Training loss: 0.10612034797668457\n",
      "Training loss: 0.09770525991916656\n",
      "Training loss: 0.10042636096477509\n",
      "Training loss: 0.10891613364219666\n",
      "Training loss: 0.10845237970352173\n",
      "Training loss: 0.11240284889936447\n",
      "Training loss: 0.11504017561674118\n",
      "Training loss: 0.10928955674171448\n",
      "Training loss: 0.10152499377727509\n",
      "Training loss: 0.09718901664018631\n",
      "Training loss: 0.10992767661809921\n",
      "Training loss: 0.10171393305063248\n",
      "Training loss: 0.09600602835416794\n",
      "Training loss: 0.1095581129193306\n",
      "Training loss: 0.10022252798080444\n",
      "Training loss: 0.10879293084144592\n",
      "Training loss: 0.10664103180170059\n",
      "Training loss: 0.10343023389577866\n",
      "Training loss: 0.1022142842411995\n",
      "Training loss: 0.1011396050453186\n",
      "Training loss: 0.10458948463201523\n",
      "Training loss: 0.09844076633453369\n",
      "Training loss: 0.09614298492670059\n",
      "Training loss: 0.10157271474599838\n",
      "Training loss: 0.10867058485746384\n",
      "Training loss: 0.09242377430200577\n",
      "Training loss: 0.10367999970912933\n",
      "Training loss: 0.10339301824569702\n",
      "Training loss: 0.10471824556589127\n",
      "Training loss: 0.09766051918268204\n",
      "Training loss: 0.10016277432441711\n",
      "Training loss: 0.11211029440164566\n",
      "Training loss: 0.10434640198945999\n",
      "Training loss: 0.10119357705116272\n",
      "Training loss: 0.09890320897102356\n",
      "Training loss: 0.0984206348657608\n",
      "Training loss: 0.10330647975206375\n",
      "Training loss: 0.10288264602422714\n",
      "Training loss: 0.10344172269105911\n",
      "Training loss: 0.09972723573446274\n",
      "Training loss: 0.09948567301034927\n",
      "Training loss: 0.10340964794158936\n",
      "Training loss: 0.10136497020721436\n",
      "Training loss: 0.10028425604104996\n",
      "Training loss: 0.09866183996200562\n",
      "Training loss: 0.10197733342647552\n",
      "Training loss: 0.10193399339914322\n",
      "Training loss: 0.10340844094753265\n",
      "Training loss: 0.09608997404575348\n",
      "Training loss: 0.10049424320459366\n",
      "Training loss: 0.10007298737764359\n",
      "Training loss: 0.10723531991243362\n",
      "Training loss: 0.10077245533466339\n",
      "Training loss: 0.0999031513929367\n",
      "Training loss: 0.09918169677257538\n",
      "Training loss: 0.10592979937791824\n",
      "Training loss: 0.09725134074687958\n",
      "Training loss: 0.09673623740673065\n",
      "Training loss: 0.09982955455780029\n",
      "Training loss: 0.10132157802581787\n",
      "Training loss: 0.09801464527845383\n",
      "Training loss: 0.10902233421802521\n",
      "Training loss: 0.09936952590942383\n",
      "Training loss: 0.10159505158662796\n",
      "Training loss: 0.10165350139141083\n",
      "Training loss: 0.1024371087551117\n",
      "Training loss: 0.09789910167455673\n",
      "Training loss: 0.10600613802671432\n",
      "Training loss: 0.10115793347358704\n",
      "Training loss: 0.09469279646873474\n",
      "Training loss: 0.1015489399433136\n",
      "Training loss: 0.10119295865297318\n",
      "Training loss: 0.1091943010687828\n",
      "Training loss: 0.10051515698432922\n",
      "Training loss: 0.09761783480644226\n",
      "Training loss: 0.09899226576089859\n",
      "Training loss: 0.0955684632062912\n",
      "Training loss: 0.09739964455366135\n",
      "Training loss: 0.10292874276638031\n",
      "Training loss: 0.09875643253326416\n",
      "Training loss: 0.10141479223966599\n",
      "Training loss: 0.09213864803314209\n",
      "Training loss: 0.09739578515291214\n",
      "Training loss: 0.10619702935218811\n",
      "Training loss: 0.10140249878168106\n",
      "Training loss: 0.10772933810949326\n",
      "Training loss: 0.09912525862455368\n",
      "Training loss: 0.09978921711444855\n",
      "Training loss: 0.09519414603710175\n",
      "Training loss: 0.1062706857919693\n",
      "Training loss: 0.0947108194231987\n",
      "Training loss: 0.095417320728302\n",
      "Training loss: 0.10305410623550415\n",
      "Training loss: 0.09668419510126114\n",
      "Training loss: 0.09560468047857285\n",
      "Training loss: 0.09676843881607056\n",
      "Training loss: 0.10061348974704742\n",
      "Training loss: 0.094744011759758\n",
      "Training loss: 0.10323027521371841\n",
      "Training loss: 0.10478653758764267\n",
      "Training loss: 0.0980185717344284\n",
      "Training loss: 0.09832792729139328\n",
      "Training loss: 0.09245430678129196\n",
      "Training loss: 0.09469188004732132\n",
      "Training loss: 0.0971500352025032\n",
      "Training loss: 0.09543899446725845\n",
      "Training loss: 0.10393193364143372\n",
      "Training loss: 0.09373625367879868\n",
      "Training loss: 0.09683658927679062\n",
      "Training loss: 0.09643632918596268\n",
      "Training loss: 0.10667356103658676\n",
      "Training loss: 0.10877390950918198\n",
      "Training loss: 0.10645605623722076\n",
      "Training loss: 0.10433875024318695\n",
      "Training loss: 0.10627495497465134\n",
      "Training loss: 0.10579539090394974\n",
      "Training loss: 0.10337754338979721\n",
      "Training loss: 0.10350719839334488\n",
      "Training loss: 0.10486932843923569\n",
      "Training loss: 0.09836068004369736\n",
      "Training loss: 0.09535843878984451\n",
      "Training loss: 0.10273382812738419\n",
      "Training loss: 0.10352076590061188\n",
      "Training loss: 0.0959606021642685\n",
      "Training loss: 0.09677489101886749\n",
      "Training loss: 0.10644488781690598\n",
      "Training loss: 0.09576047956943512\n",
      "Training loss: 0.0972241759300232\n",
      "Training loss: 0.108999103307724\n",
      "Training loss: 0.09767404198646545\n",
      "Training loss: 0.09648732841014862\n",
      "Training loss: 0.10414644330739975\n",
      "Training loss: 0.09624569863080978\n",
      "Training loss: 0.09915515035390854\n",
      "Training loss: 0.10237065702676773\n",
      "Training loss: 0.09463461488485336\n",
      "Training loss: 0.10089671611785889\n",
      "Training loss: 0.1033557802438736\n",
      "Training loss: 0.0945000872015953\n",
      "Training loss: 0.10014446824789047\n",
      "Training loss: 0.09646736830472946\n",
      "Training loss: 0.09926509857177734\n",
      "Training loss: 0.09596169739961624\n",
      "Training loss: 0.10026782006025314\n",
      "Training loss: 0.10130254924297333\n",
      "Training loss: 0.09802974760532379\n",
      "Training loss: 0.10619239509105682\n",
      "Training loss: 0.09538096189498901\n",
      "Training loss: 0.1027650311589241\n",
      "Training loss: 0.10091333091259003\n",
      "Training loss: 0.10327548533678055\n",
      "Training loss: 0.09606370329856873\n",
      "Training loss: 0.09911289811134338\n",
      "Training loss: 0.11033044010400772\n",
      "Training loss: 0.11411955207586288\n",
      "Training loss: 0.10417748242616653\n",
      "Training loss: 0.09774105995893478\n",
      "Training loss: 0.10711301118135452\n",
      "Training loss: 0.1070442646741867\n",
      "Training loss: 0.10582762956619263\n",
      "Training loss: 0.10593956708908081\n",
      "Training loss: 0.09934941679239273\n",
      "Training loss: 0.09515676647424698\n",
      "Training loss: 0.09707841277122498\n",
      "Training loss: 0.09756621718406677\n",
      "Training loss: 0.1060837134718895\n",
      "Training loss: 0.10048035532236099\n",
      "Training loss: 0.09816396981477737\n",
      "Training loss: 0.10155012458562851\n",
      "Training loss: 0.10122060030698776\n",
      "Training loss: 0.09793314337730408\n",
      "Training loss: 0.09476455301046371\n",
      "Training loss: 0.09737665951251984\n",
      "Training loss: 0.10260497033596039\n",
      "Training loss: 0.09986243396997452\n",
      "Training loss: 0.09348153322935104\n",
      "Training loss: 0.10040925443172455\n",
      "Training loss: 0.0925646647810936\n",
      "Training loss: 0.10394097864627838\n",
      "Training loss: 0.09972085058689117\n",
      "Training loss: 0.09160418808460236\n",
      "Training loss: 0.10066671669483185\n",
      "Training loss: 0.10528828948736191\n",
      "Training loss: 0.09529350697994232\n",
      "Training loss: 0.09770340472459793\n",
      "Training loss: 0.09851274639368057\n",
      "Training loss: 0.09981226921081543\n",
      "Training loss: 0.10415694117546082\n",
      "Training loss: 0.09700056910514832\n",
      "Training loss: 0.09953521937131882\n",
      "Training loss: 0.10634337365627289\n",
      "Training loss: 0.10490183532238007\n",
      "Training loss: 0.10205890983343124\n",
      "Training loss: 0.09933802485466003\n",
      "Training loss: 0.0971207320690155\n",
      "Training loss: 0.10135432332754135\n",
      "Training loss: 0.09592922776937485\n",
      "Training loss: 0.10657308250665665\n",
      "Training loss: 0.09389030933380127\n",
      "Training loss: 0.09881100803613663\n",
      "Training loss: 0.1016816645860672\n",
      "Training loss: 0.10867530107498169\n",
      "Training loss: 0.10256640613079071\n",
      "Training loss: 0.10763267427682877\n",
      "Training loss: 0.11042457073926926\n",
      "Training loss: 0.10063344985246658\n",
      "Training loss: 0.09790686517953873\n",
      "Training loss: 0.09246278554201126\n",
      "Training loss: 0.10095963627099991\n",
      "Training loss: 0.0976296216249466\n",
      "Training loss: 0.10341985523700714\n",
      "Training loss: 0.10397616028785706\n",
      "Training loss: 0.09897111356258392\n",
      "Training loss: 0.09809213876724243\n",
      "Training loss: 0.0953136757016182\n",
      "Training loss: 0.1047687977552414\n",
      "Training loss: 0.09857500344514847\n",
      "Training loss: 0.09685462713241577\n",
      "Training loss: 0.10203379392623901\n",
      "Training loss: 0.10233522951602936\n",
      "Training loss: 0.09916076809167862\n",
      "Training loss: 0.0969519093632698\n",
      "Training loss: 0.10137195885181427\n",
      "Training loss: 0.09531479328870773\n",
      "Training loss: 0.09982497990131378\n",
      "Training loss: 0.09824245423078537\n",
      "Training loss: 0.10242278128862381\n",
      "Training loss: 0.10009189695119858\n",
      "Training loss: 0.10357306152582169\n",
      "Training loss: 0.09577762335538864\n",
      "Training loss: 0.09780729562044144\n",
      "Training loss: 0.09901491552591324\n",
      "Training loss: 0.09667186439037323\n",
      "Training loss: 0.10238813608884811\n",
      "Training loss: 0.0985589474439621\n",
      "Training loss: 0.1000572070479393\n",
      "Training loss: 0.0978318601846695\n",
      "Training loss: 0.1006738692522049\n",
      "Training loss: 0.09253036975860596\n",
      "Training loss: 0.09635277092456818\n",
      "Training loss: 0.09281296283006668\n",
      "Training loss: 0.09671725332736969\n",
      "Training loss: 0.09846752882003784\n",
      "Training loss: 0.10139359533786774\n",
      "Training loss: 0.10586417466402054\n",
      "Training loss: 0.09336783736944199\n",
      "Training loss: 0.09970343112945557\n",
      "Training loss: 0.11622881144285202\n",
      "Training loss: 0.0953981876373291\n",
      "Training loss: 0.09661398828029633\n",
      "Training loss: 0.10096250474452972\n",
      "Training loss: 0.09994200617074966\n",
      "Training loss: 0.09370745718479156\n",
      "Training loss: 0.09592381119728088\n",
      "Training loss: 0.09672797471284866\n",
      "Training loss: 0.10218559950590134\n",
      "Training loss: 0.09650682657957077\n",
      "Training loss: 0.09714388847351074\n",
      "Training loss: 0.1087660938501358\n",
      "Training loss: 0.09873387962579727\n",
      "Training loss: 0.09241451323032379\n",
      "Training loss: 0.097684845328331\n",
      "Training loss: 0.10370718687772751\n",
      "Training loss: 0.09529878944158554\n",
      "Training loss: 0.09614120423793793\n",
      "Training loss: 0.10045367479324341\n",
      "Training loss: 0.09890221059322357\n",
      "Training loss: 0.10110495239496231\n",
      "Training loss: 0.10415871441364288\n",
      "Training loss: 0.10510309040546417\n",
      "Training loss: 0.1006496399641037\n",
      "Training loss: 0.09992299973964691\n",
      "Training loss: 0.10018502920866013\n",
      "Training loss: 0.09256134182214737\n",
      "Training loss: 0.09511477500200272\n",
      "Training loss: 0.09843941777944565\n",
      "Training loss: 0.09716103971004486\n",
      "Training loss: 0.10093307495117188\n",
      "Training loss: 0.09561827033758163\n",
      "Training loss: 0.09095831215381622\n",
      "Training loss: 0.09826096892356873\n",
      "Training loss: 0.09402850270271301\n",
      "Training loss: 0.09911293536424637\n",
      "Training loss: 0.09448795020580292\n",
      "Training loss: 0.09150021523237228\n",
      "Training loss: 0.09981507062911987\n",
      "Training loss: 0.10581266134977341\n",
      "Training loss: 0.1067061722278595\n",
      "Training loss: 0.10019417852163315\n",
      "Training loss: 0.09343935549259186\n",
      "Training loss: 0.09873852133750916\n",
      "Training loss: 0.09613464772701263\n",
      "Training loss: 0.09981606155633926\n",
      "Training loss: 0.09595295041799545\n",
      "Training loss: 0.09946504980325699\n",
      "Training loss: 0.10102996230125427\n",
      "Training loss: 0.10124745219945908\n",
      "Training loss: 0.10534673929214478\n",
      "Training loss: 0.09381704032421112\n",
      "Training loss: 0.09600763022899628\n",
      "Training loss: 0.10237231850624084\n",
      "Training loss: 0.10003454983234406\n",
      "Training loss: 0.10721676051616669\n",
      "Training loss: 0.09804093837738037\n",
      "Training loss: 0.09398486465215683\n",
      "Training loss: 0.09754960238933563\n",
      "Training loss: 0.09750677645206451\n",
      "Training loss: 0.0998372882604599\n",
      "Training loss: 0.10636619478464127\n",
      "Training loss: 0.09983054548501968\n",
      "Training loss: 0.10384601354598999\n",
      "Training loss: 0.10256090760231018\n",
      "Training loss: 0.09497957676649094\n",
      "Training loss: 0.09976646304130554\n",
      "Training loss: 0.10369857400655746\n",
      "Training loss: 0.1033962145447731\n",
      "Training loss: 0.10421230643987656\n",
      "Training loss: 0.1067098006606102\n",
      "Training loss: 0.1033373698592186\n",
      "Training loss: 0.10088420659303665\n",
      "Training loss: 0.10640965402126312\n",
      "Training loss: 0.10400305688381195\n",
      "Training loss: 0.10083810240030289\n",
      "Training loss: 0.10274142026901245\n",
      "Training loss: 0.10684049129486084\n",
      "Training loss: 0.10847532004117966\n",
      "Training loss: 0.10727860033512115\n",
      "Training loss: 0.09926995635032654\n",
      "Training loss: 0.10236749053001404\n",
      "Training loss: 0.09794950485229492\n",
      "Training loss: 0.103099025785923\n",
      "Training loss: 0.09962800145149231\n",
      "Training loss: 0.10133874416351318\n",
      "Training loss: 0.09879773110151291\n",
      "Training loss: 0.1036529615521431\n",
      "Training loss: 0.10207059979438782\n",
      "Training loss: 0.09334489703178406\n",
      "Training loss: 0.10623260587453842\n",
      "Training loss: 0.10213102400302887\n",
      "Training loss: 0.10222756862640381\n",
      "Training loss: 0.09795025736093521\n",
      "Training loss: 0.10272558778524399\n",
      "Training loss: 0.10627568513154984\n",
      "Training loss: 0.1097187027335167\n",
      "Training loss: 0.09674592316150665\n",
      "Training loss: 0.09830524772405624\n",
      "Training loss: 0.09632843732833862\n",
      "Training loss: 0.09051857888698578\n",
      "Training loss: 0.09544374793767929\n",
      "Training loss: 0.09654104709625244\n",
      "Training loss: 0.09348145127296448\n",
      "Training loss: 0.09843567758798599\n",
      "Training loss: 0.10060378909111023\n",
      "Training loss: 0.10063480585813522\n",
      "Training loss: 0.10022236406803131\n",
      "Training loss: 0.0943387970328331\n",
      "Training loss: 0.09568755328655243\n",
      "Training loss: 0.09505932033061981\n",
      "Training loss: 0.09897183626890182\n",
      "Training loss: 0.12632934749126434\n",
      "Training loss: 0.09892632812261581\n",
      "Training loss: 0.09724097698926926\n",
      "Training loss: 0.11554893851280212\n",
      "Training loss: 0.09763303399085999\n",
      "Training loss: 0.10939668118953705\n",
      "Training loss: 0.10327594727277756\n",
      "Training loss: 0.10485153645277023\n",
      "Training loss: 0.10307902097702026\n",
      "Training loss: 0.10505592823028564\n",
      "Training loss: 0.1031523272395134\n",
      "Training loss: 0.09937825053930283\n",
      "Training loss: 0.09946317225694656\n",
      "Training loss: 0.10099057108163834\n",
      "Training loss: 0.10675625503063202\n",
      "Training loss: 0.11504179239273071\n",
      "Training loss: 0.10328885167837143\n",
      "Training loss: 0.10518872737884521\n",
      "Training loss: 0.10114194452762604\n",
      "Training loss: 0.10537321865558624\n",
      "Training loss: 0.11328914761543274\n",
      "Training loss: 0.09889186173677444\n",
      "Training loss: 0.10753903537988663\n",
      "Training loss: 0.10292204469442368\n",
      "Training loss: 0.10430053621530533\n",
      "Training loss: 0.10592584311962128\n",
      "Training loss: 0.11271054297685623\n",
      "Training loss: 0.10474420338869095\n",
      "Training loss: 0.09315192699432373\n",
      "Training loss: 0.10454614460468292\n",
      "Training loss: 0.1074298843741417\n",
      "Training loss: 0.10433098673820496\n",
      "Training loss: 0.10422632843255997\n",
      "Training loss: 0.09735213965177536\n",
      "Training loss: 0.10610217601060867\n",
      "Training loss: 0.10794216394424438\n",
      "Training loss: 0.11255320906639099\n",
      "Training loss: 0.10529676824808121\n",
      "Training loss: 0.10621142387390137\n",
      "Training loss: 0.09723562002182007\n",
      "Training loss: 0.10901666432619095\n",
      "Training loss: 0.09943458437919617\n",
      "Training loss: 0.09907181560993195\n",
      "Training loss: 0.09443096071481705\n",
      "Training loss: 0.10079141706228256\n",
      "Training loss: 0.09998151659965515\n",
      "Training loss: 0.10101916640996933\n",
      "Training loss: 0.10034555941820145\n",
      "Training loss: 0.10169830173254013\n",
      "Training loss: 0.09797823429107666\n",
      "Training loss: 0.09889043122529984\n",
      "Training loss: 0.10099455714225769\n",
      "Training loss: 0.09675230830907822\n",
      "Training loss: 0.10289991647005081\n",
      "Training loss: 0.10558782517910004\n",
      "Training loss: 0.10018381476402283\n",
      "Training loss: 0.10016460716724396\n",
      "Training loss: 0.09286908060312271\n",
      "Training loss: 0.09367793798446655\n",
      "Training loss: 0.09371192753314972\n",
      "Training loss: 0.09800703823566437\n",
      "Training loss: 0.09716121852397919\n",
      "Training loss: 0.09457392990589142\n",
      "Training loss: 0.10242169350385666\n",
      "Training loss: 0.10222528874874115\n",
      "Training loss: 0.09204229712486267\n",
      "Training loss: 0.09783522039651871\n",
      "Training loss: 0.09644389897584915\n",
      "Training loss: 0.10380581766366959\n",
      "Training loss: 0.09861341863870621\n",
      "Training loss: 0.10623400658369064\n",
      "Training loss: 0.09634017199277878\n",
      "Training loss: 0.10240082442760468\n",
      "Training loss: 0.10448405891656876\n",
      "Training loss: 0.09443912655115128\n",
      "Training loss: 0.10157063603401184\n",
      "Training loss: 0.11864466965198517\n",
      "Training loss: 0.10199009627103806\n",
      "Training loss: 0.09823912382125854\n",
      "Training loss: 0.10314104706048965\n",
      "Training loss: 0.09775383025407791\n",
      "Training loss: 0.10086344927549362\n",
      "Training loss: 0.11422262340784073\n",
      "Training loss: 0.10301942378282547\n",
      "Training loss: 0.10580062866210938\n",
      "Training loss: 0.10392377525568008\n",
      "Training loss: 0.10659731924533844\n",
      "Training loss: 0.10714162141084671\n",
      "Training loss: 0.09888383746147156\n",
      "Training loss: 0.10619746893644333\n",
      "Training loss: 0.10211608558893204\n",
      "Training loss: 0.09636605530977249\n",
      "Training loss: 0.1101984903216362\n",
      "Training loss: 0.10257256031036377\n",
      "Training loss: 0.10273056477308273\n",
      "Training loss: 0.09889520704746246\n",
      "Training loss: 0.09594374895095825\n",
      "Training loss: 0.10152573138475418\n",
      "Training loss: 0.10576539486646652\n",
      "Training loss: 0.10414859652519226\n",
      "Training loss: 0.1027497723698616\n",
      "Training loss: 0.10557291656732559\n",
      "Training loss: 0.10429766029119492\n",
      "Training loss: 0.09726633131504059\n",
      "Training loss: 0.1017415001988411\n",
      "Training loss: 0.09269241243600845\n",
      "Training loss: 0.10232309252023697\n",
      "Training loss: 0.10026311129331589\n",
      "Training loss: 0.10102544724941254\n",
      "Training loss: 0.10076846182346344\n",
      "Training loss: 0.10196676850318909\n",
      "Training loss: 0.09215542674064636\n",
      "Training loss: 0.09827274084091187\n",
      "Training loss: 0.10252853482961655\n",
      "Training loss: 0.09633959084749222\n",
      "Training loss: 0.09381792694330215\n",
      "Training loss: 0.10137288272380829\n",
      "Training loss: 0.09587876498699188\n",
      "Training loss: 0.09573734551668167\n",
      "Training loss: 0.09519123286008835\n",
      "Training loss: 0.09095193445682526\n",
      "Training loss: 0.0927618071436882\n",
      "Training loss: 0.09583725035190582\n",
      "Training loss: 0.10343525558710098\n",
      "Training loss: 0.1010780930519104\n",
      "Training loss: 0.09356030821800232\n",
      "Training loss: 0.10149107873439789\n",
      "Training loss: 0.09485267847776413\n",
      "Training loss: 0.09380274266004562\n",
      "Training loss: 0.10725583881139755\n",
      "Training loss: 0.09877166152000427\n",
      "Training loss: 0.10111811012029648\n",
      "Training loss: 0.10700346529483795\n",
      "Training loss: 0.09671333432197571\n",
      "Training loss: 0.09725850075483322\n",
      "Training loss: 0.10263200104236603\n",
      "Training loss: 0.0957569032907486\n",
      "Training loss: 0.10208690911531448\n",
      "Training loss: 0.10082855075597763\n",
      "Training loss: 0.09457723051309586\n",
      "Training loss: 0.10275015980005264\n",
      "Training loss: 0.09516642242670059\n",
      "Training loss: 0.10274244099855423\n",
      "Training loss: 0.09163766354322433\n",
      "Training loss: 0.0953298807144165\n",
      "Training loss: 0.10040941834449768\n",
      "Training loss: 0.1036476194858551\n",
      "Training loss: 0.09918346256017685\n",
      "Training loss: 0.10322488099336624\n",
      "Training loss: 0.0979408323764801\n",
      "Training loss: 0.09885942190885544\n",
      "Training loss: 0.0936155691742897\n",
      "Training loss: 0.09232357889413834\n",
      "Training loss: 0.1000022441148758\n",
      "Training loss: 0.10156543552875519\n",
      "Training loss: 0.10163553059101105\n",
      "Training loss: 0.10213009268045425\n",
      "Training loss: 0.09489893913269043\n",
      "Training loss: 0.10171778500080109\n",
      "Training loss: 0.09513480961322784\n",
      "Training loss: 0.09579851478338242\n",
      "Training loss: 0.10079510509967804\n",
      "Training loss: 0.09897448122501373\n",
      "Training loss: 0.09464798122644424\n",
      "Training loss: 0.09197050333023071\n",
      "Training loss: 0.09564803540706635\n",
      "Training loss: 0.09389760345220566\n",
      "Training loss: 0.09260382503271103\n",
      "Training loss: 0.09099717438220978\n",
      "Training loss: 0.09823621064424515\n",
      "Training loss: 0.09445470571517944\n",
      "Training loss: 0.09847322851419449\n",
      "Training loss: 0.09764771163463593\n",
      "Training loss: 0.0901990458369255\n",
      "Training loss: 0.09219798445701599\n",
      "Training loss: 0.09819699823856354\n",
      "Training loss: 0.10123168677091599\n",
      "Training loss: 0.0949271097779274\n",
      "Training loss: 0.09951408207416534\n",
      "Training loss: 0.10346497595310211\n",
      "Training loss: 0.09362643212080002\n",
      "Training loss: 0.10888725519180298\n",
      "Training loss: 0.09575235098600388\n",
      "Training loss: 0.09478949010372162\n",
      "Training loss: 0.10709167271852493\n",
      "Training loss: 0.0983259379863739\n",
      "Training loss: 0.09641057252883911\n",
      "Training loss: 0.10124745965003967\n",
      "Training loss: 0.10551628470420837\n",
      "Training loss: 0.10639340430498123\n",
      "Training loss: 0.10571575164794922\n",
      "Training loss: 0.10454954206943512\n",
      "Training loss: 0.10395735502243042\n",
      "Training loss: 0.10366933047771454\n",
      "Training loss: 0.09563526511192322\n",
      "Training loss: 0.09611938893795013\n",
      "Training loss: 0.10663919150829315\n",
      "Training loss: 0.10302627831697464\n",
      "Training loss: 0.09940848499536514\n",
      "Training loss: 0.09676899760961533\n",
      "Training loss: 0.09546906501054764\n",
      "Training loss: 0.09418292343616486\n",
      "Training loss: 0.09554313868284225\n",
      "Training loss: 0.09444575011730194\n",
      "Training loss: 0.09897950291633606\n",
      "Training loss: 0.09300271421670914\n",
      "Training loss: 0.10004810988903046\n",
      "Training loss: 0.09531407803297043\n",
      "Training loss: 0.0982854813337326\n",
      "Training loss: 0.0945436954498291\n",
      "Training loss: 0.09563691169023514\n",
      "Training loss: 0.09365633130073547\n",
      "Training loss: 0.09517180919647217\n",
      "Training loss: 0.1026100292801857\n",
      "Training loss: 0.10481249541044235\n",
      "Training loss: 0.09551199525594711\n",
      "Training loss: 0.09719125181436539\n",
      "Training loss: 0.09850428253412247\n",
      "Training loss: 0.09871219843626022\n",
      "Training loss: 0.10204584151506424\n",
      "Training loss: 0.10260938107967377\n",
      "Training loss: 0.0993107408285141\n",
      "Training loss: 0.09030116349458694\n",
      "Training loss: 0.10077036917209625\n",
      "Training loss: 0.09605434536933899\n",
      "Training loss: 0.09864877164363861\n",
      "Training loss: 0.09817379713058472\n",
      "Training loss: 0.09997916966676712\n",
      "Training loss: 0.10781577229499817\n",
      "Training loss: 0.10126592218875885\n",
      "Training loss: 0.10099834948778152\n",
      "Training loss: 0.09505222737789154\n",
      "Training loss: 0.10001060366630554\n",
      "Training loss: 0.10228048264980316\n",
      "Training loss: 0.1000543162226677\n",
      "Training loss: 0.10633347928524017\n",
      "Training loss: 0.09992245584726334\n",
      "Training loss: 0.09641715884208679\n",
      "Training loss: 0.10086532682180405\n",
      "Training loss: 0.1065601035952568\n",
      "Training loss: 0.11161249130964279\n",
      "Training loss: 0.11303761601448059\n",
      "Training loss: 0.09511061757802963\n",
      "Training loss: 0.0936380922794342\n",
      "Training loss: 0.10626815259456635\n",
      "Training loss: 0.09729478508234024\n",
      "Training loss: 0.09570227563381195\n",
      "Training loss: 0.09459391981363297\n",
      "Training loss: 0.10484572499990463\n",
      "Training loss: 0.10495985299348831\n",
      "Training loss: 0.09892061352729797\n",
      "Training loss: 0.10661518573760986\n",
      "Training loss: 0.10085631906986237\n",
      "Training loss: 0.09928825497627258\n",
      "Training loss: 0.10455559194087982\n",
      "Training loss: 0.10733552277088165\n",
      "Training loss: 0.10188876837491989\n",
      "Training loss: 0.10547291487455368\n",
      "Training loss: 0.10306493192911148\n",
      "Training loss: 0.10069509595632553\n",
      "Training loss: 0.09261893481016159\n",
      "Training loss: 0.09733892977237701\n",
      "Training loss: 0.09984591603279114\n",
      "Training loss: 0.09699955582618713\n",
      "Training loss: 0.1064944863319397\n",
      "Training loss: 0.10217169672250748\n",
      "Training loss: 0.10240837186574936\n",
      "Training loss: 0.10763353109359741\n",
      "Training loss: 0.10599002987146378\n",
      "Training loss: 0.10446912050247192\n",
      "Training loss: 0.11558972299098969\n",
      "Training loss: 0.10786568373441696\n",
      "Training loss: 0.10198363661766052\n",
      "Training loss: 0.10319601744413376\n",
      "Training loss: 0.10616929084062576\n",
      "Training loss: 0.10251118987798691\n",
      "Training loss: 0.10401201248168945\n",
      "Training loss: 0.10536853224039078\n",
      "Training loss: 0.09890148788690567\n",
      "Training loss: 0.10133775323629379\n",
      "Training loss: 0.09751801192760468\n",
      "Training loss: 0.11376100033521652\n",
      "Training loss: 0.10310433804988861\n",
      "Training loss: 0.10197564214468002\n",
      "Training loss: 0.10341228544712067\n",
      "Training loss: 0.107694111764431\n",
      "Training loss: 0.09936939179897308\n",
      "Training loss: 0.09959493577480316\n",
      "Training loss: 0.1073787584900856\n",
      "Training loss: 0.10769397020339966\n",
      "Training loss: 0.1016666516661644\n",
      "Training loss: 0.11067301779985428\n",
      "Training loss: 0.09910206496715546\n",
      "Training loss: 0.10669344663619995\n",
      "Training loss: 0.10521434247493744\n",
      "Training loss: 0.09798353910446167\n",
      "Training loss: 0.10277099162340164\n",
      "Training loss: 0.09726990759372711\n",
      "Training loss: 0.10380581021308899\n",
      "Training loss: 0.09917349368333817\n",
      "Training loss: 0.09955047070980072\n",
      "Training loss: 0.09982488304376602\n",
      "Training loss: 0.09967629611492157\n",
      "Training loss: 0.10661760717630386\n",
      "Training loss: 0.09978640079498291\n",
      "Training loss: 0.09879807382822037\n",
      "Training loss: 0.10731994360685349\n",
      "Training loss: 0.11171316355466843\n",
      "Training loss: 0.10541985183954239\n",
      "Training loss: 0.09859578311443329\n",
      "Training loss: 0.10236428678035736\n",
      "Training loss: 0.09942981600761414\n",
      "Training loss: 0.09936625510454178\n",
      "Training loss: 0.10647980868816376\n",
      "Training loss: 0.10225541144609451\n",
      "Training loss: 0.10015788674354553\n",
      "Training loss: 0.09225407242774963\n",
      "Training loss: 0.10085327923297882\n",
      "Training loss: 0.10294797271490097\n",
      "Training loss: 0.10401371866464615\n",
      "Training loss: 0.102519690990448\n",
      "Training loss: 0.10169344395399094\n",
      "Training loss: 0.10617455095052719\n",
      "Training loss: 0.09680548310279846\n",
      "Training loss: 0.10236664861440659\n",
      "Training loss: 0.10154478996992111\n",
      "Training loss: 0.10185732692480087\n",
      "Training loss: 0.10730066150426865\n",
      "Training loss: 0.10235574841499329\n",
      "Training loss: 0.09617665410041809\n",
      "Training loss: 0.0994381308555603\n",
      "Training loss: 0.101430244743824\n",
      "Training loss: 0.0989522635936737\n",
      "Training loss: 0.09707481414079666\n",
      "Training loss: 0.10096342861652374\n",
      "Training loss: 0.1214337944984436\n",
      "Training loss: 0.09630517661571503\n",
      "Training loss: 0.09285473823547363\n",
      "Training loss: 0.09745702892541885\n",
      "Training loss: 0.10777970403432846\n",
      "Training loss: 0.09931350499391556\n",
      "Training loss: 0.10247204452753067\n",
      "Training loss: 0.10033275932073593\n",
      "Training loss: 0.10215234011411667\n",
      "Training loss: 0.10255037993192673\n",
      "Training loss: 0.10162418335676193\n",
      "Training loss: 0.09936773777008057\n",
      "Training loss: 0.10791594535112381\n",
      "Training loss: 0.10772779583930969\n",
      "Training loss: 0.10628186166286469\n",
      "Training loss: 0.09792347252368927\n",
      "Training loss: 0.09862067550420761\n",
      "Training loss: 0.10366372764110565\n",
      "Training loss: 0.10039069503545761\n",
      "Training loss: 0.09954208880662918\n",
      "Training loss: 0.10184863209724426\n",
      "Training loss: 0.0977606251835823\n",
      "Training loss: 0.095454640686512\n",
      "Training loss: 0.10003920644521713\n",
      "Training loss: 0.09776239097118378\n",
      "Training loss: 0.09564652293920517\n",
      "Training loss: 0.10206688940525055\n",
      "Training loss: 0.10409345477819443\n",
      "Training loss: 0.09445037692785263\n",
      "Training loss: 0.09501201659440994\n",
      "Training loss: 0.09837404638528824\n",
      "Training loss: 0.09672818332910538\n",
      "Training loss: 0.10243114829063416\n",
      "Training loss: 0.09613088518381119\n",
      "Training loss: 0.0967811569571495\n",
      "Training loss: 0.10463134944438934\n",
      "Training loss: 0.08956271409988403\n",
      "Training loss: 0.09075183421373367\n",
      "Training loss: 0.10045818239450455\n",
      "Training loss: 0.09098158776760101\n",
      "Training loss: 0.10127843171358109\n",
      "Training loss: 0.09282500296831131\n",
      "Training loss: 0.11119135469198227\n",
      "Training loss: 0.09148642420768738\n",
      "Training loss: 0.09361272305250168\n",
      "Training loss: 0.09364780783653259\n",
      "Training loss: 0.09443621337413788\n",
      "Training loss: 0.09885254502296448\n",
      "Training loss: 0.09661094844341278\n",
      "Training loss: 0.09798010438680649\n",
      "Training loss: 0.10504104942083359\n",
      "Training loss: 0.0966954305768013\n",
      "Training loss: 0.09014637023210526\n",
      "Training loss: 0.09120917320251465\n",
      "Training loss: 0.09881184250116348\n",
      "Training loss: 0.09367375820875168\n",
      "Training loss: 0.10042048990726471\n",
      "Training loss: 0.09472773224115372\n",
      "Training loss: 0.1007956862449646\n",
      "Training loss: 0.09687578678131104\n",
      "Training loss: 0.10470423847436905\n",
      "Training loss: 0.09845923632383347\n",
      "Training loss: 0.10354569554328918\n",
      "Training loss: 0.10074687004089355\n",
      "Training loss: 0.10172458738088608\n",
      "Training loss: 0.09492586553096771\n",
      "Training loss: 0.09747202694416046\n",
      "Training loss: 0.09377061575651169\n",
      "Training loss: 0.1023624837398529\n",
      "Training loss: 0.09772085398435593\n",
      "Training loss: 0.1013377234339714\n",
      "Training loss: 0.09800253063440323\n",
      "Training loss: 0.10757936537265778\n",
      "Training loss: 0.09725474566221237\n",
      "Training loss: 0.09564489871263504\n",
      "Training loss: 0.10223057866096497\n",
      "Training loss: 0.10677777230739594\n",
      "Training loss: 0.09589241445064545\n",
      "Training loss: 0.09890434890985489\n",
      "Training loss: 0.10046040266752243\n",
      "Training loss: 0.09708279371261597\n",
      "Training loss: 0.10735303163528442\n",
      "Training loss: 0.09884510189294815\n",
      "Training loss: 0.0955805554986\n",
      "Training loss: 0.10230936110019684\n",
      "Training loss: 0.09478586912155151\n",
      "Training loss: 0.10119014978408813\n",
      "Training loss: 0.09626682102680206\n",
      "Training loss: 0.09239418059587479\n",
      "Training loss: 0.09775549173355103\n",
      "Training loss: 0.09900309890508652\n",
      "Training loss: 0.10505491495132446\n",
      "Training loss: 0.09707602113485336\n",
      "Training loss: 0.10976818948984146\n",
      "Training loss: 0.09948412328958511\n",
      "Training loss: 0.0946359634399414\n",
      "Training loss: 0.09464112669229507\n",
      "Training loss: 0.08717885613441467\n",
      "Training loss: 0.09960287064313889\n",
      "Training loss: 0.10905449092388153\n",
      "Training loss: 0.10181151330471039\n",
      "Training loss: 0.10002106428146362\n",
      "Training loss: 0.09708857536315918\n",
      "Training loss: 0.09612394124269485\n",
      "Training loss: 0.0889982208609581\n",
      "Training loss: 0.09352997690439224\n",
      "Training loss: 0.09856332838535309\n",
      "Training loss: 0.09900553524494171\n",
      "Training loss: 0.10007280111312866\n",
      "Training loss: 0.08911584317684174\n",
      "Training loss: 0.09791670739650726\n",
      "Training loss: 0.08949965238571167\n",
      "Training loss: 0.09224259853363037\n",
      "Training loss: 0.09366350620985031\n",
      "Training loss: 0.09615958482027054\n",
      "Training loss: 0.09380961209535599\n",
      "Training loss: 0.09561251103878021\n",
      "Training loss: 0.09327851980924606\n",
      "Training loss: 0.09351479262113571\n",
      "Training loss: 0.09053861349821091\n",
      "Training loss: 0.09987591952085495\n",
      "Training loss: 0.08963543176651001\n",
      "Training loss: 0.09782138466835022\n",
      "Training loss: 0.08750497549772263\n",
      "Training loss: 0.09230911731719971\n",
      "Training loss: 0.10048749297857285\n",
      "Training loss: 0.09911289811134338\n",
      "Training loss: 0.10257526487112045\n",
      "Training loss: 0.09954310208559036\n",
      "Training loss: 0.08930811285972595\n",
      "Training loss: 0.09360045939683914\n",
      "Training loss: 0.09667814522981644\n",
      "Training loss: 0.09707978367805481\n",
      "Training loss: 0.08981617540121078\n",
      "Training loss: 0.09794909507036209\n",
      "Training loss: 0.0936778113245964\n",
      "Training loss: 0.09894191473722458\n",
      "Training loss: 0.09844132512807846\n",
      "Training loss: 0.09133175015449524\n",
      "Training loss: 0.09103940427303314\n",
      "Training loss: 0.0866679698228836\n",
      "Training loss: 0.09544762223958969\n",
      "Training loss: 0.09425830841064453\n",
      "Training loss: 0.09487888962030411\n",
      "Training loss: 0.09759446978569031\n",
      "Training loss: 0.09708290547132492\n",
      "Training loss: 0.1064402163028717\n",
      "Training loss: 0.0963798388838768\n",
      "Training loss: 0.09651079773902893\n",
      "Training loss: 0.09641018509864807\n",
      "Training loss: 0.1018102616071701\n",
      "Training loss: 0.09040607511997223\n",
      "Training loss: 0.09623808413743973\n",
      "Training loss: 0.09378360956907272\n",
      "Training loss: 0.10905183851718903\n",
      "Training loss: 0.0915306955575943\n",
      "Training loss: 0.09982864558696747\n",
      "Training loss: 0.09290455281734467\n",
      "Training loss: 0.09929462522268295\n",
      "Training loss: 0.09833844751119614\n",
      "Training loss: 0.09385489672422409\n",
      "Training loss: 0.09441988915205002\n",
      "Training loss: 0.09745985269546509\n",
      "Training loss: 0.09835461527109146\n",
      "Training loss: 0.10596746951341629\n",
      "Training loss: 0.10202763229608536\n",
      "Training loss: 0.09999478608369827\n",
      "Training loss: 0.09987087547779083\n",
      "Training loss: 0.10697902739048004\n",
      "Training loss: 0.09317845851182938\n",
      "Training loss: 0.09314607083797455\n",
      "Training loss: 0.09512147307395935\n",
      "Training loss: 0.10246146470308304\n",
      "Training loss: 0.09922516345977783\n",
      "Training loss: 0.09342579543590546\n",
      "Training loss: 0.09632977098226547\n",
      "Training loss: 0.10771702975034714\n",
      "Training loss: 0.09489921480417252\n",
      "Training loss: 0.09704123437404633\n",
      "Training loss: 0.09887780249118805\n",
      "Training loss: 0.12080788612365723\n",
      "Training loss: 0.09950590133666992\n",
      "Training loss: 0.09810999035835266\n",
      "Training loss: 0.10662303864955902\n",
      "Training loss: 0.10302576422691345\n",
      "Training loss: 0.09819063544273376\n",
      "Training loss: 0.10024568438529968\n",
      "Training loss: 0.10232166945934296\n",
      "Training loss: 0.09986874461174011\n",
      "Training loss: 0.09862811118364334\n",
      "Training loss: 0.09170595556497574\n",
      "Training loss: 0.09231880307197571\n",
      "Training loss: 0.09971978515386581\n",
      "Training loss: 0.0997573658823967\n",
      "Training loss: 0.100506491959095\n",
      "Training loss: 0.09877198189496994\n",
      "Training loss: 0.09676606208086014\n",
      "Training loss: 0.10480134189128876\n",
      "Training loss: 0.09946515411138535\n",
      "Training loss: 0.0945792868733406\n",
      "Training loss: 0.09449048340320587\n",
      "Training loss: 0.10008787363767624\n",
      "Training loss: 0.10022114217281342\n",
      "Training loss: 0.09679611027240753\n",
      "Training loss: 0.090200275182724\n",
      "Training loss: 0.09787974506616592\n",
      "Training loss: 0.09346206486225128\n",
      "Training loss: 0.09998303651809692\n",
      "Training loss: 0.09733765572309494\n",
      "Training loss: 0.10260535031557083\n",
      "Training loss: 0.08931327611207962\n",
      "Training loss: 0.10270971804857254\n",
      "Training loss: 0.09747587144374847\n",
      "Training loss: 0.10532829910516739\n",
      "Training loss: 0.08984674513339996\n",
      "Training loss: 0.09355733543634415\n",
      "Training loss: 0.09739013016223907\n",
      "Training loss: 0.10362200438976288\n",
      "Training loss: 0.09572941064834595\n",
      "Training loss: 0.09612973779439926\n",
      "Training loss: 0.09921811521053314\n",
      "Training loss: 0.09477543085813522\n",
      "Training loss: 0.09710383415222168\n",
      "Training loss: 0.09668953716754913\n",
      "Training loss: 0.09115629643201828\n",
      "Training loss: 0.09620704501867294\n",
      "Training loss: 0.09262658655643463\n",
      "Training loss: 0.09431812912225723\n",
      "Training loss: 0.10051945596933365\n",
      "Training loss: 0.09236149489879608\n",
      "Training loss: 0.09207621961832047\n",
      "Training loss: 0.0895276591181755\n",
      "Training loss: 0.09247813373804092\n",
      "Training loss: 0.09815949201583862\n",
      "Training loss: 0.0977814719080925\n",
      "Training loss: 0.09529319405555725\n",
      "Training loss: 0.09574776142835617\n",
      "Training loss: 0.09035094827413559\n",
      "Training loss: 0.10375101864337921\n",
      "Training loss: 0.09211088716983795\n",
      "Training loss: 0.097219318151474\n",
      "Training loss: 0.0981070026755333\n",
      "Training loss: 0.08874493837356567\n",
      "Training loss: 0.09522520005702972\n",
      "Training loss: 0.09170063585042953\n",
      "Training loss: 0.0901038646697998\n",
      "Training loss: 0.09334839880466461\n",
      "Training loss: 0.09671865403652191\n",
      "Training loss: 0.09185252338647842\n",
      "Training loss: 0.10658056288957596\n",
      "Training loss: 0.09790868312120438\n",
      "Training loss: 0.10766705870628357\n",
      "Training loss: 0.09929097443819046\n",
      "Training loss: 0.1004800945520401\n",
      "Training loss: 0.10616104304790497\n",
      "Training loss: 0.0966513380408287\n",
      "Training loss: 0.09205136448144913\n",
      "Training loss: 0.09321186691522598\n",
      "Training loss: 0.10090094804763794\n",
      "Training loss: 0.09321028739213943\n",
      "Training loss: 0.09804581105709076\n",
      "Training loss: 0.09409870952367783\n",
      "Training loss: 0.09799185395240784\n",
      "Training loss: 0.09966140240430832\n",
      "Training loss: 0.0954638347029686\n",
      "Training loss: 0.0947604775428772\n",
      "Training loss: 0.09560572355985641\n",
      "Training loss: 0.10643932968378067\n",
      "Training loss: 0.09305652230978012\n",
      "Training loss: 0.09407022595405579\n",
      "Training loss: 0.09558457881212234\n",
      "Training loss: 0.08705455809831619\n",
      "Training loss: 0.08975233137607574\n",
      "Training loss: 0.09430339187383652\n",
      "Training loss: 0.09603973478078842\n",
      "Training loss: 0.10574124753475189\n",
      "Training loss: 0.10199102014303207\n",
      "Training loss: 0.09899972379207611\n",
      "Training loss: 0.10088391602039337\n",
      "Training loss: 0.08818437904119492\n",
      "Training loss: 0.08980463445186615\n",
      "Training loss: 0.08831106126308441\n",
      "Training loss: 0.10809201747179031\n",
      "Training loss: 0.1003287211060524\n",
      "Training loss: 0.09499164670705795\n",
      "Training loss: 0.08625253289937973\n",
      "Training loss: 0.09095077961683273\n",
      "Training loss: 0.09820275753736496\n",
      "Training loss: 0.09688854217529297\n",
      "Training loss: 0.08893629908561707\n",
      "Training loss: 0.09035499393939972\n",
      "Training loss: 0.09663361310958862\n",
      "Training loss: 0.09904752671718597\n",
      "Training loss: 0.09425657987594604\n",
      "Training loss: 0.09578438848257065\n",
      "Training loss: 0.08854146301746368\n",
      "Training loss: 0.10221482068300247\n",
      "Training loss: 0.09542940557003021\n",
      "Training loss: 0.09030357003211975\n",
      "Training loss: 0.09484393894672394\n",
      "Training loss: 0.09271111339330673\n",
      "Training loss: 0.09994598478078842\n",
      "Training loss: 0.09062618762254715\n",
      "Training loss: 0.08611558377742767\n",
      "Training loss: 0.08742937445640564\n",
      "Training loss: 0.10078594833612442\n",
      "Training loss: 0.08899541199207306\n",
      "Training loss: 0.08739209175109863\n",
      "Training loss: 0.08654599636793137\n",
      "Training loss: 0.09191504865884781\n",
      "Training loss: 0.09648266434669495\n",
      "Training loss: 0.09672341495752335\n",
      "Training loss: 0.09796153008937836\n",
      "Training loss: 0.08693736791610718\n",
      "Training loss: 0.09596860408782959\n",
      "Training loss: 0.09569881111383438\n",
      "Training loss: 0.09828061610460281\n",
      "Training loss: 0.10006241500377655\n",
      "Training loss: 0.09061768651008606\n",
      "Training loss: 0.09650696814060211\n",
      "Training loss: 0.09249978512525558\n",
      "Training loss: 0.0904153361916542\n",
      "Training loss: 0.09081690013408661\n",
      "Training loss: 0.10100318491458893\n",
      "Training loss: 0.09212084859609604\n",
      "Training loss: 0.09928179532289505\n",
      "Training loss: 0.08988681435585022\n",
      "Training loss: 0.09113602340221405\n",
      "Training loss: 0.0910259559750557\n",
      "Training loss: 0.08733680844306946\n",
      "Training loss: 0.09006614238023758\n",
      "Training loss: 0.09056875854730606\n",
      "Training loss: 0.08983878791332245\n",
      "Training loss: 0.09378043562173843\n",
      "Training loss: 0.08845395594835281\n",
      "Training loss: 0.0865386351943016\n",
      "Training loss: 0.08881073445081711\n",
      "Training loss: 0.08901651948690414\n",
      "Training loss: 0.08559691905975342\n",
      "Training loss: 0.09281044453382492\n",
      "Training loss: 0.0865466445684433\n",
      "Training loss: 0.09016717970371246\n",
      "Training loss: 0.09114927798509598\n",
      "Training loss: 0.09246010333299637\n",
      "Training loss: 0.09498391300439835\n",
      "Training loss: 0.09389972686767578\n",
      "Training loss: 0.10009779781103134\n",
      "Training loss: 0.10009434074163437\n",
      "Training loss: 0.09058628976345062\n",
      "Training loss: 0.10091384500265121\n",
      "Training loss: 0.09611322730779648\n",
      "Training loss: 0.09272892028093338\n",
      "Training loss: 0.09335906058549881\n",
      "Training loss: 0.09047917276620865\n",
      "Training loss: 0.09072857350111008\n",
      "Training loss: 0.08982284367084503\n",
      "Training loss: 0.08773034811019897\n",
      "Training loss: 0.09548568725585938\n",
      "Training loss: 0.0935898870229721\n",
      "Training loss: 0.09103241562843323\n",
      "Training loss: 0.09152664989233017\n",
      "Training loss: 0.09247028082609177\n",
      "Training loss: 0.09111660718917847\n",
      "Training loss: 0.09397517889738083\n",
      "Training loss: 0.10036106407642365\n",
      "Training loss: 0.09171372652053833\n",
      "Training loss: 0.09665410220623016\n",
      "Training loss: 0.0983467623591423\n",
      "Training loss: 0.10006083548069\n",
      "Training loss: 0.09518995136022568\n",
      "Training loss: 0.09640161693096161\n",
      "Training loss: 0.0961848720908165\n",
      "Training loss: 0.10017533600330353\n",
      "Training loss: 0.10396768152713776\n",
      "Training loss: 0.09984079748392105\n",
      "Training loss: 0.09705173224210739\n",
      "Training loss: 0.09903732687234879\n",
      "Training loss: 0.09347334504127502\n",
      "Training loss: 0.09220512956380844\n",
      "Training loss: 0.09923206269741058\n",
      "Training loss: 0.09846016019582748\n",
      "Training loss: 0.09607482701539993\n",
      "Training loss: 0.1131971925497055\n",
      "Training loss: 0.09688019007444382\n",
      "Training loss: 0.10324782133102417\n",
      "Training loss: 0.09871404618024826\n",
      "Training loss: 0.09778536856174469\n",
      "Training loss: 0.09796666353940964\n",
      "Training loss: 0.1010439321398735\n",
      "Training loss: 0.09493383020162582\n",
      "Training loss: 0.09529058635234833\n",
      "Training loss: 0.09881503880023956\n",
      "Training loss: 0.09997856616973877\n",
      "Training loss: 0.09666053205728531\n",
      "Training loss: 0.09631317853927612\n",
      "Training loss: 0.09839467704296112\n",
      "Training loss: 0.10251523554325104\n",
      "Training loss: 0.10337810218334198\n",
      "Training loss: 0.09359912574291229\n",
      "Training loss: 0.09681754559278488\n",
      "Training loss: 0.10223361849784851\n",
      "Training loss: 0.0952458381652832\n",
      "Training loss: 0.09315125644207001\n",
      "Training loss: 0.09492027759552002\n",
      "Training loss: 0.09278634190559387\n",
      "Training loss: 0.09726881235837936\n",
      "Training loss: 0.09263407438993454\n",
      "Training loss: 0.09513092786073685\n",
      "Training loss: 0.1003076434135437\n",
      "Training loss: 0.0995253473520279\n",
      "Training loss: 0.09145069122314453\n",
      "Training loss: 0.08935536444187164\n",
      "Training loss: 0.10219675302505493\n",
      "Training loss: 0.09472339600324631\n",
      "Training loss: 0.09428702294826508\n",
      "Training loss: 0.10361120104789734\n",
      "Training loss: 0.08815332502126694\n",
      "Training loss: 0.09402487426996231\n",
      "Training loss: 0.09031816571950912\n",
      "Training loss: 0.08979137986898422\n",
      "Training loss: 0.08820217102766037\n",
      "Training loss: 0.0947786346077919\n",
      "Training loss: 0.08712036907672882\n",
      "Training loss: 0.08931221812963486\n",
      "Training loss: 0.08425237238407135\n",
      "Training loss: 0.09678791463375092\n",
      "Training loss: 0.09372887760400772\n",
      "Training loss: 0.08686289936304092\n",
      "Training loss: 0.09141845256090164\n",
      "Training loss: 0.09216497093439102\n",
      "Training loss: 0.09097538143396378\n",
      "Training loss: 0.08880677074193954\n",
      "Training loss: 0.10037228465080261\n",
      "Training loss: 0.0922383964061737\n",
      "Training loss: 0.08981636166572571\n",
      "Training loss: 0.0879407525062561\n",
      "Training loss: 0.10698582231998444\n",
      "Training loss: 0.09205135703086853\n",
      "Training loss: 0.088786281645298\n",
      "Training loss: 0.09648170322179794\n",
      "Training loss: 0.0876460000872612\n",
      "Training loss: 0.09822645038366318\n",
      "Training loss: 0.09193532168865204\n",
      "Training loss: 0.09123818576335907\n",
      "Training loss: 0.09659122675657272\n",
      "Training loss: 0.10451838374137878\n",
      "Training loss: 0.09330637753009796\n",
      "Training loss: 0.08810322731733322\n",
      "Training loss: 0.09262144565582275\n",
      "Training loss: 0.09677508473396301\n",
      "Training loss: 0.1002776101231575\n",
      "Training loss: 0.09543817490339279\n",
      "Training loss: 0.09695548564195633\n",
      "Training loss: 0.1052011102437973\n",
      "Training loss: 0.09026143699884415\n",
      "Training loss: 0.09300369769334793\n",
      "Training loss: 0.09182218462228775\n",
      "Training loss: 0.09301190078258514\n",
      "Training loss: 0.09065505117177963\n",
      "Training loss: 0.09096825122833252\n",
      "Training loss: 0.08863144367933273\n",
      "Training loss: 0.09526985883712769\n",
      "Training loss: 0.0861581563949585\n",
      "Training loss: 0.09150522202253342\n",
      "Training loss: 0.09211896359920502\n",
      "Training loss: 0.09756822139024734\n",
      "Training loss: 0.08888666331768036\n",
      "Training loss: 0.08428430557250977\n",
      "Training loss: 0.0883050188422203\n",
      "Training loss: 0.08768356591463089\n",
      "Training loss: 0.09334409236907959\n",
      "Training loss: 0.08793309330940247\n",
      "Training loss: 0.08660530298948288\n",
      "Training loss: 0.08287538588047028\n",
      "Training loss: 0.08883107453584671\n",
      "Training loss: 0.09103697538375854\n",
      "Training loss: 0.09454989433288574\n",
      "Training loss: 0.0870688185095787\n",
      "Training loss: 0.09160207957029343\n",
      "Training loss: 0.08890023827552795\n",
      "Training loss: 0.09395790845155716\n",
      "Training loss: 0.09468010067939758\n",
      "Training loss: 0.08954556286334991\n",
      "Training loss: 0.08608192950487137\n",
      "Training loss: 0.09059940278530121\n",
      "Training loss: 0.09314709901809692\n",
      "Training loss: 0.09650275111198425\n",
      "Training loss: 0.08596587926149368\n",
      "Training loss: 0.08748531341552734\n",
      "Training loss: 0.10267390310764313\n",
      "Training loss: 0.09183675050735474\n",
      "Training loss: 0.08939233422279358\n",
      "Training loss: 0.0897749736905098\n",
      "Training loss: 0.09303771704435349\n",
      "Training loss: 0.09438343346118927\n",
      "Training loss: 0.09261482208967209\n",
      "Training loss: 0.08797746151685715\n",
      "Training loss: 0.08735865354537964\n",
      "Training loss: 0.09465078264474869\n",
      "Training loss: 0.0938531905412674\n",
      "Training loss: 0.08537928760051727\n",
      "Training loss: 0.09018395841121674\n",
      "Training loss: 0.09136859327554703\n",
      "Training loss: 0.09262338280677795\n",
      "Training loss: 0.09352216124534607\n",
      "Training loss: 0.09118623286485672\n",
      "Training loss: 0.09548544883728027\n",
      "Training loss: 0.08749904483556747\n",
      "Training loss: 0.08593106269836426\n",
      "Training loss: 0.09329116344451904\n",
      "Training loss: 0.08958835154771805\n",
      "Training loss: 0.08851686865091324\n",
      "Training loss: 0.09689926356077194\n",
      "Training loss: 0.08496242761611938\n",
      "Training loss: 0.09448117762804031\n",
      "Training loss: 0.08401471376419067\n",
      "Training loss: 0.08260015398263931\n",
      "Training loss: 0.08432891964912415\n",
      "Training loss: 0.09091702103614807\n",
      "Training loss: 0.08337713032960892\n",
      "Training loss: 0.09387791901826859\n",
      "Training loss: 0.08991358429193497\n",
      "Training loss: 0.09127636253833771\n",
      "Training loss: 0.09532976895570755\n",
      "Training loss: 0.09109596163034439\n",
      "Training loss: 0.0900798812508583\n",
      "Training loss: 0.08783511817455292\n",
      "Training loss: 0.08157978951931\n",
      "Training loss: 0.0926596149802208\n",
      "Training loss: 0.09117932617664337\n",
      "Training loss: 0.0876481831073761\n",
      "Training loss: 0.09036418050527573\n",
      "Training loss: 0.09255154430866241\n",
      "Training loss: 0.09026286005973816\n",
      "Training loss: 0.0838349238038063\n",
      "Training loss: 0.08893612027168274\n",
      "Training loss: 0.09339366108179092\n",
      "Training loss: 0.09087620675563812\n",
      "Training loss: 0.08723355829715729\n",
      "Training loss: 0.08787093311548233\n",
      "Training loss: 0.08726495504379272\n",
      "Training loss: 0.08362657576799393\n",
      "Training loss: 0.0846763551235199\n",
      "Training loss: 0.09624134749174118\n",
      "Training loss: 0.08906158059835434\n",
      "Training loss: 0.08575423806905746\n",
      "Training loss: 0.08544790744781494\n",
      "Training loss: 0.09333169460296631\n",
      "Training loss: 0.08277799189090729\n",
      "Training loss: 0.09091641753911972\n",
      "Training loss: 0.08952856808900833\n",
      "Training loss: 0.08983279019594193\n",
      "Training loss: 0.09597130864858627\n",
      "Training loss: 0.08923928439617157\n",
      "Training loss: 0.08591321855783463\n",
      "Training loss: 0.08755036443471909\n",
      "Training loss: 0.097439244389534\n",
      "Training loss: 0.08981636166572571\n",
      "Training loss: 0.08917812258005142\n",
      "Training loss: 0.08943761140108109\n",
      "Training loss: 0.08861877024173737\n",
      "Training loss: 0.09016682952642441\n",
      "Training loss: 0.08734395354986191\n",
      "Training loss: 0.08714205771684647\n",
      "Training loss: 0.08818142861127853\n",
      "Training loss: 0.09135253727436066\n",
      "Training loss: 0.08915936946868896\n",
      "Training loss: 0.0923139750957489\n",
      "Training loss: 0.0892745852470398\n",
      "Training loss: 0.08713457733392715\n",
      "Training loss: 0.08177604526281357\n",
      "Training loss: 0.08736582845449448\n",
      "Training loss: 0.09276564419269562\n",
      "Training loss: 0.08559858798980713\n",
      "Training loss: 0.08415959030389786\n",
      "Training loss: 0.08154328167438507\n",
      "Training loss: 0.08912364393472672\n",
      "Training loss: 0.0859157145023346\n",
      "Training loss: 0.08616326004266739\n",
      "Training loss: 0.09432865679264069\n",
      "Training loss: 0.08778052777051926\n",
      "Training loss: 0.08940361440181732\n",
      "Training loss: 0.08943627774715424\n",
      "Training loss: 0.08592643588781357\n",
      "Training loss: 0.09170246869325638\n",
      "Training loss: 0.08509112149477005\n",
      "Training loss: 0.09543541073799133\n",
      "Training loss: 0.08494092524051666\n",
      "Training loss: 0.08719882369041443\n",
      "Training loss: 0.08324887603521347\n",
      "Training loss: 0.09107519686222076\n",
      "Training loss: 0.0979493111371994\n",
      "Training loss: 0.08782219141721725\n",
      "Training loss: 0.09411992132663727\n",
      "Training loss: 0.08755350857973099\n",
      "Training loss: 0.09179060906171799\n",
      "Training loss: 0.0831286609172821\n",
      "Training loss: 0.09091363102197647\n",
      "Training loss: 0.08825080841779709\n",
      "Training loss: 0.08928704261779785\n",
      "Training loss: 0.09075553715229034\n",
      "Training loss: 0.09333190321922302\n",
      "Training loss: 0.09959935396909714\n",
      "Training loss: 0.09677934646606445\n",
      "Training loss: 0.09222937375307083\n",
      "Training loss: 0.09047325700521469\n",
      "Training loss: 0.09159056097269058\n",
      "Training loss: 0.08613178879022598\n",
      "Training loss: 0.09108016639947891\n",
      "Training loss: 0.08864859491586685\n",
      "Training loss: 0.08955681324005127\n",
      "Training loss: 0.08315819501876831\n",
      "Training loss: 0.09945124387741089\n",
      "Training loss: 0.08776122331619263\n",
      "Training loss: 0.09032903611660004\n",
      "Training loss: 0.08264940977096558\n",
      "Training loss: 0.09268434345722198\n",
      "Training loss: 0.09308154135942459\n",
      "Training loss: 0.08358123898506165\n",
      "Training loss: 0.08887942880392075\n",
      "Training loss: 0.09480919688940048\n",
      "Training loss: 0.08664052933454514\n",
      "Training loss: 0.09261684864759445\n",
      "Training loss: 0.0849488377571106\n",
      "Training loss: 0.08349453657865524\n",
      "Training loss: 0.0843285545706749\n",
      "Training loss: 0.08510397374629974\n",
      "Training loss: 0.09653732180595398\n",
      "Training loss: 0.08399705588817596\n",
      "Training loss: 0.08974452316761017\n",
      "Training loss: 0.0904281809926033\n",
      "Training loss: 0.0902925580739975\n",
      "Training loss: 0.0898163765668869\n",
      "Training loss: 0.08455309271812439\n",
      "Training loss: 0.08201559633016586\n",
      "Training loss: 0.08872737735509872\n",
      "Training loss: 0.08267565816640854\n",
      "Training loss: 0.08740947395563126\n",
      "Training loss: 0.08569181710481644\n",
      "Training loss: 0.08521375805139542\n",
      "Training loss: 0.08555344492197037\n",
      "Training loss: 0.08524667471647263\n",
      "Training loss: 0.0908036082983017\n",
      "Training loss: 0.09301257133483887\n",
      "Training loss: 0.08987357467412949\n",
      "Training loss: 0.08313217014074326\n",
      "Training loss: 0.0908856987953186\n",
      "Training loss: 0.0890074148774147\n",
      "Training loss: 0.09126828610897064\n",
      "Training loss: 0.09031617641448975\n",
      "Training loss: 0.0942503958940506\n",
      "Training loss: 0.08965146541595459\n",
      "Training loss: 0.09139017015695572\n",
      "Training loss: 0.08949427306652069\n",
      "Training loss: 0.08716820180416107\n",
      "Training loss: 0.08812617510557175\n",
      "Training loss: 0.0949774757027626\n",
      "Training loss: 0.08657206594944\n",
      "Training loss: 0.09220881760120392\n",
      "Training loss: 0.08783572912216187\n",
      "Training loss: 0.09122820943593979\n",
      "Training loss: 0.0850079134106636\n",
      "Training loss: 0.08556774258613586\n",
      "Training loss: 0.09965834766626358\n",
      "Training loss: 0.0905870795249939\n",
      "Training loss: 0.0916062667965889\n",
      "Training loss: 0.09684910625219345\n",
      "Training loss: 0.10027558356523514\n",
      "Training loss: 0.09085822105407715\n",
      "Training loss: 0.09335796535015106\n",
      "Training loss: 0.09878814220428467\n",
      "Training loss: 0.09429875016212463\n",
      "Training loss: 0.09333447366952896\n",
      "Training loss: 0.08703901618719101\n",
      "Training loss: 0.09547098726034164\n",
      "Training loss: 0.09777296334505081\n",
      "Training loss: 0.09101929515600204\n",
      "Training loss: 0.08705101907253265\n",
      "Training loss: 0.09312719106674194\n",
      "Training loss: 0.09474501758813858\n",
      "Training loss: 0.08995071798563004\n",
      "Training loss: 0.08954060077667236\n",
      "Training loss: 0.08856328576803207\n",
      "Training loss: 0.0919128805398941\n",
      "Training loss: 0.08560923486948013\n",
      "Training loss: 0.09390914440155029\n",
      "Training loss: 0.09398338198661804\n",
      "Training loss: 0.09046610444784164\n",
      "Training loss: 0.0875648483633995\n",
      "Training loss: 0.09117096662521362\n",
      "Training loss: 0.09791257232427597\n",
      "Training loss: 0.09142062813043594\n",
      "Training loss: 0.0901179239153862\n",
      "Training loss: 0.08801090717315674\n",
      "Training loss: 0.09330615401268005\n",
      "Training loss: 0.09303267300128937\n",
      "Training loss: 0.0902228131890297\n",
      "Training loss: 0.08826147764921188\n",
      "Training loss: 0.08796527236700058\n",
      "Training loss: 0.09367407113313675\n",
      "Training loss: 0.09354563057422638\n",
      "Training loss: 0.0896850973367691\n",
      "Training loss: 0.09172789752483368\n",
      "Training loss: 0.09311649948358536\n",
      "Training loss: 0.09200633317232132\n",
      "Training loss: 0.09210021793842316\n",
      "Training loss: 0.08945509791374207\n",
      "Training loss: 0.09017682075500488\n",
      "Training loss: 0.09023795276880264\n",
      "Training loss: 0.09174071252346039\n",
      "Training loss: 0.09353332966566086\n",
      "Training loss: 0.0900408923625946\n",
      "Training loss: 0.09018650650978088\n",
      "Training loss: 0.0883990228176117\n",
      "Training loss: 0.09074273705482483\n",
      "Training loss: 0.08627728372812271\n",
      "Training loss: 0.0912708267569542\n",
      "Training loss: 0.08943431824445724\n",
      "Training loss: 0.08697463572025299\n",
      "Training loss: 0.09133221954107285\n",
      "Training loss: 0.09699814766645432\n",
      "Training loss: 0.09176792949438095\n",
      "Training loss: 0.09007975459098816\n",
      "Training loss: 0.08828762173652649\n",
      "Training loss: 0.09562378376722336\n",
      "Training loss: 0.09454074501991272\n",
      "Training loss: 0.08833342045545578\n",
      "Training loss: 0.09082545340061188\n",
      "Training loss: 0.09266968816518784\n",
      "Training loss: 0.08770133554935455\n",
      "Training loss: 0.08486904203891754\n",
      "Training loss: 0.09153230488300323\n",
      "Training loss: 0.09042564034461975\n",
      "Training loss: 0.08832532167434692\n",
      "Training loss: 0.08879964798688889\n",
      "Training loss: 0.08634580671787262\n",
      "Training loss: 0.09215766936540604\n",
      "Training loss: 0.08523212373256683\n",
      "Training loss: 0.08684343099594116\n",
      "Training loss: 0.08556322753429413\n",
      "Training loss: 0.091868095099926\n",
      "Training loss: 0.096678227186203\n",
      "Training loss: 0.08608213812112808\n",
      "Training loss: 0.09196183830499649\n",
      "Training loss: 0.08717266470193863\n",
      "Training loss: 0.09361125528812408\n",
      "Training loss: 0.0920107439160347\n",
      "Training loss: 0.08898850530385971\n",
      "Training loss: 0.09026307612657547\n",
      "Training loss: 0.09363363683223724\n",
      "Training loss: 0.08607004582881927\n",
      "Training loss: 0.09437241405248642\n",
      "Training loss: 0.09006038308143616\n",
      "Training loss: 0.09117113053798676\n",
      "Training loss: 0.09373952448368073\n",
      "Training loss: 0.08820781111717224\n",
      "Training loss: 0.09729126840829849\n",
      "Training loss: 0.09605848044157028\n",
      "Training loss: 0.09115389734506607\n",
      "Training loss: 0.08728531748056412\n",
      "Training loss: 0.0897081270813942\n",
      "Training loss: 0.08602484315633774\n",
      "Training loss: 0.08195792138576508\n",
      "Training loss: 0.09235062450170517\n",
      "Training loss: 0.09751913696527481\n",
      "Training loss: 0.09349165856838226\n",
      "Training loss: 0.09046564996242523\n",
      "Training loss: 0.08980882912874222\n",
      "Training loss: 0.08886001259088516\n",
      "Training loss: 0.09348563104867935\n",
      "Training loss: 0.09399192780256271\n",
      "Training loss: 0.10095380246639252\n",
      "Training loss: 0.08650463074445724\n",
      "Training loss: 0.09540640562772751\n",
      "Training loss: 0.10547056049108505\n",
      "Training loss: 0.09651952981948853\n",
      "Training loss: 0.09295430034399033\n",
      "Training loss: 0.10021545737981796\n",
      "Training loss: 0.09795628488063812\n",
      "Training loss: 0.09701602905988693\n",
      "Training loss: 0.08970535546541214\n",
      "Training loss: 0.0896439254283905\n",
      "Training loss: 0.0994616150856018\n",
      "Training loss: 0.09592218697071075\n",
      "Training loss: 0.09149481356143951\n",
      "Training loss: 0.0869370549917221\n",
      "Training loss: 0.0931885689496994\n",
      "Training loss: 0.09981569647789001\n",
      "Training loss: 0.09895183145999908\n",
      "Training loss: 0.0942419171333313\n",
      "Training loss: 0.09495861083269119\n",
      "Training loss: 0.09296683967113495\n",
      "Training loss: 0.08919994533061981\n",
      "Training loss: 0.09214232861995697\n",
      "Training loss: 0.08972165733575821\n",
      "Training loss: 0.09435521066188812\n",
      "Training loss: 0.08895694464445114\n",
      "Training loss: 0.09259282052516937\n",
      "Training loss: 0.08917196840047836\n",
      "Training loss: 0.08878771215677261\n",
      "Training loss: 0.10135301202535629\n",
      "Training loss: 0.09546440094709396\n",
      "Training loss: 0.09444225579500198\n",
      "Training loss: 0.08661721646785736\n",
      "Training loss: 0.08990664780139923\n",
      "Training loss: 0.08809439837932587\n",
      "Training loss: 0.08873773366212845\n",
      "Training loss: 0.09157266467809677\n",
      "Training loss: 0.09205776453018188\n",
      "Training loss: 0.08961665630340576\n",
      "Training loss: 0.08999411761760712\n",
      "Training loss: 0.09042293578386307\n",
      "Training loss: 0.08810319006443024\n",
      "Training loss: 0.09223300218582153\n",
      "Training loss: 0.093128502368927\n",
      "Training loss: 0.0942639708518982\n",
      "Training loss: 0.0875873789191246\n",
      "Training loss: 0.09444574266672134\n",
      "Training loss: 0.08863259106874466\n",
      "Training loss: 0.08977364748716354\n",
      "Training loss: 0.09044717252254486\n",
      "Training loss: 0.09723199903964996\n",
      "Training loss: 0.08421671390533447\n",
      "Training loss: 0.09195562452077866\n",
      "Training loss: 0.09043289721012115\n",
      "Training loss: 0.0837254747748375\n",
      "Training loss: 0.09285105764865875\n",
      "Training loss: 0.08158595114946365\n",
      "Training loss: 0.08981459587812424\n",
      "Training loss: 0.09113455563783646\n",
      "Training loss: 0.09603945165872574\n",
      "Training loss: 0.0934114009141922\n",
      "Training loss: 0.08959955722093582\n",
      "Training loss: 0.09059938788414001\n",
      "Training loss: 0.09124422073364258\n",
      "Training loss: 0.09642962366342545\n",
      "Training loss: 0.0893435925245285\n",
      "Training loss: 0.08726773411035538\n",
      "Training loss: 0.09159897267818451\n",
      "Training loss: 0.08893364667892456\n",
      "Training loss: 0.08680889755487442\n",
      "Training loss: 0.09508487582206726\n",
      "Training loss: 0.09746130555868149\n",
      "Training loss: 0.08793654292821884\n",
      "Training loss: 0.09297465533018112\n",
      "Training loss: 0.08575121313333511\n",
      "Training loss: 0.08993270993232727\n",
      "Training loss: 0.0925513505935669\n",
      "Training loss: 0.09563765674829483\n",
      "Training loss: 0.09854821860790253\n",
      "Training loss: 0.0948360413312912\n",
      "Training loss: 0.09322895109653473\n",
      "Training loss: 0.09390251338481903\n",
      "Training loss: 0.08769580721855164\n",
      "Training loss: 0.09255985915660858\n",
      "Training loss: 0.0936901867389679\n",
      "Training loss: 0.09209690243005753\n",
      "Training loss: 0.09560036659240723\n",
      "Training loss: 0.09554184228181839\n",
      "Training loss: 0.09259791672229767\n",
      "Training loss: 0.09165282547473907\n",
      "Training loss: 0.09746447205543518\n",
      "Training loss: 0.09035694599151611\n",
      "Training loss: 0.0903659462928772\n",
      "Training loss: 0.08808358013629913\n",
      "Training loss: 0.08673017472028732\n",
      "Training loss: 0.08652252703905106\n",
      "Training loss: 0.08951821178197861\n",
      "Training loss: 0.09425567835569382\n",
      "Training loss: 0.09058117866516113\n",
      "Training loss: 0.09031162410974503\n",
      "Training loss: 0.08821631968021393\n",
      "Training loss: 0.08476182073354721\n",
      "Training loss: 0.08846104890108109\n",
      "Training loss: 0.09026754647493362\n",
      "Training loss: 0.08669622987508774\n",
      "Training loss: 0.08914963155984879\n",
      "Training loss: 0.092662513256073\n",
      "Training loss: 0.08858814835548401\n",
      "Training loss: 0.09393011778593063\n",
      "Training loss: 0.08356758207082748\n",
      "Training loss: 0.09417255222797394\n",
      "Training loss: 0.0926220491528511\n",
      "Training loss: 0.09283235669136047\n",
      "Training loss: 0.09649952501058578\n",
      "Training loss: 0.0955086350440979\n",
      "Training loss: 0.08957745879888535\n",
      "Training loss: 0.0926133245229721\n",
      "Training loss: 0.08978599309921265\n",
      "Training loss: 0.09045311063528061\n",
      "Training loss: 0.08724180608987808\n",
      "Training loss: 0.09047383815050125\n",
      "Training loss: 0.09383069723844528\n",
      "Training loss: 0.08837494999170303\n",
      "Training loss: 0.0948210135102272\n",
      "Training loss: 0.08417429029941559\n",
      "Training loss: 0.09053655713796616\n",
      "Training loss: 0.09233603626489639\n",
      "Training loss: 0.08958669006824493\n",
      "Training loss: 0.0868283286690712\n",
      "Training loss: 0.09055567532777786\n",
      "Training loss: 0.09043220430612564\n",
      "Training loss: 0.08834390342235565\n",
      "Training loss: 0.08875656127929688\n",
      "Training loss: 0.08562085032463074\n",
      "Training loss: 0.09073172509670258\n",
      "Training loss: 0.08903303742408752\n",
      "Training loss: 0.08583160489797592\n",
      "Training loss: 0.0832788273692131\n",
      "Training loss: 0.08217625319957733\n",
      "Training loss: 0.09064733982086182\n",
      "Training loss: 0.09067053347826004\n",
      "Training loss: 0.09180687367916107\n",
      "Training loss: 0.09404926002025604\n",
      "Training loss: 0.0844166949391365\n",
      "Training loss: 0.08787637203931808\n",
      "Training loss: 0.08693800866603851\n",
      "Training loss: 0.08322056382894516\n",
      "Training loss: 0.0869162455201149\n",
      "Training loss: 0.0868135541677475\n",
      "Training loss: 0.08591104298830032\n",
      "Training loss: 0.0852213203907013\n",
      "Training loss: 0.09021545201539993\n",
      "Training loss: 0.09016913920640945\n",
      "Training loss: 0.08491363376379013\n",
      "Training loss: 0.08782248944044113\n",
      "Training loss: 0.08433566242456436\n",
      "Training loss: 0.08188866078853607\n",
      "Training loss: 0.07890067994594574\n",
      "Training loss: 0.0839829295873642\n",
      "Training loss: 0.0868760272860527\n",
      "Training loss: 0.08216995745897293\n",
      "Training loss: 0.0873834490776062\n",
      "Training loss: 0.08147431910037994\n",
      "Training loss: 0.08690750598907471\n",
      "Training loss: 0.08430532366037369\n",
      "Training loss: 0.0818445160984993\n",
      "Training loss: 0.08486811071634293\n",
      "Training loss: 0.08581790328025818\n",
      "Training loss: 0.09168403595685959\n",
      "Training loss: 0.0875663012266159\n",
      "Training loss: 0.08724450320005417\n",
      "Training loss: 0.09920459240674973\n",
      "Training loss: 0.09188001602888107\n",
      "Training loss: 0.09040875732898712\n",
      "Training loss: 0.09652058780193329\n",
      "Training loss: 0.08663620054721832\n",
      "Training loss: 0.09349478781223297\n",
      "Training loss: 0.08255699276924133\n",
      "Training loss: 0.08624003082513809\n",
      "Training loss: 0.08606553822755814\n",
      "Training loss: 0.08473224937915802\n",
      "Training loss: 0.08416657149791718\n",
      "Training loss: 0.08422005921602249\n",
      "Training loss: 0.0856645256280899\n",
      "Training loss: 0.0869402140378952\n",
      "Training loss: 0.08320476859807968\n",
      "Training loss: 0.08650115132331848\n",
      "Training loss: 0.08673517405986786\n",
      "Training loss: 0.08861526101827621\n",
      "Training loss: 0.08625856041908264\n",
      "Training loss: 0.08922667801380157\n",
      "Training loss: 0.08571465313434601\n",
      "Training loss: 0.09101048856973648\n",
      "Training loss: 0.08391661196947098\n",
      "Training loss: 0.0858408510684967\n",
      "Training loss: 0.08922062814235687\n",
      "Training loss: 0.09146006405353546\n",
      "Training loss: 0.08740253001451492\n",
      "Training loss: 0.09321117401123047\n",
      "Training loss: 0.09165727347135544\n",
      "Training loss: 0.0924467071890831\n",
      "Training loss: 0.08510155975818634\n",
      "Training loss: 0.08936221897602081\n",
      "Training loss: 0.08862756192684174\n",
      "Training loss: 0.08384211361408234\n",
      "Training loss: 0.08769350498914719\n",
      "Training loss: 0.0882069319486618\n",
      "Training loss: 0.08932007849216461\n",
      "Training loss: 0.09364436566829681\n",
      "Training loss: 0.08868693560361862\n",
      "Training loss: 0.0860418751835823\n",
      "Training loss: 0.08516010642051697\n",
      "Training loss: 0.08539777994155884\n",
      "Training loss: 0.08589407801628113\n",
      "Training loss: 0.09333174675703049\n",
      "Training loss: 0.08230356127023697\n",
      "Training loss: 0.0926346480846405\n",
      "Training loss: 0.09068015217781067\n",
      "Training loss: 0.08580514043569565\n",
      "Training loss: 0.09079593420028687\n",
      "Training loss: 0.08872630447149277\n",
      "Training loss: 0.08078941702842712\n",
      "Training loss: 0.08801290392875671\n",
      "Training loss: 0.08190831542015076\n",
      "Training loss: 0.08276455849409103\n",
      "Training loss: 0.08160494267940521\n",
      "Training loss: 0.08541565388441086\n",
      "Training loss: 0.08257270604372025\n",
      "Training loss: 0.08598114550113678\n",
      "Training loss: 0.08512525260448456\n",
      "Training loss: 0.08937720954418182\n",
      "Training loss: 0.08180394768714905\n",
      "Training loss: 0.08810897916555405\n",
      "Training loss: 0.08813722431659698\n",
      "Training loss: 0.08416343480348587\n",
      "Training loss: 0.08373866975307465\n",
      "Training loss: 0.08292420953512192\n",
      "Training loss: 0.08303577452898026\n",
      "Training loss: 0.07739335298538208\n",
      "Training loss: 0.07797075808048248\n",
      "Training loss: 0.08557777851819992\n",
      "Training loss: 0.08529797196388245\n",
      "Training loss: 0.08568401634693146\n",
      "Training loss: 0.08414287120103836\n",
      "Training loss: 0.08625244349241257\n",
      "Training loss: 0.08801960945129395\n",
      "Training loss: 0.08783020824193954\n",
      "Training loss: 0.08235990256071091\n",
      "Training loss: 0.09007047116756439\n",
      "Training loss: 0.0849817544221878\n",
      "Training loss: 0.08022210001945496\n",
      "Training loss: 0.08493252843618393\n",
      "Training loss: 0.08784479647874832\n",
      "Training loss: 0.08428728580474854\n",
      "Training loss: 0.08898128569126129\n",
      "Training loss: 0.07959295809268951\n",
      "Training loss: 0.08528430759906769\n",
      "Training loss: 0.08210572600364685\n",
      "Training loss: 0.08248702436685562\n",
      "Training loss: 0.0821656882762909\n",
      "Training loss: 0.0891944020986557\n",
      "Training loss: 0.08474065363407135\n",
      "Training loss: 0.08926023542881012\n",
      "Training loss: 0.08358573913574219\n",
      "Training loss: 0.08356180787086487\n",
      "Training loss: 0.08280488103628159\n",
      "Training loss: 0.07978396862745285\n",
      "Training loss: 0.0932135134935379\n",
      "Training loss: 0.0908031091094017\n",
      "Training loss: 0.08275525271892548\n",
      "Training loss: 0.08767398446798325\n",
      "Training loss: 0.08225913345813751\n",
      "Training loss: 0.08132711797952652\n",
      "Training loss: 0.09618589282035828\n",
      "Training loss: 0.0847829058766365\n",
      "Training loss: 0.08803258836269379\n",
      "Training loss: 0.08422589302062988\n",
      "Training loss: 0.09221924096345901\n",
      "Training loss: 0.08760326355695724\n",
      "Training loss: 0.08504363894462585\n",
      "Training loss: 0.08764822781085968\n",
      "Training loss: 0.09218677878379822\n",
      "Training loss: 0.08900415152311325\n",
      "Training loss: 0.08413499593734741\n",
      "Training loss: 0.08422259986400604\n",
      "Training loss: 0.09174538403749466\n",
      "Training loss: 0.08773220330476761\n",
      "Training loss: 0.08359738439321518\n",
      "Training loss: 0.0988357737660408\n",
      "Training loss: 0.08854043483734131\n",
      "Training loss: 0.08498542010784149\n",
      "Training loss: 0.08763790130615234\n",
      "Training loss: 0.08147678524255753\n",
      "Training loss: 0.08768013119697571\n",
      "Training loss: 0.08845051378011703\n",
      "Training loss: 0.08298203349113464\n",
      "Training loss: 0.09010474383831024\n",
      "Training loss: 0.08753940463066101\n",
      "Training loss: 0.08353440463542938\n",
      "Training loss: 0.09226778894662857\n",
      "Training loss: 0.08078478276729584\n",
      "Training loss: 0.08465835452079773\n",
      "Training loss: 0.08582711219787598\n",
      "Training loss: 0.08391807228326797\n",
      "Training loss: 0.08968684822320938\n",
      "Training loss: 0.08778093010187149\n",
      "Training loss: 0.08598233759403229\n",
      "Training loss: 0.09126605093479156\n",
      "Training loss: 0.08777690678834915\n",
      "Training loss: 0.08644392341375351\n",
      "Training loss: 0.08861120045185089\n",
      "Training loss: 0.0831088125705719\n",
      "Training loss: 0.09450653940439224\n",
      "Training loss: 0.09884873032569885\n",
      "Training loss: 0.08549372851848602\n",
      "Training loss: 0.08553062379360199\n",
      "Training loss: 0.08310125768184662\n",
      "Training loss: 0.09301186352968216\n",
      "Training loss: 0.08592194318771362\n",
      "Training loss: 0.09427215903997421\n",
      "Training loss: 0.09150084853172302\n",
      "Training loss: 0.08551781624555588\n",
      "Training loss: 0.09062712639570236\n",
      "Training loss: 0.08781757950782776\n",
      "Training loss: 0.08593674749135971\n",
      "Training loss: 0.08867360651493073\n",
      "Training loss: 0.0843203067779541\n",
      "Training loss: 0.08811406046152115\n",
      "Training loss: 0.09187494963407516\n",
      "Training loss: 0.08846083283424377\n",
      "Training loss: 0.08692331612110138\n",
      "Training loss: 0.09212881326675415\n",
      "Training loss: 0.0861843153834343\n",
      "Training loss: 0.09263032674789429\n",
      "Training loss: 0.08025654405355453\n",
      "Training loss: 0.08370477706193924\n",
      "Training loss: 0.08194723725318909\n",
      "Training loss: 0.08930188417434692\n",
      "Training loss: 0.09506526589393616\n",
      "Training loss: 0.07982209324836731\n",
      "Training loss: 0.0848461464047432\n",
      "Training loss: 0.08893219381570816\n",
      "Training loss: 0.084676094353199\n",
      "Training loss: 0.08084780722856522\n",
      "Training loss: 0.09179192036390305\n",
      "Training loss: 0.08266118913888931\n",
      "Training loss: 0.09199827164411545\n",
      "Training loss: 0.08236952871084213\n",
      "Training loss: 0.08320344984531403\n",
      "Training loss: 0.08606602251529694\n",
      "Training loss: 0.07954316586256027\n",
      "Training loss: 0.08061473071575165\n",
      "Training loss: 0.07905761897563934\n",
      "Training loss: 0.08141397684812546\n",
      "Training loss: 0.08254644274711609\n",
      "Training loss: 0.08040351420640945\n",
      "Training loss: 0.08431712538003922\n",
      "Training loss: 0.08166297525167465\n",
      "Training loss: 0.08170618861913681\n",
      "Training loss: 0.08575210720300674\n",
      "Training loss: 0.0857386440038681\n",
      "Training loss: 0.08276065438985825\n",
      "Training loss: 0.08283930271863937\n",
      "Training loss: 0.09303595125675201\n",
      "Training loss: 0.08838997781276703\n",
      "Training loss: 0.08393247425556183\n",
      "Training loss: 0.09031617641448975\n",
      "Training loss: 0.09392662346363068\n",
      "Training loss: 0.08518430590629578\n",
      "Training loss: 0.08915700018405914\n",
      "Training loss: 0.0829121321439743\n",
      "Training loss: 0.08711444586515427\n",
      "Training loss: 0.08739657700061798\n",
      "Training loss: 0.09381097555160522\n",
      "Training loss: 0.08212894946336746\n",
      "Training loss: 0.0826912447810173\n",
      "Training loss: 0.08531079441308975\n",
      "Training loss: 0.08865836262702942\n",
      "Training loss: 0.08386117964982986\n",
      "Training loss: 0.0871787965297699\n",
      "Training loss: 0.08898352831602097\n",
      "Training loss: 0.10476768016815186\n",
      "Training loss: 0.08782706409692764\n",
      "Training loss: 0.07914610952138901\n",
      "Training loss: 0.08450287580490112\n",
      "Training loss: 0.0894683450460434\n",
      "Training loss: 0.08732499182224274\n",
      "Training loss: 0.09023210406303406\n",
      "Training loss: 0.096213199198246\n",
      "Training loss: 0.09173127263784409\n",
      "Training loss: 0.11164814233779907\n",
      "Training loss: 0.08334004133939743\n",
      "Training loss: 0.09424792975187302\n",
      "Training loss: 0.08729734271764755\n",
      "Training loss: 0.08827801793813705\n",
      "Training loss: 0.0936981663107872\n",
      "Training loss: 0.09267204254865646\n",
      "Training loss: 0.0886940136551857\n",
      "Training loss: 0.08595871180295944\n",
      "Training loss: 0.08630052208900452\n",
      "Training loss: 0.09479010850191116\n",
      "Training loss: 0.08440960943698883\n",
      "Training loss: 0.08291251957416534\n",
      "Training loss: 0.08940084278583527\n",
      "Training loss: 0.08872035890817642\n",
      "Training loss: 0.09770242869853973\n",
      "Training loss: 0.089936763048172\n",
      "Training loss: 0.08509156852960587\n",
      "Training loss: 0.08553877472877502\n",
      "Training loss: 0.08939575403928757\n",
      "Training loss: 0.09130965173244476\n",
      "Training loss: 0.0891006588935852\n",
      "Training loss: 0.08622798323631287\n",
      "Training loss: 0.0856931284070015\n",
      "Training loss: 0.08761471509933472\n",
      "Training loss: 0.0868799090385437\n",
      "Training loss: 0.08914806693792343\n",
      "Training loss: 0.08113818615674973\n",
      "Training loss: 0.08222232758998871\n",
      "Training loss: 0.08984246104955673\n",
      "Training loss: 0.08950556814670563\n",
      "Training loss: 0.08465918153524399\n",
      "Training loss: 0.09192579984664917\n",
      "Training loss: 0.08785922080278397\n",
      "Training loss: 0.08328749239444733\n",
      "Training loss: 0.08292977511882782\n",
      "Training loss: 0.08170723915100098\n",
      "Training loss: 0.08567126095294952\n",
      "Training loss: 0.07728627324104309\n",
      "Training loss: 0.08503016084432602\n",
      "Training loss: 0.08968087285757065\n",
      "Training loss: 0.08177658170461655\n",
      "Training loss: 0.08567149192094803\n",
      "Training loss: 0.0816570445895195\n",
      "Training loss: 0.08331172913312912\n",
      "Training loss: 0.09663075953722\n",
      "Training loss: 0.08643462508916855\n",
      "Training loss: 0.08317674696445465\n",
      "Training loss: 0.09109372645616531\n",
      "Training loss: 0.09095843881368637\n",
      "Training loss: 0.08912474662065506\n",
      "Training loss: 0.08249972760677338\n",
      "Training loss: 0.09839824587106705\n",
      "Training loss: 0.08796540647745132\n",
      "Training loss: 0.08576170355081558\n",
      "Training loss: 0.0988512933254242\n",
      "Training loss: 0.0859135091304779\n",
      "Training loss: 0.08856377750635147\n",
      "Training loss: 0.08607230335474014\n",
      "Training loss: 0.09493832290172577\n",
      "Training loss: 0.08431942015886307\n",
      "Training loss: 0.088584303855896\n",
      "Training loss: 0.09141532331705093\n",
      "Training loss: 0.09259810298681259\n",
      "Training loss: 0.09216468781232834\n",
      "Training loss: 0.08864303678274155\n",
      "Training loss: 0.08372709155082703\n",
      "Training loss: 0.08336962759494781\n",
      "Training loss: 0.0908864438533783\n",
      "Training loss: 0.08712022006511688\n",
      "Training loss: 0.09339354932308197\n",
      "Training loss: 0.08538807183504105\n",
      "Training loss: 0.0872260108590126\n",
      "Training loss: 0.0815722644329071\n",
      "Training loss: 0.08859676867723465\n",
      "Training loss: 0.09044308960437775\n",
      "Training loss: 0.09054231643676758\n",
      "Training loss: 0.08529674261808395\n",
      "Training loss: 0.08261027932167053\n",
      "Training loss: 0.08934544026851654\n",
      "Training loss: 0.09448893368244171\n",
      "Training loss: 0.08014681190252304\n",
      "Training loss: 0.08772852271795273\n",
      "Training loss: 0.0906352773308754\n",
      "Training loss: 0.08380106091499329\n",
      "Training loss: 0.07993945479393005\n",
      "Training loss: 0.08963838964700699\n",
      "Training loss: 0.09158749878406525\n",
      "Training loss: 0.0882866159081459\n",
      "Training loss: 0.08783670514822006\n",
      "Training loss: 0.08538393676280975\n",
      "Training loss: 0.08561349660158157\n",
      "Training loss: 0.08214074373245239\n",
      "Training loss: 0.08838298916816711\n",
      "Training loss: 0.081537164747715\n",
      "Training loss: 0.08102501928806305\n",
      "Training loss: 0.08674423396587372\n",
      "Training loss: 0.08604755252599716\n",
      "Training loss: 0.08161451667547226\n",
      "Training loss: 0.08648338168859482\n",
      "Training loss: 0.08942201733589172\n",
      "Training loss: 0.08360029011964798\n",
      "Training loss: 0.08520682156085968\n",
      "Training loss: 0.08411671966314316\n",
      "Training loss: 0.08277557790279388\n",
      "Training loss: 0.0883759930729866\n",
      "Training loss: 0.08219090104103088\n",
      "Training loss: 0.09561733901500702\n",
      "Training loss: 0.08329673111438751\n",
      "Training loss: 0.08767272531986237\n",
      "Training loss: 0.08490006625652313\n",
      "Training loss: 0.08571934700012207\n",
      "Training loss: 0.08707625418901443\n",
      "Training loss: 0.09179266542196274\n",
      "Training loss: 0.09374246001243591\n",
      "Training loss: 0.09808257967233658\n",
      "Training loss: 0.08615931868553162\n",
      "Training loss: 0.09048941731452942\n",
      "Training loss: 0.08983545750379562\n",
      "Training loss: 0.08649363368749619\n",
      "Training loss: 0.08314190059900284\n",
      "Training loss: 0.08623563498258591\n",
      "Training loss: 0.08267271518707275\n",
      "Training loss: 0.08836173266172409\n",
      "Training loss: 0.0903066024184227\n",
      "Training loss: 0.08528713881969452\n",
      "Training loss: 0.08519227802753448\n",
      "Training loss: 0.0898364931344986\n",
      "Training loss: 0.08386064320802689\n",
      "Training loss: 0.08907942473888397\n",
      "Training loss: 0.08420158177614212\n",
      "Training loss: 0.08789148181676865\n",
      "Training loss: 0.08310804516077042\n",
      "Training loss: 0.08749986439943314\n",
      "Training loss: 0.08554086089134216\n",
      "Training loss: 0.08004993945360184\n",
      "Training loss: 0.07907646894454956\n",
      "Training loss: 0.07826706022024155\n",
      "Training loss: 0.07840126752853394\n",
      "Training loss: 0.09531909972429276\n",
      "Training loss: 0.07609201222658157\n",
      "Training loss: 0.0789269283413887\n",
      "Training loss: 0.0866876021027565\n",
      "Training loss: 0.07846426218748093\n",
      "Training loss: 0.07863465696573257\n",
      "Training loss: 0.0812983363866806\n",
      "Training loss: 0.08184521645307541\n",
      "Training loss: 0.08169849961996078\n",
      "Training loss: 0.07842923700809479\n",
      "Training loss: 0.07700143754482269\n",
      "Training loss: 0.07981278747320175\n",
      "Training loss: 0.08302830904722214\n",
      "Training loss: 0.08500882238149643\n",
      "Training loss: 0.08191882818937302\n",
      "Training loss: 0.08441749215126038\n",
      "Training loss: 0.08944133669137955\n",
      "Training loss: 0.08482106775045395\n",
      "Training loss: 0.0899212434887886\n",
      "Training loss: 0.08603876084089279\n",
      "Training loss: 0.07455337792634964\n",
      "Training loss: 0.07937465608119965\n",
      "Training loss: 0.08307430148124695\n",
      "Training loss: 0.08273432403802872\n",
      "Training loss: 0.08100682497024536\n",
      "Training loss: 0.08016893267631531\n",
      "Training loss: 0.08702096343040466\n",
      "Training loss: 0.08388051390647888\n",
      "Training loss: 0.08763612806797028\n",
      "Training loss: 0.0814887061715126\n",
      "Training loss: 0.08368782699108124\n",
      "Training loss: 0.0796264261007309\n",
      "Training loss: 0.08376338332891464\n",
      "Training loss: 0.08919407427310944\n",
      "Training loss: 0.07525550574064255\n",
      "Training loss: 0.08424873650074005\n",
      "Training loss: 0.08130770921707153\n",
      "Training loss: 0.08501136302947998\n",
      "Training loss: 0.07877743244171143\n",
      "Training loss: 0.07909909635782242\n",
      "Training loss: 0.0988447293639183\n",
      "Training loss: 0.08574004471302032\n",
      "Training loss: 0.08775223046541214\n",
      "Training loss: 0.08806496113538742\n",
      "Training loss: 0.08812787383794785\n",
      "Training loss: 0.0824219286441803\n",
      "Training loss: 0.08363820612430573\n",
      "Training loss: 0.08419882506132126\n",
      "Training loss: 0.08552029728889465\n",
      "Training loss: 0.08696386218070984\n",
      "Training loss: 0.08618944138288498\n",
      "Training loss: 0.07971242815256119\n",
      "Training loss: 0.09070349484682083\n",
      "Training loss: 0.08591867983341217\n",
      "Training loss: 0.09223836660385132\n",
      "Training loss: 0.08327838033437729\n",
      "Training loss: 0.08103899657726288\n",
      "Training loss: 0.08667010068893433\n",
      "Training loss: 0.08566676825284958\n",
      "Training loss: 0.0836678296327591\n",
      "Training loss: 0.08748544007539749\n",
      "Training loss: 0.08627288043498993\n",
      "Training loss: 0.08720241487026215\n",
      "Training loss: 0.08777892589569092\n",
      "Training loss: 0.07992063462734222\n",
      "Training loss: 0.07871361821889877\n",
      "Training loss: 0.08842471241950989\n",
      "Training loss: 0.08270616829395294\n",
      "Training loss: 0.08749445527791977\n",
      "Training loss: 0.07743986696004868\n",
      "Training loss: 0.08458297699689865\n",
      "Training loss: 0.08583467453718185\n",
      "Training loss: 0.08202408254146576\n",
      "Training loss: 0.08235461264848709\n",
      "Training loss: 0.08623730391263962\n",
      "Training loss: 0.08047729730606079\n",
      "Training loss: 0.0909743383526802\n",
      "Training loss: 0.0915297418832779\n",
      "Training loss: 0.08401579409837723\n",
      "Training loss: 0.08722422271966934\n",
      "Training loss: 0.0824655294418335\n",
      "Training loss: 0.0864177942276001\n",
      "Training loss: 0.0857040286064148\n",
      "Training loss: 0.08905892819166183\n",
      "Training loss: 0.07890079915523529\n",
      "Training loss: 0.0895053818821907\n",
      "Training loss: 0.077900730073452\n",
      "Training loss: 0.09082577377557755\n",
      "Training loss: 0.09074196219444275\n",
      "Training loss: 0.07884886115789413\n",
      "Training loss: 0.08194396644830704\n",
      "Training loss: 0.08587167412042618\n",
      "Training loss: 0.07710740715265274\n",
      "Training loss: 0.07915642857551575\n",
      "Training loss: 0.09017346054315567\n",
      "Training loss: 0.0834723711013794\n",
      "Training loss: 0.08196165412664413\n",
      "Training loss: 0.08679605275392532\n",
      "Training loss: 0.08357710391283035\n",
      "Training loss: 0.08265019208192825\n",
      "Training loss: 0.08002405613660812\n",
      "Training loss: 0.08526686578989029\n",
      "Training loss: 0.08396125584840775\n",
      "Training loss: 0.07807821780443192\n",
      "Training loss: 0.07970065623521805\n",
      "Training loss: 0.0815628245472908\n",
      "Training loss: 0.08598975837230682\n",
      "Training loss: 0.07901694625616074\n",
      "Training loss: 0.08388972282409668\n",
      "Training loss: 0.08420465886592865\n",
      "Training loss: 0.0862267017364502\n",
      "Training loss: 0.08129452913999557\n",
      "Training loss: 0.0832749456167221\n",
      "Training loss: 0.0834917277097702\n",
      "Training loss: 0.08384454250335693\n",
      "Training loss: 0.09003177285194397\n",
      "Training loss: 0.0922975167632103\n",
      "Training loss: 0.08410673588514328\n",
      "Training loss: 0.08768467605113983\n",
      "Training loss: 0.08714579790830612\n",
      "Training loss: 0.0818692296743393\n",
      "Training loss: 0.08548254519701004\n",
      "Training loss: 0.08316962420940399\n",
      "Training loss: 0.07998862117528915\n",
      "Training loss: 0.0790211632847786\n",
      "Training loss: 0.08105544000864029\n",
      "Training loss: 0.0864369124174118\n",
      "Training loss: 0.07843469083309174\n",
      "Training loss: 0.08273787051439285\n",
      "Training loss: 0.08751051872968674\n",
      "Training loss: 0.08710186183452606\n",
      "Training loss: 0.08773855119943619\n",
      "Training loss: 0.07824591547250748\n",
      "Training loss: 0.07693523168563843\n",
      "Training loss: 0.09293224662542343\n",
      "Training loss: 0.0868697464466095\n",
      "Training loss: 0.07853703200817108\n",
      "Training loss: 0.0872553288936615\n",
      "Training loss: 0.0842997208237648\n",
      "Training loss: 0.0849468857049942\n",
      "Training loss: 0.09191038459539413\n",
      "Training loss: 0.0865560993552208\n",
      "Training loss: 0.08624874800443649\n",
      "Training loss: 0.08584524691104889\n",
      "Training loss: 0.08049456775188446\n",
      "Training loss: 0.08293859660625458\n",
      "Training loss: 0.07715176790952682\n",
      "Training loss: 0.08115039765834808\n",
      "Training loss: 0.0785938948392868\n",
      "Training loss: 0.08789034932851791\n",
      "Training loss: 0.0837395116686821\n",
      "Training loss: 0.0816587284207344\n",
      "Training loss: 0.07885785400867462\n",
      "Training loss: 0.08242085576057434\n",
      "Training loss: 0.08205686509609222\n",
      "Training loss: 0.0788639485836029\n",
      "Training loss: 0.08068645745515823\n",
      "Training loss: 0.08483341336250305\n",
      "Training loss: 0.08132196962833405\n",
      "Training loss: 0.08050910383462906\n",
      "Training loss: 0.0816577598452568\n",
      "Training loss: 0.08232708275318146\n",
      "Training loss: 0.07939688861370087\n",
      "Training loss: 0.07839478552341461\n",
      "Training loss: 0.08078116178512573\n",
      "Training loss: 0.09427643567323685\n",
      "Training loss: 0.07590150833129883\n",
      "Training loss: 0.0815972313284874\n",
      "Training loss: 0.09063665568828583\n",
      "Training loss: 0.0832226425409317\n",
      "Training loss: 0.07905266433954239\n",
      "Training loss: 0.09938943386077881\n",
      "Training loss: 0.09223315864801407\n",
      "Training loss: 0.08768884092569351\n",
      "Training loss: 0.07680061459541321\n",
      "Training loss: 0.0822412297129631\n",
      "Training loss: 0.09009493142366409\n",
      "Training loss: 0.08400632441043854\n",
      "Training loss: 0.08230210840702057\n",
      "Training loss: 0.07826797664165497\n",
      "Training loss: 0.08536094427108765\n",
      "Training loss: 0.08037134259939194\n",
      "Training loss: 0.08388546854257584\n",
      "Training loss: 0.08150327950716019\n",
      "Training loss: 0.08558164536952972\n",
      "Training loss: 0.08539650589227676\n",
      "Training loss: 0.08987338840961456\n",
      "Training loss: 0.08359035849571228\n",
      "Training loss: 0.08075672388076782\n",
      "Training loss: 0.08658745884895325\n",
      "Training loss: 0.0871562585234642\n",
      "Training loss: 0.08152464777231216\n",
      "Training loss: 0.07970388978719711\n",
      "Training loss: 0.07651782035827637\n",
      "Training loss: 0.08532685041427612\n",
      "Training loss: 0.07519416511058807\n",
      "Training loss: 0.07928724586963654\n",
      "Training loss: 0.07586993277072906\n",
      "Training loss: 0.2695331871509552\n",
      "Training loss: 0.08408831059932709\n",
      "Training loss: 0.080755315721035\n",
      "Training loss: 0.08267852663993835\n",
      "Training loss: 0.08357477933168411\n",
      "Training loss: 0.08364398032426834\n",
      "Training loss: 0.08843999356031418\n",
      "Training loss: 0.0816909670829773\n",
      "Training loss: 0.08339311182498932\n",
      "Training loss: 0.08267903327941895\n",
      "Training loss: 0.08548372983932495\n",
      "Training loss: 0.08501407504081726\n",
      "Training loss: 0.0891035944223404\n",
      "Training loss: 0.09202056378126144\n",
      "Training loss: 0.08956482261419296\n",
      "Training loss: 0.08107687532901764\n",
      "Training loss: 0.08926708996295929\n",
      "Training loss: 0.0924391895532608\n",
      "Training loss: 0.08994488418102264\n",
      "Training loss: 0.081688292324543\n",
      "Training loss: 0.08244556188583374\n",
      "Training loss: 0.08308666199445724\n",
      "Training loss: 0.08788923174142838\n",
      "Training loss: 0.09135161340236664\n",
      "Training loss: 0.0847667008638382\n",
      "Training loss: 0.08541662245988846\n",
      "Training loss: 0.08607422560453415\n",
      "Training loss: 0.08177240192890167\n",
      "Training loss: 0.08332107961177826\n",
      "Training loss: 0.08964875340461731\n",
      "Training loss: 0.08558253943920135\n",
      "Training loss: 0.08751652389764786\n",
      "Training loss: 0.08277744799852371\n",
      "Training loss: 0.08965276926755905\n",
      "Training loss: 0.0864131823182106\n",
      "Training loss: 0.08381542563438416\n",
      "Training loss: 0.08260396867990494\n",
      "Training loss: 0.08244027197360992\n",
      "Training loss: 0.0806422159075737\n",
      "Training loss: 0.08458027243614197\n",
      "Training loss: 0.08971468359231949\n",
      "Training loss: 0.0834851786494255\n",
      "Training loss: 0.07766947150230408\n",
      "Training loss: 0.08887382596731186\n",
      "Training loss: 0.08730804920196533\n",
      "Training loss: 0.08535309880971909\n",
      "Training loss: 0.08289525657892227\n",
      "Training loss: 0.08428585529327393\n",
      "Training loss: 0.08251185715198517\n",
      "Training loss: 0.08805734664201736\n",
      "Training loss: 0.08120672404766083\n",
      "Training loss: 0.08459403365850449\n",
      "Training loss: 0.08509572595357895\n",
      "Training loss: 0.08371495455503464\n",
      "Training loss: 0.08787950128316879\n",
      "Training loss: 0.08514337986707687\n",
      "Training loss: 0.08706998080015182\n",
      "Training loss: 0.08734850585460663\n",
      "Training loss: 0.08846971392631531\n",
      "Training loss: 0.08036002516746521\n",
      "Training loss: 0.08988258242607117\n",
      "Training loss: 0.08806919306516647\n",
      "Training loss: 0.08933202177286148\n",
      "Training loss: 0.09056635946035385\n",
      "Training loss: 0.08272945880889893\n",
      "Training loss: 0.08731444925069809\n",
      "Training loss: 0.08436790108680725\n",
      "Training loss: 0.08330037444829941\n",
      "Training loss: 0.08120526373386383\n",
      "Training loss: 0.08798453211784363\n",
      "Training loss: 0.08467700332403183\n",
      "Training loss: 0.0854618176817894\n",
      "Training loss: 0.08419864624738693\n",
      "Training loss: 0.08668459206819534\n",
      "Training loss: 0.08139616250991821\n",
      "Training loss: 0.08803538978099823\n",
      "Training loss: 0.08997975289821625\n",
      "Training loss: 0.08725550025701523\n",
      "Training loss: 0.08417369425296783\n",
      "Training loss: 0.0833020880818367\n",
      "Training loss: 0.097064308822155\n",
      "Training loss: 0.08311118930578232\n",
      "Training loss: 0.07754179835319519\n",
      "Training loss: 0.09912662208080292\n",
      "Training loss: 0.0955030545592308\n",
      "Training loss: 0.08813348412513733\n",
      "Training loss: 0.08431451767683029\n",
      "Training loss: 0.08422629535198212\n",
      "Training loss: 0.0867181271314621\n",
      "Training loss: 0.078243188560009\n",
      "Training loss: 0.08328420668840408\n",
      "Training loss: 0.0878288671374321\n",
      "Training loss: 0.085403673350811\n",
      "Training loss: 0.08162834495306015\n",
      "Training loss: 0.09044791758060455\n",
      "Training loss: 0.0901007279753685\n",
      "Training loss: 0.09346547722816467\n",
      "Training loss: 0.08608082681894302\n",
      "Training loss: 0.08455199748277664\n",
      "Training loss: 0.09011072665452957\n",
      "Training loss: 0.08994196355342865\n",
      "Training loss: 0.09825354814529419\n",
      "Training loss: 0.08303996920585632\n",
      "Training loss: 0.08847183734178543\n",
      "Training loss: 0.08509368449449539\n",
      "Training loss: 0.08123329281806946\n",
      "Training loss: 0.08444622904062271\n",
      "Training loss: 0.09040381014347076\n",
      "Training loss: 0.08721345663070679\n",
      "Training loss: 0.084398552775383\n",
      "Training loss: 0.07993580400943756\n",
      "Training loss: 0.09049005806446075\n",
      "Training loss: 0.0836465060710907\n",
      "Training loss: 0.08673480153083801\n",
      "Training loss: 0.09002576023340225\n",
      "Training loss: 0.08929971605539322\n",
      "Training loss: 0.08535084873437881\n",
      "Training loss: 0.07946359366178513\n",
      "Training loss: 0.08558586239814758\n",
      "Training loss: 0.08284188061952591\n",
      "Training loss: 0.0802600309252739\n",
      "Training loss: 0.08443821966648102\n",
      "Training loss: 0.08443519473075867\n",
      "Training loss: 0.08349408209323883\n",
      "Training loss: 0.08462993055582047\n",
      "Training loss: 0.08101251721382141\n",
      "Training loss: 0.0834130197763443\n",
      "Training loss: 0.07863857597112656\n",
      "Training loss: 0.08566094934940338\n",
      "Training loss: 0.0771787092089653\n",
      "Training loss: 0.08403056114912033\n",
      "Training loss: 0.08909884840250015\n",
      "Training loss: 0.0893961489200592\n",
      "Training loss: 0.07970590144395828\n",
      "Training loss: 0.08150926977396011\n",
      "Training loss: 0.0792599692940712\n",
      "Training loss: 0.08660757541656494\n",
      "Training loss: 0.08266034722328186\n",
      "Training loss: 0.08161544054746628\n",
      "Training loss: 0.08181802183389664\n",
      "Training loss: 0.07902782410383224\n",
      "Training loss: 0.08584026992321014\n",
      "Training loss: 0.0933147743344307\n",
      "Training loss: 0.08880680054426193\n",
      "Training loss: 0.08585833013057709\n",
      "Training loss: 0.08721110969781876\n",
      "Training loss: 0.08631958812475204\n",
      "Training loss: 0.0860041156411171\n",
      "Training loss: 0.07991498708724976\n",
      "Training loss: 0.08543629944324493\n",
      "Training loss: 0.07824637740850449\n",
      "Training loss: 0.08288007974624634\n",
      "Training loss: 0.08918255567550659\n",
      "Training loss: 0.08531608432531357\n",
      "Training loss: 0.08240391314029694\n",
      "Training loss: 0.08324188739061356\n",
      "Training loss: 0.0855034738779068\n",
      "Training loss: 0.08388359844684601\n",
      "Training loss: 0.0817359909415245\n",
      "Training loss: 0.08390021324157715\n",
      "Training loss: 0.08472014963626862\n",
      "Training loss: 0.08426575362682343\n",
      "Training loss: 0.07855860143899918\n",
      "Training loss: 0.08809658885002136\n",
      "Training loss: 0.0825880914926529\n",
      "Training loss: 0.07573632895946503\n",
      "Training loss: 0.08229681104421616\n",
      "Training loss: 0.08667590469121933\n",
      "Training loss: 0.0846913605928421\n",
      "Training loss: 0.08135561645030975\n",
      "Training loss: 0.0784066691994667\n",
      "Training loss: 0.07731302827596664\n",
      "Training loss: 0.08798881620168686\n",
      "Training loss: 0.08953636139631271\n",
      "Training loss: 0.07762739807367325\n",
      "Training loss: 0.08224530518054962\n",
      "Training loss: 0.07867453992366791\n",
      "Training loss: 0.08277270942926407\n",
      "Training loss: 0.0787799209356308\n",
      "Training loss: 0.08562830090522766\n",
      "Training loss: 0.0833163931965828\n",
      "Training loss: 0.07714438438415527\n",
      "Training loss: 0.08638479560613632\n",
      "Training loss: 0.08131780475378036\n",
      "Training loss: 0.0807342380285263\n",
      "Training loss: 0.08202821761369705\n",
      "Training loss: 0.07481829077005386\n",
      "Training loss: 0.08092077076435089\n",
      "Training loss: 0.08048088848590851\n",
      "Training loss: 0.08555936068296432\n",
      "Training loss: 0.08255360275506973\n",
      "Training loss: 0.08283617347478867\n",
      "Training loss: 0.08108150213956833\n",
      "Training loss: 0.0833808034658432\n",
      "Training loss: 0.08221466839313507\n",
      "Training loss: 0.07769086956977844\n",
      "Training loss: 0.08159694075584412\n",
      "Training loss: 0.0815991461277008\n",
      "Training loss: 0.08304587006568909\n",
      "Training loss: 0.08512484282255173\n",
      "Training loss: 0.07860754430294037\n",
      "Training loss: 0.08075622469186783\n",
      "Training loss: 0.08391804248094559\n",
      "Training loss: 0.07988519966602325\n",
      "Training loss: 0.08080887049436569\n",
      "Training loss: 0.07974845170974731\n",
      "Training loss: 0.09183133393526077\n",
      "Training loss: 0.08108209073543549\n",
      "Training loss: 0.08865353465080261\n",
      "Training loss: 0.07673416286706924\n",
      "Training loss: 0.07383162528276443\n",
      "Training loss: 0.07829132676124573\n",
      "Training loss: 0.08437374979257584\n",
      "Training loss: 0.07620230317115784\n",
      "Training loss: 0.08402025699615479\n",
      "Training loss: 0.08306407928466797\n",
      "Training loss: 0.09178189188241959\n",
      "Training loss: 0.0767100378870964\n",
      "Training loss: 0.07851047068834305\n",
      "Training loss: 0.08672391623258591\n",
      "Training loss: 0.08153088390827179\n",
      "Training loss: 0.08448125422000885\n",
      "Training loss: 0.08532796800136566\n",
      "Training loss: 0.08251932263374329\n",
      "Training loss: 0.08066723495721817\n",
      "Training loss: 0.0777926966547966\n",
      "Training loss: 0.08760884404182434\n",
      "Training loss: 0.0853780210018158\n",
      "Training loss: 0.09034654498100281\n",
      "Training loss: 0.0793120339512825\n",
      "Training loss: 0.0841488242149353\n",
      "Training loss: 0.10082048177719116\n",
      "Training loss: 0.0774713084101677\n",
      "Training loss: 0.07892916351556778\n",
      "Training loss: 0.08155502378940582\n",
      "Training loss: 0.08536674082279205\n",
      "Training loss: 0.09176005423069\n",
      "Training loss: 0.08484798669815063\n",
      "Training loss: 0.09180839359760284\n",
      "Training loss: 0.09342214465141296\n",
      "Training loss: 0.07922811061143875\n",
      "Training loss: 0.08164314180612564\n",
      "Training loss: 0.09062396734952927\n",
      "Training loss: 0.08701252192258835\n",
      "Training loss: 0.0864996388554573\n",
      "Training loss: 0.08411774784326553\n",
      "Training loss: 0.08811566233634949\n",
      "Training loss: 0.09019935131072998\n",
      "Training loss: 0.08956050127744675\n",
      "Training loss: 0.08487061411142349\n",
      "Training loss: 0.08751590549945831\n",
      "Training loss: 0.08863720297813416\n",
      "Training loss: 0.08239235728979111\n",
      "Training loss: 0.08872080594301224\n",
      "Training loss: 0.08942520618438721\n",
      "Training loss: 0.08522140979766846\n",
      "Training loss: 0.08395155519247055\n",
      "Training loss: 0.09283957630395889\n",
      "Training loss: 0.08289717137813568\n",
      "Training loss: 0.08887777477502823\n",
      "Training loss: 0.09206389635801315\n",
      "Training loss: 0.08636970818042755\n",
      "Training loss: 0.08310127258300781\n",
      "Training loss: 0.08221845328807831\n",
      "Training loss: 0.08303114771842957\n",
      "Training loss: 0.08405320346355438\n",
      "Training loss: 0.07665881514549255\n",
      "Training loss: 0.0808078944683075\n",
      "Training loss: 0.0816602036356926\n",
      "Training loss: 0.08414726704359055\n",
      "Training loss: 0.08233683556318283\n",
      "Training loss: 0.08649390935897827\n",
      "Training loss: 0.08183998614549637\n",
      "Training loss: 0.07869867980480194\n",
      "Training loss: 0.08352003991603851\n",
      "Training loss: 0.09319328516721725\n",
      "Training loss: 0.08197051286697388\n",
      "Training loss: 0.08634605258703232\n",
      "Training loss: 0.0801459401845932\n",
      "Training loss: 0.07982850074768066\n",
      "Training loss: 0.07731226831674576\n",
      "Training loss: 0.08682264387607574\n",
      "Training loss: 0.08615516871213913\n",
      "Training loss: 0.07896307855844498\n",
      "Training loss: 0.08115527778863907\n",
      "Training loss: 0.09466323256492615\n",
      "Training loss: 0.08716047555208206\n",
      "Training loss: 0.09253290295600891\n",
      "Training loss: 0.08140043914318085\n",
      "Training loss: 0.08524084091186523\n",
      "Training loss: 0.08523990958929062\n",
      "Training loss: 0.08220724016427994\n",
      "Training loss: 0.08031333982944489\n",
      "Training loss: 0.09601372480392456\n",
      "Training loss: 0.08135165274143219\n",
      "Training loss: 0.07902048528194427\n",
      "Training loss: 0.08219394832849503\n",
      "Training loss: 0.08670399338006973\n",
      "Training loss: 0.08260969072580338\n",
      "Training loss: 0.08123175799846649\n",
      "Training loss: 0.08257914334535599\n",
      "Training loss: 0.08514047414064407\n",
      "Training loss: 0.08872227370738983\n",
      "Training loss: 0.07859352976083755\n",
      "Training loss: 0.07933393120765686\n",
      "Training loss: 0.08109413832426071\n",
      "Training loss: 0.08066525310277939\n",
      "Training loss: 0.07980848848819733\n",
      "Training loss: 0.08593539893627167\n",
      "Training loss: 0.07997399568557739\n",
      "Training loss: 0.07842066884040833\n",
      "Training loss: 0.07833094149827957\n",
      "Training loss: 0.08754511922597885\n",
      "Training loss: 0.08848892152309418\n",
      "Training loss: 0.08966439962387085\n",
      "Training loss: 0.0856899693608284\n",
      "Training loss: 0.0784720629453659\n",
      "Training loss: 0.0796012431383133\n",
      "Training loss: 0.0794081762433052\n",
      "Training loss: 0.08347321301698685\n",
      "Training loss: 0.08489944040775299\n",
      "Training loss: 0.08633065223693848\n",
      "Training loss: 0.08139478415250778\n",
      "Training loss: 0.0795992836356163\n",
      "Training loss: 0.09385506808757782\n",
      "Training loss: 0.08270728588104248\n",
      "Training loss: 0.08688129484653473\n",
      "Training loss: 0.08774807304143906\n",
      "Training loss: 0.08388486504554749\n",
      "Training loss: 0.08682309091091156\n",
      "Training loss: 0.08721345663070679\n",
      "Training loss: 0.08425915986299515\n",
      "Training loss: 0.0853615403175354\n",
      "Training loss: 0.0859595388174057\n",
      "Training loss: 0.08718067407608032\n",
      "Training loss: 0.08757234364748001\n",
      "Training loss: 0.08038009703159332\n",
      "Training loss: 0.07705572247505188\n",
      "Training loss: 0.07929035276174545\n",
      "Training loss: 0.07753591239452362\n",
      "Training loss: 0.08087412267923355\n",
      "Training loss: 0.0751459151506424\n",
      "Training loss: 0.09202291071414948\n",
      "Training loss: 0.0834183320403099\n",
      "Training loss: 0.08264339715242386\n",
      "Training loss: 0.07401348650455475\n",
      "Training loss: 0.08001518994569778\n",
      "Training loss: 0.07630394399166107\n",
      "Training loss: 0.08261249214410782\n",
      "Training loss: 0.08051692694425583\n",
      "Training loss: 0.0808432325720787\n",
      "Training loss: 0.08209706097841263\n",
      "Training loss: 0.0800326019525528\n",
      "Training loss: 0.07739537209272385\n",
      "Training loss: 0.08484603464603424\n",
      "Training loss: 0.0826142430305481\n",
      "Training loss: 0.07441765069961548\n",
      "Training loss: 0.08378374576568604\n",
      "Training loss: 0.07499878108501434\n",
      "Training loss: 0.07807941734790802\n",
      "Training loss: 0.08578794449567795\n",
      "Training loss: 0.07927650958299637\n",
      "Training loss: 0.0746714249253273\n",
      "Training loss: 0.08164505660533905\n",
      "Training loss: 0.09032730758190155\n",
      "Training loss: 0.0788315162062645\n",
      "Training loss: 0.0850946381688118\n",
      "Training loss: 0.0808112770318985\n",
      "Training loss: 0.0868794322013855\n",
      "Training loss: 0.07733400166034698\n",
      "Training loss: 0.08525236696004868\n",
      "Training loss: 0.0878259539604187\n",
      "Training loss: 0.08224540203809738\n",
      "Training loss: 0.08342441916465759\n",
      "Training loss: 0.0793759897351265\n",
      "Training loss: 0.08213689178228378\n",
      "Training loss: 0.08313398063182831\n",
      "Training loss: 0.08231881260871887\n",
      "Training loss: 0.08514615893363953\n",
      "Training loss: 0.08488404750823975\n",
      "Training loss: 0.0825405940413475\n",
      "Training loss: 0.08652567863464355\n",
      "Training loss: 0.08412306010723114\n",
      "Training loss: 0.07531070709228516\n",
      "Training loss: 0.0902768149971962\n",
      "Training loss: 0.07964381575584412\n",
      "Training loss: 0.0801633894443512\n",
      "Training loss: 0.07531683892011642\n",
      "Training loss: 0.0887681320309639\n",
      "Training loss: 0.09103617072105408\n",
      "Training loss: 0.08644912391901016\n",
      "Training loss: 0.08011388778686523\n",
      "Training loss: 0.08755537867546082\n",
      "Training loss: 0.07854795455932617\n",
      "Training loss: 0.08870630711317062\n",
      "Training loss: 0.08306077867746353\n",
      "Training loss: 0.07856150716543198\n",
      "Training loss: 0.08223831653594971\n",
      "Training loss: 0.08514557778835297\n",
      "Training loss: 0.07620356231927872\n",
      "Training loss: 0.07771813124418259\n",
      "Training loss: 0.08710746467113495\n",
      "Training loss: 0.0801529735326767\n",
      "Training loss: 0.08572928607463837\n",
      "Training loss: 0.08415812253952026\n",
      "Training loss: 0.08306171745061874\n",
      "Training loss: 0.07968129962682724\n",
      "Training loss: 0.08705717325210571\n",
      "Training loss: 0.08499494194984436\n",
      "Training loss: 0.07933557778596878\n",
      "Training loss: 0.08093446493148804\n",
      "Training loss: 0.0835118368268013\n",
      "Training loss: 0.0888010561466217\n",
      "Training loss: 0.08362139761447906\n",
      "Training loss: 0.09050777554512024\n",
      "Training loss: 0.08268199115991592\n",
      "Training loss: 0.08649025857448578\n",
      "Training loss: 0.08713646233081818\n",
      "Training loss: 0.0869312733411789\n",
      "Training loss: 0.08831723779439926\n",
      "Training loss: 0.07999547570943832\n",
      "Training loss: 0.07984025031328201\n",
      "Training loss: 0.07810381799936295\n",
      "Training loss: 0.08306591957807541\n",
      "Training loss: 0.07769805192947388\n",
      "Training loss: 0.07546432316303253\n",
      "Training loss: 0.07759910821914673\n",
      "Training loss: 0.07861454039812088\n",
      "Training loss: 0.08357945084571838\n",
      "Training loss: 0.07774180173873901\n",
      "Training loss: 0.07598141580820084\n",
      "Training loss: 0.07921187579631805\n",
      "Training loss: 0.08225651830434799\n",
      "Training loss: 0.07893868535757065\n",
      "Training loss: 0.08855405449867249\n",
      "Training loss: 0.08683034777641296\n",
      "Training loss: 0.07855498790740967\n",
      "Training loss: 0.07627233117818832\n",
      "Training loss: 0.08778803050518036\n",
      "Training loss: 0.08174508810043335\n",
      "Training loss: 0.08371816575527191\n",
      "Training loss: 0.0879940390586853\n",
      "Training loss: 0.08767073601484299\n",
      "Training loss: 0.08490949869155884\n",
      "Training loss: 0.07849420607089996\n",
      "Training loss: 0.08127181231975555\n",
      "Training loss: 0.07772701233625412\n",
      "Training loss: 0.08374593406915665\n",
      "Training loss: 0.09035789966583252\n",
      "Training loss: 0.08792319148778915\n",
      "Training loss: 0.08371911942958832\n",
      "Training loss: 0.08305543661117554\n",
      "Training loss: 0.08934629708528519\n",
      "Training loss: 0.09370625764131546\n",
      "Training loss: 0.08026173710823059\n",
      "Training loss: 0.09758938103914261\n",
      "Training loss: 0.0842052474617958\n",
      "Training loss: 0.08532951772212982\n",
      "Training loss: 0.08155778050422668\n",
      "Training loss: 0.08341019600629807\n",
      "Training loss: 0.08337195217609406\n",
      "Training loss: 0.09327078610658646\n",
      "Training loss: 0.09025292843580246\n",
      "Training loss: 0.0939771756529808\n",
      "Training loss: 0.08313615620136261\n",
      "Training loss: 0.083584263920784\n",
      "Training loss: 0.08788526803255081\n",
      "Training loss: 0.08817482739686966\n",
      "Training loss: 0.09174766391515732\n",
      "Training loss: 0.08420167863368988\n",
      "Training loss: 0.08479344099760056\n",
      "Training loss: 0.08303418755531311\n",
      "Training loss: 0.08309120684862137\n",
      "Training loss: 0.0809202715754509\n",
      "Training loss: 0.08592738211154938\n",
      "Training loss: 0.08610858023166656\n",
      "Training loss: 0.0891476422548294\n",
      "Training loss: 0.08688322454690933\n",
      "Training loss: 0.08779111504554749\n",
      "Training loss: 0.09432601928710938\n",
      "Training loss: 0.08313166350126266\n",
      "Training loss: 0.08539294451475143\n",
      "Training loss: 0.08598881959915161\n",
      "Training loss: 0.07805696129798889\n",
      "Training loss: 0.07661518454551697\n",
      "Training loss: 0.0887921005487442\n",
      "Training loss: 0.08520805835723877\n",
      "Training loss: 0.08449607342481613\n",
      "Training loss: 0.08058609813451767\n",
      "Training loss: 0.07955663651227951\n",
      "Training loss: 0.07998963445425034\n",
      "Training loss: 0.09879027307033539\n",
      "Training loss: 0.08162082731723785\n",
      "Training loss: 0.08584342896938324\n",
      "Training loss: 0.09365188330411911\n",
      "Training loss: 0.0870312750339508\n",
      "Training loss: 0.08404509723186493\n",
      "Training loss: 0.09671466052532196\n",
      "Training loss: 0.08983942121267319\n",
      "Training loss: 0.08156508207321167\n",
      "Training loss: 0.0898311659693718\n",
      "Training loss: 0.09563520550727844\n",
      "Training loss: 0.0929635763168335\n",
      "Training loss: 0.08933912217617035\n",
      "Training loss: 0.08967515826225281\n",
      "Training loss: 0.08306188881397247\n",
      "Training loss: 0.07716930657625198\n",
      "Training loss: 0.08715260773897171\n",
      "Training loss: 0.09169503301382065\n",
      "Training loss: 0.07649616152048111\n",
      "Training loss: 0.08192324638366699\n",
      "Training loss: 0.07975302636623383\n",
      "Training loss: 0.08056160807609558\n",
      "Training loss: 0.08637650310993195\n",
      "Training loss: 0.08217180520296097\n",
      "Training loss: 0.07854051142930984\n",
      "Training loss: 0.08164800703525543\n",
      "Training loss: 0.0791134387254715\n",
      "Training loss: 0.08334266394376755\n",
      "Training loss: 0.07773450762033463\n",
      "Training loss: 0.07453355193138123\n",
      "Training loss: 0.07948103547096252\n",
      "Training loss: 0.07879625260829926\n",
      "Training loss: 0.08828689157962799\n",
      "Training loss: 0.08776377886533737\n",
      "Training loss: 0.07685677707195282\n",
      "Training loss: 0.08237031102180481\n",
      "Training loss: 0.07476940751075745\n",
      "Training loss: 0.0840192660689354\n",
      "Training loss: 0.07870935648679733\n",
      "Training loss: 0.08281063288450241\n",
      "Training loss: 0.07359591871500015\n",
      "Training loss: 0.08397267013788223\n",
      "Training loss: 0.08535882830619812\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    log_ps = model(X_train_tensor)\n",
    "    loss = criterion(log_ps, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Training loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b2-rnrfDh-D"
   },
   "outputs": [],
   "source": [
    "y_pred = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCR4eeAYDh-G"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5sLnH8_Dh-K"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IRTVSfcjDh-O",
    "outputId": "9361fd99-77da-4a5d-ff01-45b494a06cef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9637533875338753"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_train_tensor.to('cpu')),np.array( y_pred.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "g8tck57DDh-R",
    "outputId": "09500235-1ad2-43d8-ef97-8c022de6d711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test_np = preprocessor.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGooGhsZDh0k"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1330, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 10)\n",
    "        self.fc7 = nn.Linear(10, 3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened        \n",
    "        x = self.dropout(F.leaky_relu(self.fc1(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc2(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc3(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc4(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc5(x)))\n",
    "        x = self.dropout(F.leaky_relu(self.fc6(x)))\n",
    "        x = F.log_softmax(self.fc7(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfKTwrCoDh03"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y81Yu40xDh1C"
   },
   "outputs": [],
   "source": [
    "model = Classifier().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7eMDywHDh1I"
   },
   "outputs": [],
   "source": [
    "X_train_np = preprocessor.fit_transform(train_final.drop('outage_duration',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-R2LXEKDDh1M"
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train_tensor =  torch.Tensor(train_final.outage_duration.values).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xeBDoLwHDh1Q",
    "outputId": "8e45d594-bef7-471b-c57c-ff772adc72b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training loss: 0.1354953646659851\n",
      "Training loss: 0.14191880822181702\n",
      "Training loss: 0.1379302740097046\n",
      "Training loss: 0.1288977414369583\n",
      "Training loss: 0.13673637807369232\n",
      "Training loss: 0.13814379274845123\n",
      "Training loss: 0.13252395391464233\n",
      "Training loss: 0.13412542641162872\n",
      "Training loss: 0.1537761241197586\n",
      "Training loss: 0.13998520374298096\n",
      "Training loss: 0.13475528359413147\n",
      "Training loss: 0.14020457863807678\n",
      "Training loss: 0.13688607513904572\n",
      "Training loss: 0.14135347306728363\n",
      "Training loss: 0.1372460573911667\n",
      "Training loss: 0.13814012706279755\n",
      "Training loss: 0.1364450603723526\n",
      "Training loss: 0.14482347667217255\n",
      "Training loss: 0.1350596398115158\n",
      "Training loss: 0.13199393451213837\n",
      "Training loss: 0.1300664097070694\n",
      "Training loss: 0.13455703854560852\n",
      "Training loss: 0.14048106968402863\n",
      "Training loss: 0.12928713858127594\n",
      "Training loss: 0.13232630491256714\n",
      "Training loss: 0.13475997745990753\n",
      "Training loss: 0.13141265511512756\n",
      "Training loss: 0.15862718224525452\n",
      "Training loss: 0.13904887437820435\n",
      "Training loss: 0.13356728851795197\n",
      "Training loss: 0.13874833285808563\n",
      "Training loss: 0.13147325813770294\n",
      "Training loss: 0.1334504932165146\n",
      "Training loss: 0.13461799919605255\n",
      "Training loss: 0.13095888495445251\n",
      "Training loss: 0.1357051283121109\n",
      "Training loss: 0.12946422398090363\n",
      "Training loss: 0.13522222638130188\n",
      "Training loss: 0.13005681335926056\n",
      "Training loss: 0.12851937115192413\n",
      "Training loss: 0.1393972635269165\n",
      "Training loss: 0.12586429715156555\n",
      "Training loss: 0.13493278622627258\n",
      "Training loss: 0.13472986221313477\n",
      "Training loss: 0.14480061829090118\n",
      "Training loss: 0.13161800801753998\n",
      "Training loss: 0.1315358728170395\n",
      "Training loss: 0.12835043668746948\n",
      "Training loss: 0.12778626382350922\n",
      "Training loss: 0.13147254288196564\n",
      "Training loss: 0.13099609315395355\n",
      "Training loss: 0.13068881630897522\n",
      "Training loss: 0.13616971671581268\n",
      "Training loss: 0.13634036481380463\n",
      "Training loss: 0.13408784568309784\n",
      "Training loss: 0.1423463523387909\n",
      "Training loss: 0.14730305969715118\n",
      "Training loss: 0.14197172224521637\n",
      "Training loss: 0.13556234538555145\n",
      "Training loss: 0.14508499205112457\n",
      "Training loss: 0.13306567072868347\n",
      "Training loss: 0.13423247635364532\n",
      "Training loss: 0.1385420858860016\n",
      "Training loss: 0.13013865053653717\n",
      "Training loss: 0.13797110319137573\n",
      "Training loss: 0.14553101360797882\n",
      "Training loss: 0.13591334223747253\n",
      "Training loss: 0.13510456681251526\n",
      "Training loss: 0.14494390785694122\n",
      "Training loss: 0.13778960704803467\n",
      "Training loss: 0.1338006556034088\n",
      "Training loss: 0.13385659456253052\n",
      "Training loss: 0.1302381008863449\n",
      "Training loss: 0.13486535847187042\n",
      "Training loss: 0.13526375591754913\n",
      "Training loss: 0.13407258689403534\n",
      "Training loss: 0.13948020339012146\n",
      "Training loss: 0.13799451291561127\n",
      "Training loss: 0.1264033019542694\n",
      "Training loss: 0.1337764710187912\n",
      "Training loss: 0.14087414741516113\n",
      "Training loss: 0.1347460001707077\n",
      "Training loss: 0.12550385296344757\n",
      "Training loss: 0.130305215716362\n",
      "Training loss: 0.12773846089839935\n",
      "Training loss: 0.13028542697429657\n",
      "Training loss: 0.12504325807094574\n",
      "Training loss: 0.1383732259273529\n",
      "Training loss: 0.13300564885139465\n",
      "Training loss: 0.13044312596321106\n",
      "Training loss: 0.14195924997329712\n",
      "Training loss: 0.12771199643611908\n",
      "Training loss: 0.13519492745399475\n",
      "Training loss: 0.12786231935024261\n",
      "Training loss: 0.1365930438041687\n",
      "Training loss: 0.134329691529274\n",
      "Training loss: 0.13777683675289154\n",
      "Training loss: 0.12803910672664642\n",
      "Training loss: 0.13877522945404053\n",
      "Training loss: 0.13059474527835846\n",
      "Training loss: 0.1358024775981903\n",
      "Training loss: 0.13175101578235626\n",
      "Training loss: 0.1329725980758667\n",
      "Training loss: 0.12829747796058655\n",
      "Training loss: 0.13226532936096191\n",
      "Training loss: 0.13043493032455444\n",
      "Training loss: 0.13081800937652588\n",
      "Training loss: 0.14406085014343262\n",
      "Training loss: 0.13484260439872742\n",
      "Training loss: 0.13332706689834595\n",
      "Training loss: 0.1360761970281601\n",
      "Training loss: 0.13543270528316498\n",
      "Training loss: 0.1270529329776764\n",
      "Training loss: 0.1296338587999344\n",
      "Training loss: 0.13401320576667786\n",
      "Training loss: 0.12840773165225983\n",
      "Training loss: 0.12578284740447998\n",
      "Training loss: 0.12856584787368774\n",
      "Training loss: 0.13187648355960846\n",
      "Training loss: 0.12399190664291382\n",
      "Training loss: 0.12505410611629486\n",
      "Training loss: 0.1251591145992279\n",
      "Training loss: 0.14083899557590485\n",
      "Training loss: 0.12608209252357483\n",
      "Training loss: 0.13645043969154358\n",
      "Training loss: 0.1443122774362564\n",
      "Training loss: 0.1330590397119522\n",
      "Training loss: 0.1308247148990631\n",
      "Training loss: 0.13287587463855743\n",
      "Training loss: 0.13095803558826447\n",
      "Training loss: 0.13457423448562622\n",
      "Training loss: 0.13134202361106873\n",
      "Training loss: 0.13273024559020996\n",
      "Training loss: 0.1328064650297165\n",
      "Training loss: 0.1309683471918106\n",
      "Training loss: 0.13141071796417236\n",
      "Training loss: 0.13134291768074036\n",
      "Training loss: 0.13411536812782288\n",
      "Training loss: 0.14340975880622864\n",
      "Training loss: 0.1319563090801239\n",
      "Training loss: 0.14234808087348938\n",
      "Training loss: 0.1350516378879547\n",
      "Training loss: 0.1289324015378952\n",
      "Training loss: 0.1352728307247162\n",
      "Training loss: 0.1253218650817871\n",
      "Training loss: 0.13658994436264038\n",
      "Training loss: 0.12867820262908936\n",
      "Training loss: 0.1263294816017151\n",
      "Training loss: 0.13218143582344055\n",
      "Training loss: 0.12655311822891235\n",
      "Training loss: 0.1351432502269745\n",
      "Training loss: 0.1374858021736145\n",
      "Training loss: 0.1329174041748047\n",
      "Training loss: 0.12747368216514587\n",
      "Training loss: 0.13223831355571747\n",
      "Training loss: 0.1323518604040146\n",
      "Training loss: 0.12931609153747559\n",
      "Training loss: 0.1388850212097168\n",
      "Training loss: 0.134249746799469\n",
      "Training loss: 0.12690193951129913\n",
      "Training loss: 0.1322948932647705\n",
      "Training loss: 0.12191078811883926\n",
      "Training loss: 0.1316967010498047\n",
      "Training loss: 0.1322583705186844\n",
      "Training loss: 0.12824594974517822\n",
      "Training loss: 0.12948094308376312\n",
      "Training loss: 0.1322159320116043\n",
      "Training loss: 0.12410853803157806\n",
      "Training loss: 0.1340150237083435\n",
      "Training loss: 0.12722277641296387\n",
      "Training loss: 0.13769176602363586\n",
      "Training loss: 0.14052756130695343\n",
      "Training loss: 0.1441652625799179\n",
      "Training loss: 0.12749645113945007\n",
      "Training loss: 0.1309853047132492\n",
      "Training loss: 0.1287357360124588\n",
      "Training loss: 0.1403302252292633\n",
      "Training loss: 0.1418638527393341\n",
      "Training loss: 0.12669172883033752\n",
      "Training loss: 0.13629601895809174\n",
      "Training loss: 0.13099493086338043\n",
      "Training loss: 0.1406463384628296\n",
      "Training loss: 0.12969361245632172\n",
      "Training loss: 0.12524071335792542\n",
      "Training loss: 0.13568015396595\n",
      "Training loss: 0.12761282920837402\n",
      "Training loss: 0.13525564968585968\n",
      "Training loss: 0.13139788806438446\n",
      "Training loss: 0.1359291523694992\n",
      "Training loss: 0.12897077202796936\n",
      "Training loss: 0.13418112695217133\n",
      "Training loss: 0.13154320418834686\n",
      "Training loss: 0.13145378232002258\n",
      "Training loss: 0.12322840094566345\n",
      "Training loss: 0.126747727394104\n",
      "Training loss: 0.12482821196317673\n",
      "Training loss: 0.1289401650428772\n",
      "Training loss: 0.12945976853370667\n",
      "Training loss: 0.12423698604106903\n",
      "Training loss: 0.13188914954662323\n",
      "Training loss: 0.12376049160957336\n",
      "Training loss: 0.1273242086172104\n",
      "Training loss: 0.13912706077098846\n",
      "Training loss: 0.1305389255285263\n",
      "Training loss: 0.1219739019870758\n",
      "Training loss: 0.13579556345939636\n",
      "Training loss: 0.12758053839206696\n",
      "Training loss: 0.12144917994737625\n",
      "Training loss: 0.12621787190437317\n",
      "Training loss: 0.12431930005550385\n",
      "Training loss: 0.12383750081062317\n",
      "Training loss: 0.1254260092973709\n",
      "Training loss: 0.12344539910554886\n",
      "Training loss: 0.13261812925338745\n",
      "Training loss: 0.12817126512527466\n",
      "Training loss: 0.12632013857364655\n",
      "Training loss: 0.12648190557956696\n",
      "Training loss: 0.12510167062282562\n",
      "Training loss: 0.1212957352399826\n",
      "Training loss: 0.12466171383857727\n",
      "Training loss: 0.13398058712482452\n",
      "Training loss: 0.13823333382606506\n",
      "Training loss: 0.128540500998497\n",
      "Training loss: 0.12046784907579422\n",
      "Training loss: 0.13260024785995483\n",
      "Training loss: 0.12898695468902588\n",
      "Training loss: 0.13099415600299835\n",
      "Training loss: 0.12851379811763763\n",
      "Training loss: 0.12329208850860596\n",
      "Training loss: 0.12974005937576294\n",
      "Training loss: 0.12250536680221558\n",
      "Training loss: 0.12225805222988129\n",
      "Training loss: 0.12574824690818787\n",
      "Training loss: 0.12277578562498093\n",
      "Training loss: 0.1269412636756897\n",
      "Training loss: 0.1328282356262207\n",
      "Training loss: 0.13622771203517914\n",
      "Training loss: 0.1422649770975113\n",
      "Training loss: 0.12834255397319794\n",
      "Training loss: 0.13733404874801636\n",
      "Training loss: 0.13702981173992157\n",
      "Training loss: 0.13549336791038513\n",
      "Training loss: 0.1376493126153946\n",
      "Training loss: 0.1253310889005661\n",
      "Training loss: 0.12418147176504135\n",
      "Training loss: 0.12757249176502228\n",
      "Training loss: 0.1358708292245865\n",
      "Training loss: 0.12981656193733215\n",
      "Training loss: 0.12765280902385712\n",
      "Training loss: 0.12481185048818588\n",
      "Training loss: 0.12464411556720734\n",
      "Training loss: 0.127983957529068\n",
      "Training loss: 0.12680980563163757\n",
      "Training loss: 0.12779642641544342\n",
      "Training loss: 0.1320854276418686\n",
      "Training loss: 0.12111306190490723\n",
      "Training loss: 0.13182079792022705\n",
      "Training loss: 0.13380193710327148\n",
      "Training loss: 0.1327056884765625\n",
      "Training loss: 0.12493301182985306\n",
      "Training loss: 0.13936983048915863\n",
      "Training loss: 0.1260179877281189\n",
      "Training loss: 0.13329732418060303\n",
      "Training loss: 0.144559845328331\n",
      "Training loss: 0.1328640580177307\n",
      "Training loss: 0.13029474020004272\n",
      "Training loss: 0.12465193122625351\n",
      "Training loss: 0.1308135688304901\n",
      "Training loss: 0.1360280066728592\n",
      "Training loss: 0.12394596636295319\n",
      "Training loss: 0.12409847229719162\n",
      "Training loss: 0.1282331794500351\n",
      "Training loss: 0.14356794953346252\n",
      "Training loss: 0.1317272186279297\n",
      "Training loss: 0.1306706815958023\n",
      "Training loss: 0.13052992522716522\n",
      "Training loss: 0.1505628377199173\n",
      "Training loss: 0.12945470213890076\n",
      "Training loss: 0.12867595255374908\n",
      "Training loss: 0.13114479184150696\n",
      "Training loss: 0.12229806929826736\n",
      "Training loss: 0.1294160932302475\n",
      "Training loss: 0.12473157793283463\n",
      "Training loss: 0.12970302999019623\n",
      "Training loss: 0.12989170849323273\n",
      "Training loss: 0.12465105205774307\n",
      "Training loss: 0.13252729177474976\n",
      "Training loss: 0.13625088334083557\n",
      "Training loss: 0.1244245171546936\n",
      "Training loss: 0.12870241701602936\n",
      "Training loss: 0.13035057485103607\n",
      "Training loss: 0.12611670792102814\n",
      "Training loss: 0.12960760295391083\n",
      "Training loss: 0.12813453376293182\n",
      "Training loss: 0.14631083607673645\n",
      "Training loss: 0.12244654446840286\n",
      "Training loss: 0.12471389025449753\n",
      "Training loss: 0.1241217851638794\n",
      "Training loss: 0.1262597292661667\n",
      "Training loss: 0.1286548376083374\n",
      "Training loss: 0.1254120022058487\n",
      "Training loss: 0.12574459612369537\n",
      "Training loss: 0.12297677248716354\n",
      "Training loss: 0.13350534439086914\n",
      "Training loss: 0.12657848000526428\n",
      "Training loss: 0.13076351583003998\n",
      "Training loss: 0.12232588976621628\n",
      "Training loss: 0.11795409023761749\n",
      "Training loss: 0.13994711637496948\n",
      "Training loss: 0.12601397931575775\n",
      "Training loss: 0.12077344954013824\n",
      "Training loss: 0.1334751397371292\n",
      "Training loss: 0.12917427718639374\n",
      "Training loss: 0.12547385692596436\n",
      "Training loss: 0.12023912370204926\n",
      "Training loss: 0.1226881667971611\n",
      "Training loss: 0.12851153314113617\n",
      "Training loss: 0.11860105395317078\n",
      "Training loss: 0.12620986998081207\n",
      "Training loss: 0.139929860830307\n",
      "Training loss: 0.13055899739265442\n",
      "Training loss: 0.13771027326583862\n",
      "Training loss: 0.12417437136173248\n",
      "Training loss: 0.12198244035243988\n",
      "Training loss: 0.11976224184036255\n",
      "Training loss: 0.12315592169761658\n",
      "Training loss: 0.13564875721931458\n",
      "Training loss: 0.12801842391490936\n",
      "Training loss: 0.12744179368019104\n",
      "Training loss: 0.12985287606716156\n",
      "Training loss: 0.1295531988143921\n",
      "Training loss: 0.12813901901245117\n",
      "Training loss: 0.12624491751194\n",
      "Training loss: 0.12464886158704758\n",
      "Training loss: 0.11855652183294296\n",
      "Training loss: 0.1174175888299942\n",
      "Training loss: 0.12726618349552155\n",
      "Training loss: 0.12549619376659393\n",
      "Training loss: 0.12658415734767914\n",
      "Training loss: 0.11514683067798615\n",
      "Training loss: 0.12743230164051056\n",
      "Training loss: 0.12668494880199432\n",
      "Training loss: 0.12362882494926453\n",
      "Training loss: 0.12232242524623871\n",
      "Training loss: 0.12403728067874908\n",
      "Training loss: 0.12554407119750977\n",
      "Training loss: 0.11978168040513992\n",
      "Training loss: 0.1220986396074295\n",
      "Training loss: 0.12529443204402924\n",
      "Training loss: 0.12255280464887619\n",
      "Training loss: 0.12163840234279633\n",
      "Training loss: 0.1423317790031433\n",
      "Training loss: 0.12684482336044312\n",
      "Training loss: 0.14607015252113342\n",
      "Training loss: 0.1273842602968216\n",
      "Training loss: 0.12723349034786224\n",
      "Training loss: 0.12601912021636963\n",
      "Training loss: 0.1305098682641983\n",
      "Training loss: 0.12412460148334503\n",
      "Training loss: 0.1220116838812828\n",
      "Training loss: 0.12726502120494843\n",
      "Training loss: 0.1512029767036438\n",
      "Training loss: 0.12514643371105194\n",
      "Training loss: 0.12469056248664856\n",
      "Training loss: 0.13105957210063934\n",
      "Training loss: 0.12774460017681122\n",
      "Training loss: 0.13426697254180908\n",
      "Training loss: 0.12758557498455048\n",
      "Training loss: 0.13665229082107544\n",
      "Training loss: 0.17757511138916016\n",
      "Training loss: 0.13191434741020203\n",
      "Training loss: 0.13508811593055725\n",
      "Training loss: 0.1337895542383194\n",
      "Training loss: 0.1339111179113388\n",
      "Training loss: 0.12849825620651245\n",
      "Training loss: 0.15213105082511902\n",
      "Training loss: 0.1336691528558731\n",
      "Training loss: 0.130751371383667\n",
      "Training loss: 0.1328459233045578\n",
      "Training loss: 0.12996578216552734\n",
      "Training loss: 0.12754781544208527\n",
      "Training loss: 0.12670356035232544\n",
      "Training loss: 0.13151220977306366\n",
      "Training loss: 0.13233479857444763\n",
      "Training loss: 0.1262146383523941\n",
      "Training loss: 0.13018031418323517\n",
      "Training loss: 0.13096842169761658\n",
      "Training loss: 0.13195061683654785\n",
      "Training loss: 0.12597593665122986\n",
      "Training loss: 0.13007721304893494\n",
      "Training loss: 0.13139091432094574\n",
      "Training loss: 0.12267737835645676\n",
      "Training loss: 0.12361989915370941\n",
      "Training loss: 0.13090357184410095\n",
      "Training loss: 0.1297535002231598\n",
      "Training loss: 0.12903675436973572\n",
      "Training loss: 0.1370801031589508\n",
      "Training loss: 0.13768500089645386\n",
      "Training loss: 0.1298416256904602\n",
      "Training loss: 0.13400863111019135\n",
      "Training loss: 0.1298636496067047\n",
      "Training loss: 0.1354759931564331\n",
      "Training loss: 0.14760948717594147\n",
      "Training loss: 0.13315314054489136\n",
      "Training loss: 0.1266825795173645\n",
      "Training loss: 0.13494513928890228\n",
      "Training loss: 0.12714987993240356\n",
      "Training loss: 0.13488775491714478\n",
      "Training loss: 0.1299814134836197\n",
      "Training loss: 0.12373130023479462\n",
      "Training loss: 0.12468580156564713\n",
      "Training loss: 0.12524840235710144\n",
      "Training loss: 0.12463869154453278\n",
      "Training loss: 0.12830998003482819\n",
      "Training loss: 0.1234780102968216\n",
      "Training loss: 0.13121014833450317\n",
      "Training loss: 0.12820878624916077\n",
      "Training loss: 0.13901092112064362\n",
      "Training loss: 0.1352720558643341\n",
      "Training loss: 0.12992171943187714\n",
      "Training loss: 0.1314367651939392\n",
      "Training loss: 0.12213342636823654\n",
      "Training loss: 0.12284429371356964\n",
      "Training loss: 0.12885235249996185\n",
      "Training loss: 0.1283196061849594\n",
      "Training loss: 0.1267194151878357\n",
      "Training loss: 0.12645992636680603\n",
      "Training loss: 0.12589755654335022\n",
      "Training loss: 0.12929178774356842\n",
      "Training loss: 0.12339600920677185\n",
      "Training loss: 0.12397923320531845\n",
      "Training loss: 0.1196959912776947\n",
      "Training loss: 0.12400863319635391\n",
      "Training loss: 0.12970876693725586\n",
      "Training loss: 0.12084803730249405\n",
      "Training loss: 0.12869538366794586\n",
      "Training loss: 0.12130996584892273\n",
      "Training loss: 0.12223994731903076\n",
      "Training loss: 0.12473580241203308\n",
      "Training loss: 0.12391655147075653\n",
      "Training loss: 0.12326104938983917\n",
      "Training loss: 0.11535289138555527\n",
      "Training loss: 0.12718504667282104\n",
      "Training loss: 0.12539736926555634\n",
      "Training loss: 0.11859536916017532\n",
      "Training loss: 0.12373583763837814\n",
      "Training loss: 0.11935319006443024\n",
      "Training loss: 0.11981320381164551\n",
      "Training loss: 0.12284330278635025\n",
      "Training loss: 0.127278670668602\n",
      "Training loss: 0.12177566438913345\n",
      "Training loss: 0.12939660251140594\n",
      "Training loss: 0.11498215794563293\n",
      "Training loss: 0.12713037431240082\n",
      "Training loss: 0.12010148167610168\n",
      "Training loss: 0.12174037843942642\n",
      "Training loss: 0.1290115863084793\n",
      "Training loss: 0.13045676052570343\n",
      "Training loss: 0.12156148254871368\n",
      "Training loss: 0.12646253407001495\n",
      "Training loss: 0.12451019138097763\n",
      "Training loss: 0.13372015953063965\n",
      "Training loss: 0.12357552349567413\n",
      "Training loss: 0.12852327525615692\n",
      "Training loss: 0.12131726741790771\n",
      "Training loss: 0.1262872964143753\n",
      "Training loss: 0.12646667659282684\n",
      "Training loss: 0.12712298333644867\n",
      "Training loss: 0.12864693999290466\n",
      "Training loss: 0.12879794836044312\n",
      "Training loss: 0.12899789214134216\n",
      "Training loss: 0.1333044171333313\n",
      "Training loss: 0.13263344764709473\n",
      "Training loss: 0.12644898891448975\n",
      "Training loss: 0.1148427352309227\n",
      "Training loss: 0.12086660414934158\n",
      "Training loss: 0.13550913333892822\n",
      "Training loss: 0.11521556973457336\n",
      "Training loss: 0.1309053599834442\n",
      "Training loss: 0.125202938914299\n",
      "Training loss: 0.122235968708992\n",
      "Training loss: 0.12673959136009216\n",
      "Training loss: 0.12215389311313629\n",
      "Training loss: 0.12680071592330933\n",
      "Training loss: 0.1229015365242958\n",
      "Training loss: 0.12079177796840668\n",
      "Training loss: 0.12547168135643005\n",
      "Training loss: 0.12855474650859833\n",
      "Training loss: 0.13055719435214996\n",
      "Training loss: 0.12393561750650406\n",
      "Training loss: 0.1264255791902542\n",
      "Training loss: 0.12412257492542267\n",
      "Training loss: 0.11897594481706619\n",
      "Training loss: 0.12732970714569092\n",
      "Training loss: 0.14621330797672272\n",
      "Training loss: 0.12603245675563812\n",
      "Training loss: 0.12491638958454132\n",
      "Training loss: 0.12440560013055801\n",
      "Training loss: 0.12323158979415894\n",
      "Training loss: 0.12194578349590302\n",
      "Training loss: 0.12314897030591965\n",
      "Training loss: 0.12165208905935287\n",
      "Training loss: 0.12148257344961166\n",
      "Training loss: 0.13450650870800018\n",
      "Training loss: 0.12473054975271225\n",
      "Training loss: 0.13794437050819397\n",
      "Training loss: 0.13437947630882263\n",
      "Training loss: 0.12355735152959824\n",
      "Training loss: 0.13237901031970978\n",
      "Training loss: 0.12531349062919617\n",
      "Training loss: 0.1302105188369751\n",
      "Training loss: 0.12581372261047363\n",
      "Training loss: 0.12045000493526459\n",
      "Training loss: 0.12415630370378494\n",
      "Training loss: 0.12745846807956696\n",
      "Training loss: 0.12608923017978668\n",
      "Training loss: 0.12054101377725601\n",
      "Training loss: 0.12587149441242218\n",
      "Training loss: 0.13374072313308716\n",
      "Training loss: 0.12100524455308914\n",
      "Training loss: 0.12747187912464142\n",
      "Training loss: 0.11944467574357986\n",
      "Training loss: 0.12595976889133453\n",
      "Training loss: 0.12493659555912018\n",
      "Training loss: 0.12884992361068726\n",
      "Training loss: 0.12538886070251465\n",
      "Training loss: 0.12592577934265137\n",
      "Training loss: 0.1227152943611145\n",
      "Training loss: 0.1272045224905014\n",
      "Training loss: 0.12263787537813187\n",
      "Training loss: 0.12124894559383392\n",
      "Training loss: 0.13139881193637848\n",
      "Training loss: 0.12004884332418442\n",
      "Training loss: 0.117603600025177\n",
      "Training loss: 0.1301306188106537\n",
      "Training loss: 0.12825676798820496\n",
      "Training loss: 0.1251213252544403\n",
      "Training loss: 0.12543454766273499\n",
      "Training loss: 0.13150791823863983\n",
      "Training loss: 0.1265992373228073\n",
      "Training loss: 0.1364731639623642\n",
      "Training loss: 0.12671560049057007\n",
      "Training loss: 0.13153457641601562\n",
      "Training loss: 0.12789733707904816\n",
      "Training loss: 0.12984029948711395\n",
      "Training loss: 0.1276363581418991\n",
      "Training loss: 0.13138143718242645\n",
      "Training loss: 0.12722192704677582\n",
      "Training loss: 0.12002421915531158\n",
      "Training loss: 0.125046968460083\n",
      "Training loss: 0.13712774217128754\n",
      "Training loss: 0.11911708861589432\n",
      "Training loss: 0.12610283493995667\n",
      "Training loss: 0.12107137590646744\n",
      "Training loss: 0.13127680122852325\n",
      "Training loss: 0.1274074912071228\n",
      "Training loss: 0.11958438158035278\n",
      "Training loss: 0.12069155275821686\n",
      "Training loss: 0.12472381442785263\n",
      "Training loss: 0.1629905104637146\n",
      "Training loss: 0.134743794798851\n",
      "Training loss: 0.12742123007774353\n",
      "Training loss: 0.12305144220590591\n",
      "Training loss: 0.12978766858577728\n",
      "Training loss: 0.1253286749124527\n",
      "Training loss: 0.12828347086906433\n",
      "Training loss: 0.13305692374706268\n",
      "Training loss: 0.12725085020065308\n",
      "Training loss: 0.125821053981781\n",
      "Training loss: 0.1284748911857605\n",
      "Training loss: 0.12006442993879318\n",
      "Training loss: 0.12732857465744019\n",
      "Training loss: 0.12281932681798935\n",
      "Training loss: 0.1144605278968811\n",
      "Training loss: 0.12630948424339294\n",
      "Training loss: 0.11661851406097412\n",
      "Training loss: 0.12465528398752213\n",
      "Training loss: 0.12571187317371368\n",
      "Training loss: 0.12027276307344437\n",
      "Training loss: 0.12445725500583649\n",
      "Training loss: 0.14467816054821014\n",
      "Training loss: 0.12445291876792908\n",
      "Training loss: 0.13258421421051025\n",
      "Training loss: 0.13750475645065308\n",
      "Training loss: 0.11848483979701996\n",
      "Training loss: 0.11693432182073593\n",
      "Training loss: 0.12768080830574036\n",
      "Training loss: 0.13002125918865204\n",
      "Training loss: 0.13077102601528168\n",
      "Training loss: 0.11705000698566437\n",
      "Training loss: 0.12909752130508423\n",
      "Training loss: 0.12065289169549942\n",
      "Training loss: 0.12648743391036987\n",
      "Training loss: 0.12114250659942627\n",
      "Training loss: 0.12201372534036636\n",
      "Training loss: 0.1325603574514389\n",
      "Training loss: 0.12992540001869202\n",
      "Training loss: 0.13025444746017456\n",
      "Training loss: 0.1288376748561859\n",
      "Training loss: 0.13052073121070862\n",
      "Training loss: 0.1273868978023529\n",
      "Training loss: 0.12658292055130005\n",
      "Training loss: 0.12095052748918533\n",
      "Training loss: 0.11957232654094696\n",
      "Training loss: 0.12561386823654175\n",
      "Training loss: 0.12580464780330658\n",
      "Training loss: 0.12646716833114624\n",
      "Training loss: 0.12422247231006622\n",
      "Training loss: 0.12612377107143402\n",
      "Training loss: 0.12667584419250488\n",
      "Training loss: 0.1233842521905899\n",
      "Training loss: 0.12469060719013214\n",
      "Training loss: 0.12608857452869415\n",
      "Training loss: 0.12462704628705978\n",
      "Training loss: 0.1278323084115982\n",
      "Training loss: 0.1199045181274414\n",
      "Training loss: 0.12334346026182175\n",
      "Training loss: 0.1232926994562149\n",
      "Training loss: 0.12423797696828842\n",
      "Training loss: 0.12480684369802475\n",
      "Training loss: 0.14015473425388336\n",
      "Training loss: 0.11674121022224426\n",
      "Training loss: 0.11787768453359604\n",
      "Training loss: 0.12613585591316223\n",
      "Training loss: 0.11692643910646439\n",
      "Training loss: 0.11920054256916046\n",
      "Training loss: 0.1155758798122406\n",
      "Training loss: 0.12577477097511292\n",
      "Training loss: 0.12374197691679001\n",
      "Training loss: 0.12201064079999924\n",
      "Training loss: 0.11909610033035278\n",
      "Training loss: 0.1341809779405594\n",
      "Training loss: 0.11854685842990875\n",
      "Training loss: 0.11913822591304779\n",
      "Training loss: 0.12220966815948486\n",
      "Training loss: 0.128290593624115\n",
      "Training loss: 0.13156446814537048\n",
      "Training loss: 0.1185787245631218\n",
      "Training loss: 0.12793631851673126\n",
      "Training loss: 0.11665400862693787\n",
      "Training loss: 0.12414766848087311\n",
      "Training loss: 0.12845081090927124\n",
      "Training loss: 0.11998657882213593\n",
      "Training loss: 0.12354308366775513\n",
      "Training loss: 0.12516389787197113\n",
      "Training loss: 0.11804666370153427\n",
      "Training loss: 0.13345935940742493\n",
      "Training loss: 0.12769563496112823\n",
      "Training loss: 0.12592019140720367\n",
      "Training loss: 0.12257858365774155\n",
      "Training loss: 0.12848907709121704\n",
      "Training loss: 0.12064547836780548\n",
      "Training loss: 0.12024224549531937\n",
      "Training loss: 0.1290360391139984\n",
      "Training loss: 0.11689739674329758\n",
      "Training loss: 0.12266335636377335\n",
      "Training loss: 0.12589876353740692\n",
      "Training loss: 0.12580518424510956\n",
      "Training loss: 0.12649548053741455\n",
      "Training loss: 0.12923865020275116\n",
      "Training loss: 0.12430074065923691\n",
      "Training loss: 0.12666866183280945\n",
      "Training loss: 0.12644799053668976\n",
      "Training loss: 0.11922454088926315\n",
      "Training loss: 0.12413232773542404\n",
      "Training loss: 0.12241768836975098\n",
      "Training loss: 0.11785741150379181\n",
      "Training loss: 0.11961326748132706\n",
      "Training loss: 0.11589709669351578\n",
      "Training loss: 0.12432121485471725\n",
      "Training loss: 0.12859125435352325\n",
      "Training loss: 0.12246090918779373\n",
      "Training loss: 0.1250651776790619\n",
      "Training loss: 0.12483537942171097\n",
      "Training loss: 0.12133154273033142\n",
      "Training loss: 0.12261731177568436\n",
      "Training loss: 0.12054121494293213\n",
      "Training loss: 0.1159830242395401\n",
      "Training loss: 0.1325138360261917\n",
      "Training loss: 0.11380984634160995\n",
      "Training loss: 0.1216239333152771\n",
      "Training loss: 0.12066443264484406\n",
      "Training loss: 0.11463546007871628\n",
      "Training loss: 0.12399227172136307\n",
      "Training loss: 0.11835585534572601\n",
      "Training loss: 0.12128730863332748\n",
      "Training loss: 0.1153179481625557\n",
      "Training loss: 0.11509488523006439\n",
      "Training loss: 0.11697492003440857\n",
      "Training loss: 0.120655857026577\n",
      "Training loss: 0.11700613796710968\n",
      "Training loss: 0.1250704526901245\n",
      "Training loss: 0.11902187019586563\n",
      "Training loss: 0.12233510613441467\n",
      "Training loss: 0.11815155297517776\n",
      "Training loss: 0.12055955082178116\n",
      "Training loss: 0.11829768866300583\n",
      "Training loss: 0.11958424746990204\n",
      "Training loss: 0.11559327691793442\n",
      "Training loss: 0.11459305882453918\n",
      "Training loss: 0.11323048174381256\n",
      "Training loss: 0.11513271182775497\n",
      "Training loss: 0.12222041189670563\n",
      "Training loss: 0.12233710289001465\n",
      "Training loss: 0.11496403068304062\n",
      "Training loss: 0.1275491565465927\n",
      "Training loss: 0.12133899331092834\n",
      "Training loss: 0.12448888272047043\n",
      "Training loss: 0.12496380507946014\n",
      "Training loss: 0.12299084663391113\n",
      "Training loss: 0.11991202086210251\n",
      "Training loss: 0.12009517848491669\n",
      "Training loss: 0.11738664656877518\n",
      "Training loss: 0.124981589615345\n",
      "Training loss: 0.12598538398742676\n",
      "Training loss: 0.1377849280834198\n",
      "Training loss: 0.12728692591190338\n",
      "Training loss: 0.11861347407102585\n",
      "Training loss: 0.1159493550658226\n",
      "Training loss: 0.12747633457183838\n",
      "Training loss: 0.11436638981103897\n",
      "Training loss: 0.1398872435092926\n",
      "Training loss: 0.12088377773761749\n",
      "Training loss: 0.11798346042633057\n",
      "Training loss: 0.1200399100780487\n",
      "Training loss: 0.12151483446359634\n",
      "Training loss: 0.11932141333818436\n",
      "Training loss: 0.11938831955194473\n",
      "Training loss: 0.11934421211481094\n",
      "Training loss: 0.1262560337781906\n",
      "Training loss: 0.12704575061798096\n",
      "Training loss: 0.12993845343589783\n",
      "Training loss: 0.11776168644428253\n",
      "Training loss: 0.11554307490587234\n",
      "Training loss: 0.12197868525981903\n",
      "Training loss: 0.11876664310693741\n",
      "Training loss: 0.12707673013210297\n",
      "Training loss: 0.13149425387382507\n",
      "Training loss: 0.12050174921751022\n",
      "Training loss: 0.12157511711120605\n",
      "Training loss: 0.1264122724533081\n",
      "Training loss: 0.12298648804426193\n",
      "Training loss: 0.12232133746147156\n",
      "Training loss: 0.12102682143449783\n",
      "Training loss: 0.11536954343318939\n",
      "Training loss: 0.11469295620918274\n",
      "Training loss: 0.1212642714381218\n",
      "Training loss: 0.12521977722644806\n",
      "Training loss: 0.11907056719064713\n",
      "Training loss: 0.11931262910366058\n",
      "Training loss: 0.12263710051774979\n",
      "Training loss: 0.12605607509613037\n",
      "Training loss: 0.12043065577745438\n",
      "Training loss: 0.1186070665717125\n",
      "Training loss: 0.11624753475189209\n",
      "Training loss: 0.1191931739449501\n",
      "Training loss: 0.1212490126490593\n",
      "Training loss: 0.11299965530633926\n",
      "Training loss: 0.12023032456636429\n",
      "Training loss: 0.13196603953838348\n",
      "Training loss: 0.12089263647794724\n",
      "Training loss: 0.13109765946865082\n",
      "Training loss: 0.11548164486885071\n",
      "Training loss: 0.13711346685886383\n",
      "Training loss: 0.12112031131982803\n",
      "Training loss: 0.12836642563343048\n",
      "Training loss: 0.12540386617183685\n",
      "Training loss: 0.12037698179483414\n",
      "Training loss: 0.12176982313394547\n",
      "Training loss: 0.1263928860425949\n",
      "Training loss: 0.11161819845438004\n",
      "Training loss: 0.1174325942993164\n",
      "Training loss: 0.12526999413967133\n",
      "Training loss: 0.12170764058828354\n",
      "Training loss: 0.11715289205312729\n",
      "Training loss: 0.11407368630170822\n",
      "Training loss: 0.11216679960489273\n",
      "Training loss: 0.11805056035518646\n",
      "Training loss: 0.12650199234485626\n",
      "Training loss: 0.1197773665189743\n",
      "Training loss: 0.1148177906870842\n",
      "Training loss: 0.11874748021364212\n",
      "Training loss: 0.11967755854129791\n",
      "Training loss: 0.11985475569963455\n",
      "Training loss: 0.11571067571640015\n",
      "Training loss: 0.11596406996250153\n",
      "Training loss: 0.11704801768064499\n",
      "Training loss: 0.11483100801706314\n",
      "Training loss: 0.11691470444202423\n",
      "Training loss: 0.11733002960681915\n",
      "Training loss: 0.13110505044460297\n",
      "Training loss: 0.12237370014190674\n",
      "Training loss: 0.11912582069635391\n",
      "Training loss: 0.115972138941288\n",
      "Training loss: 0.12779469788074493\n",
      "Training loss: 0.11578924208879471\n",
      "Training loss: 0.1188177838921547\n",
      "Training loss: 0.12279750406742096\n",
      "Training loss: 0.11227846890687943\n",
      "Training loss: 0.12727724015712738\n",
      "Training loss: 0.11957445740699768\n",
      "Training loss: 0.37477046251296997\n",
      "Training loss: 0.12252829223871231\n",
      "Training loss: 0.12104920297861099\n",
      "Training loss: 0.12582939863204956\n",
      "Training loss: 0.12507323920726776\n",
      "Training loss: 0.12562444806098938\n",
      "Training loss: 0.12657681107521057\n",
      "Training loss: 0.11758014559745789\n",
      "Training loss: 0.12673337757587433\n",
      "Training loss: 0.1230427622795105\n",
      "Training loss: 0.13777922093868256\n",
      "Training loss: 0.1185954138636589\n",
      "Training loss: 0.11613352596759796\n",
      "Training loss: 0.1159011647105217\n",
      "Training loss: 0.12309184670448303\n",
      "Training loss: 0.12210056185722351\n",
      "Training loss: 0.12695510685443878\n",
      "Training loss: 0.12619717419147491\n",
      "Training loss: 0.13212482631206512\n",
      "Training loss: 0.12211576104164124\n",
      "Training loss: 0.12573598325252533\n",
      "Training loss: 0.12076721340417862\n",
      "Training loss: 0.12695170938968658\n",
      "Training loss: 0.12445530295372009\n",
      "Training loss: 0.11474572122097015\n",
      "Training loss: 0.11835405975580215\n",
      "Training loss: 0.12035709619522095\n",
      "Training loss: 0.1239953264594078\n",
      "Training loss: 0.11904069036245346\n",
      "Training loss: 0.12424994260072708\n",
      "Training loss: 0.11642907559871674\n",
      "Training loss: 0.12042068690061569\n",
      "Training loss: 0.11671332269906998\n",
      "Training loss: 0.13012316823005676\n",
      "Training loss: 0.12848098576068878\n",
      "Training loss: 0.11881155520677567\n",
      "Training loss: 0.12094569951295853\n",
      "Training loss: 0.11775001883506775\n",
      "Training loss: 0.12590350210666656\n",
      "Training loss: 0.12324018031358719\n",
      "Training loss: 0.13548988103866577\n",
      "Training loss: 0.12137625366449356\n",
      "Training loss: 0.12222525477409363\n",
      "Training loss: 0.11443830281496048\n",
      "Training loss: 0.11644051969051361\n",
      "Training loss: 0.11632406711578369\n",
      "Training loss: 0.12021811306476593\n",
      "Training loss: 0.11885558813810349\n",
      "Training loss: 0.1252182275056839\n",
      "Training loss: 0.12188086658716202\n",
      "Training loss: 0.12183462083339691\n",
      "Training loss: 0.13300853967666626\n",
      "Training loss: 0.1147274374961853\n",
      "Training loss: 0.12237237393856049\n",
      "Training loss: 0.13195160031318665\n",
      "Training loss: 0.1182243824005127\n",
      "Training loss: 0.12232376635074615\n",
      "Training loss: 0.12382082641124725\n",
      "Training loss: 0.12791062891483307\n",
      "Training loss: 0.12111960351467133\n",
      "Training loss: 0.11897213011980057\n",
      "Training loss: 0.10996951162815094\n",
      "Training loss: 0.12017624080181122\n",
      "Training loss: 0.1260797083377838\n",
      "Training loss: 0.1306280493736267\n",
      "Training loss: 0.11991418898105621\n",
      "Training loss: 0.11773397028446198\n",
      "Training loss: 0.1308237463235855\n",
      "Training loss: 0.11768535524606705\n",
      "Training loss: 0.11542820930480957\n",
      "Training loss: 0.13098682463169098\n",
      "Training loss: 0.1193562000989914\n",
      "Training loss: 0.1198417916893959\n",
      "Training loss: 0.11445243656635284\n",
      "Training loss: 0.11439039558172226\n",
      "Training loss: 0.11927428096532822\n",
      "Training loss: 0.1220831423997879\n",
      "Training loss: 0.11561134457588196\n",
      "Training loss: 0.1206013634800911\n",
      "Training loss: 0.11744385957717896\n",
      "Training loss: 0.11485922336578369\n",
      "Training loss: 0.11378761380910873\n",
      "Training loss: 0.11849711090326309\n",
      "Training loss: 0.12156936526298523\n",
      "Training loss: 0.11643272638320923\n",
      "Training loss: 0.11256934702396393\n",
      "Training loss: 0.12316489964723587\n",
      "Training loss: 0.11052311956882477\n",
      "Training loss: 0.1191079244017601\n",
      "Training loss: 0.1161099374294281\n",
      "Training loss: 0.11514807492494583\n",
      "Training loss: 0.1124645248055458\n",
      "Training loss: 0.11689533293247223\n",
      "Training loss: 0.11166282743215561\n",
      "Training loss: 0.11857065558433533\n",
      "Training loss: 0.11389029026031494\n",
      "Training loss: 0.11439114063978195\n",
      "Training loss: 0.12226926535367966\n",
      "Training loss: 0.1138630360364914\n",
      "Training loss: 0.11496985703706741\n",
      "Training loss: 0.11073452979326248\n",
      "Training loss: 0.11939479410648346\n",
      "Training loss: 0.11489886790513992\n",
      "Training loss: 0.10585840791463852\n",
      "Training loss: 0.11364918947219849\n",
      "Training loss: 0.11789478361606598\n",
      "Training loss: 0.11948935687541962\n",
      "Training loss: 0.120049849152565\n",
      "Training loss: 0.11437041312456131\n",
      "Training loss: 0.1179284080862999\n",
      "Training loss: 0.11046109348535538\n",
      "Training loss: 0.11742686480283737\n",
      "Training loss: 0.11722370237112045\n",
      "Training loss: 0.11622179299592972\n",
      "Training loss: 0.12495990842580795\n",
      "Training loss: 0.1194051131606102\n",
      "Training loss: 0.11379772424697876\n",
      "Training loss: 0.11270227283239365\n",
      "Training loss: 0.12946298718452454\n",
      "Training loss: 0.11236961930990219\n",
      "Training loss: 0.11722534894943237\n",
      "Training loss: 0.11409751325845718\n",
      "Training loss: 0.12069107592105865\n",
      "Training loss: 0.12019966542720795\n",
      "Training loss: 0.12640784680843353\n",
      "Training loss: 0.11679597944021225\n",
      "Training loss: 0.1160237193107605\n",
      "Training loss: 0.1172088161110878\n",
      "Training loss: 0.10952901095151901\n",
      "Training loss: 0.1117665246129036\n",
      "Training loss: 0.12142559885978699\n",
      "Training loss: 0.11525612324476242\n",
      "Training loss: 0.11278237402439117\n",
      "Training loss: 0.11403939127922058\n",
      "Training loss: 0.11363472789525986\n",
      "Training loss: 0.11272972822189331\n",
      "Training loss: 0.1178293228149414\n",
      "Training loss: 0.11510185152292252\n",
      "Training loss: 0.10928798466920853\n",
      "Training loss: 0.11729210615158081\n",
      "Training loss: 0.11131377518177032\n",
      "Training loss: 0.1098610982298851\n",
      "Training loss: 0.11317925900220871\n",
      "Training loss: 0.11923260241746902\n",
      "Training loss: 0.11345978826284409\n",
      "Training loss: 0.11730305850505829\n",
      "Training loss: 0.11891921609640121\n",
      "Training loss: 0.12441656738519669\n",
      "Training loss: 0.11613768339157104\n",
      "Training loss: 0.1223379597067833\n",
      "Training loss: 0.11781273782253265\n",
      "Training loss: 0.11260690540075302\n",
      "Training loss: 0.1112905889749527\n",
      "Training loss: 0.12200234830379486\n",
      "Training loss: 0.1245599314570427\n",
      "Training loss: 0.11698173731565475\n",
      "Training loss: 0.11879511922597885\n",
      "Training loss: 0.11654279381036758\n",
      "Training loss: 0.11208371073007584\n",
      "Training loss: 0.11486486345529556\n",
      "Training loss: 0.11973495036363602\n",
      "Training loss: 0.12080158293247223\n",
      "Training loss: 0.11778043210506439\n",
      "Training loss: 0.11375141143798828\n",
      "Training loss: 0.12026924639940262\n",
      "Training loss: 0.12399563193321228\n",
      "Training loss: 0.11800169199705124\n",
      "Training loss: 0.11815463751554489\n",
      "Training loss: 0.12052955478429794\n",
      "Training loss: 0.11771231889724731\n",
      "Training loss: 0.11663372069597244\n",
      "Training loss: 0.12972047924995422\n",
      "Training loss: 0.11549073457717896\n",
      "Training loss: 0.11256138980388641\n",
      "Training loss: 0.12307488173246384\n",
      "Training loss: 0.11644097417593002\n",
      "Training loss: 0.12843166291713715\n",
      "Training loss: 0.12070384621620178\n",
      "Training loss: 0.12001056969165802\n",
      "Training loss: 0.1138177141547203\n",
      "Training loss: 0.11239226162433624\n",
      "Training loss: 0.12212569266557693\n",
      "Training loss: 0.12148725986480713\n",
      "Training loss: 0.11466041207313538\n",
      "Training loss: 0.11683251708745956\n",
      "Training loss: 0.11744407564401627\n",
      "Training loss: 0.11300957202911377\n",
      "Training loss: 0.12550021708011627\n",
      "Training loss: 0.12343914061784744\n",
      "Training loss: 0.12376555800437927\n",
      "Training loss: 0.12085998803377151\n",
      "Training loss: 0.11742699146270752\n",
      "Training loss: 0.11771467328071594\n",
      "Training loss: 0.14703191816806793\n",
      "Training loss: 0.12016158550977707\n",
      "Training loss: 0.12324822694063187\n",
      "Training loss: 0.1205030009150505\n",
      "Training loss: 0.12128102034330368\n",
      "Training loss: 0.1102212443947792\n",
      "Training loss: 0.12152579426765442\n",
      "Training loss: 0.12325127422809601\n",
      "Training loss: 0.11089248210191727\n",
      "Training loss: 0.11639685183763504\n",
      "Training loss: 0.11697667092084885\n",
      "Training loss: 0.11355610191822052\n",
      "Training loss: 0.12142746895551682\n",
      "Training loss: 0.12174107134342194\n",
      "Training loss: 0.12001791596412659\n",
      "Training loss: 0.119790218770504\n",
      "Training loss: 0.121857188642025\n",
      "Training loss: 0.11503742635250092\n",
      "Training loss: 0.11320962011814117\n",
      "Training loss: 0.11715271323919296\n",
      "Training loss: 0.11023175716400146\n",
      "Training loss: 0.1139458566904068\n",
      "Training loss: 0.11537645012140274\n",
      "Training loss: 0.12254273891448975\n",
      "Training loss: 0.115426205098629\n",
      "Training loss: 0.1125127375125885\n",
      "Training loss: 0.1128004714846611\n",
      "Training loss: 0.11894484609365463\n",
      "Training loss: 0.11827578395605087\n",
      "Training loss: 0.11199459433555603\n",
      "Training loss: 0.12483465671539307\n",
      "Training loss: 0.12355022132396698\n",
      "Training loss: 0.11804697662591934\n",
      "Training loss: 0.11537586897611618\n",
      "Training loss: 0.11534470319747925\n",
      "Training loss: 0.1134132519364357\n",
      "Training loss: 0.1180049329996109\n",
      "Training loss: 0.11730274558067322\n",
      "Training loss: 0.11333884298801422\n",
      "Training loss: 0.10743686556816101\n",
      "Training loss: 0.11834313720464706\n",
      "Training loss: 0.11120184510946274\n",
      "Training loss: 0.1154080405831337\n",
      "Training loss: 0.12005089968442917\n",
      "Training loss: 0.11356887221336365\n",
      "Training loss: 0.11934222280979156\n",
      "Training loss: 0.11414436250925064\n",
      "Training loss: 0.11301308125257492\n",
      "Training loss: 0.10703180730342865\n",
      "Training loss: 0.11084504425525665\n",
      "Training loss: 0.11060744524002075\n",
      "Training loss: 0.10891501605510712\n",
      "Training loss: 0.1128622442483902\n",
      "Training loss: 0.11756224185228348\n",
      "Training loss: 0.11574825644493103\n",
      "Training loss: 0.11252015829086304\n",
      "Training loss: 0.12233608961105347\n",
      "Training loss: 0.1119229719042778\n",
      "Training loss: 0.11187140643596649\n",
      "Training loss: 0.12047459930181503\n",
      "Training loss: 0.11229313910007477\n",
      "Training loss: 0.11543445289134979\n",
      "Training loss: 0.11574933677911758\n",
      "Training loss: 0.10728742927312851\n",
      "Training loss: 0.10622560977935791\n",
      "Training loss: 0.11144766211509705\n",
      "Training loss: 0.11309614777565002\n",
      "Training loss: 0.11347928643226624\n",
      "Training loss: 0.1111556813120842\n",
      "Training loss: 0.1081836074590683\n",
      "Training loss: 0.1130213513970375\n",
      "Training loss: 0.10668560862541199\n",
      "Training loss: 0.11198535561561584\n",
      "Training loss: 0.11387158930301666\n",
      "Training loss: 0.11643781512975693\n",
      "Training loss: 0.11024938523769379\n",
      "Training loss: 0.10992194712162018\n",
      "Training loss: 0.11084996163845062\n",
      "Training loss: 0.10627075284719467\n",
      "Training loss: 0.1104709655046463\n",
      "Training loss: 0.11810670793056488\n",
      "Training loss: 0.11844179034233093\n",
      "Training loss: 0.11415039002895355\n",
      "Training loss: 0.1179719865322113\n",
      "Training loss: 0.11522813141345978\n",
      "Training loss: 0.11242888867855072\n",
      "Training loss: 0.11777792125940323\n",
      "Training loss: 0.1101190447807312\n",
      "Training loss: 0.11277680844068527\n",
      "Training loss: 0.11420920491218567\n",
      "Training loss: 0.11057937145233154\n",
      "Training loss: 0.1157032921910286\n",
      "Training loss: 0.12354543060064316\n",
      "Training loss: 0.10734996199607849\n",
      "Training loss: 0.10611685365438461\n",
      "Training loss: 0.11181320250034332\n",
      "Training loss: 0.11822754889726639\n",
      "Training loss: 0.11233674734830856\n",
      "Training loss: 0.12091437727212906\n",
      "Training loss: 0.11276276409626007\n",
      "Training loss: 0.1161690279841423\n",
      "Training loss: 0.11419320851564407\n",
      "Training loss: 0.11473672091960907\n",
      "Training loss: 0.11679016053676605\n",
      "Training loss: 0.12071940302848816\n",
      "Training loss: 0.11100977659225464\n",
      "Training loss: 0.11361270397901535\n",
      "Training loss: 0.11187586933374405\n",
      "Training loss: 0.11990996450185776\n",
      "Training loss: 0.114021435379982\n",
      "Training loss: 0.10501493513584137\n",
      "Training loss: 0.11849913746118546\n",
      "Training loss: 0.11601278930902481\n",
      "Training loss: 0.11986478418111801\n",
      "Training loss: 0.1160675436258316\n",
      "Training loss: 0.11484821885824203\n",
      "Training loss: 0.11076977103948593\n",
      "Training loss: 0.11417867243289948\n",
      "Training loss: 0.11224039644002914\n",
      "Training loss: 0.11052725464105606\n",
      "Training loss: 0.11560915410518646\n",
      "Training loss: 0.11299032717943192\n",
      "Training loss: 0.11378122121095657\n",
      "Training loss: 0.11462618410587311\n",
      "Training loss: 0.11750270426273346\n",
      "Training loss: 0.10808499902486801\n",
      "Training loss: 0.11850210279226303\n",
      "Training loss: 0.11756478250026703\n",
      "Training loss: 0.11233197152614594\n",
      "Training loss: 0.11798587441444397\n",
      "Training loss: 0.11109420657157898\n",
      "Training loss: 0.11323422938585281\n",
      "Training loss: 0.11101166158914566\n",
      "Training loss: 0.11514952033758163\n",
      "Training loss: 0.11461558938026428\n",
      "Training loss: 0.11059670895338058\n",
      "Training loss: 0.11365141719579697\n",
      "Training loss: 0.11729615926742554\n",
      "Training loss: 0.11663901060819626\n",
      "Training loss: 0.10578739643096924\n",
      "Training loss: 0.11369185894727707\n",
      "Training loss: 0.11505810171365738\n",
      "Training loss: 0.11191198229789734\n",
      "Training loss: 0.1097598522901535\n",
      "Training loss: 0.11276653409004211\n",
      "Training loss: 0.11973563581705093\n",
      "Training loss: 0.10898537188768387\n",
      "Training loss: 0.10901065915822983\n",
      "Training loss: 0.1210368424654007\n",
      "Training loss: 0.10882462561130524\n",
      "Training loss: 0.11496374756097794\n",
      "Training loss: 0.10621416568756104\n",
      "Training loss: 0.11389659345149994\n",
      "Training loss: 0.11329878121614456\n",
      "Training loss: 0.10546387732028961\n",
      "Training loss: 0.11891188472509384\n",
      "Training loss: 0.11465458571910858\n",
      "Training loss: 0.13014142215251923\n",
      "Training loss: 0.1149045005440712\n",
      "Training loss: 0.1132267490029335\n",
      "Training loss: 0.11464715749025345\n",
      "Training loss: 0.11627169698476791\n",
      "Training loss: 0.11044967174530029\n",
      "Training loss: 0.11574460566043854\n",
      "Training loss: 0.10690937936306\n",
      "Training loss: 0.10496385395526886\n",
      "Training loss: 0.11305075883865356\n",
      "Training loss: 0.1117539331316948\n",
      "Training loss: 0.12025494128465652\n",
      "Training loss: 0.11183331906795502\n",
      "Training loss: 0.10929293930530548\n",
      "Training loss: 0.10911265760660172\n",
      "Training loss: 0.11956621706485748\n",
      "Training loss: 0.10889261960983276\n",
      "Training loss: 0.11222919821739197\n",
      "Training loss: 0.10899218916893005\n",
      "Training loss: 0.13293962180614471\n",
      "Training loss: 0.10880650579929352\n",
      "Training loss: 0.1077294573187828\n",
      "Training loss: 0.11786822974681854\n",
      "Training loss: 0.11421012133359909\n",
      "Training loss: 0.11768236756324768\n",
      "Training loss: 0.1160891130566597\n",
      "Training loss: 0.10296285897493362\n",
      "Training loss: 0.11738554388284683\n",
      "Training loss: 0.11256638169288635\n",
      "Training loss: 0.1103423610329628\n",
      "Training loss: 0.11147308349609375\n",
      "Training loss: 0.11234113574028015\n",
      "Training loss: 0.10924915969371796\n",
      "Training loss: 0.11205200105905533\n",
      "Training loss: 0.10873054713010788\n",
      "Training loss: 0.11726287752389908\n",
      "Training loss: 0.11547359824180603\n",
      "Training loss: 0.11180415749549866\n",
      "Training loss: 0.11172012984752655\n",
      "Training loss: 0.11943861842155457\n",
      "Training loss: 0.11441918462514877\n",
      "Training loss: 0.11161097884178162\n",
      "Training loss: 0.11495435237884521\n",
      "Training loss: 0.11862270534038544\n",
      "Training loss: 0.12789469957351685\n",
      "Training loss: 0.11700528115034103\n",
      "Training loss: 0.11826124787330627\n",
      "Training loss: 0.11587616056203842\n",
      "Training loss: 0.11236762255430222\n",
      "Training loss: 0.12390130758285522\n",
      "Training loss: 0.10757266730070114\n",
      "Training loss: 0.11803555488586426\n",
      "Training loss: 0.11321648955345154\n",
      "Training loss: 0.11179859191179276\n",
      "Training loss: 0.11513116955757141\n",
      "Training loss: 0.11497686058282852\n",
      "Training loss: 0.11101951450109482\n",
      "Training loss: 0.11705679446458817\n",
      "Training loss: 0.11527691781520844\n",
      "Training loss: 0.11238333582878113\n",
      "Training loss: 0.11143570393323898\n",
      "Training loss: 0.11444788426160812\n",
      "Training loss: 0.10738220810890198\n",
      "Training loss: 0.11933011561632156\n",
      "Training loss: 0.11309117823839188\n",
      "Training loss: 0.1137532889842987\n",
      "Training loss: 0.11158759891986847\n",
      "Training loss: 0.11142868548631668\n",
      "Training loss: 0.11393885314464569\n",
      "Training loss: 0.13084962964057922\n",
      "Training loss: 0.11350701004266739\n",
      "Training loss: 0.11727145314216614\n",
      "Training loss: 0.11721748858690262\n",
      "Training loss: 0.11849350482225418\n",
      "Training loss: 0.10997102409601212\n",
      "Training loss: 0.11447221040725708\n",
      "Training loss: 0.10878840833902359\n",
      "Training loss: 0.11396205425262451\n",
      "Training loss: 0.11388923227787018\n",
      "Training loss: 0.11013055592775345\n",
      "Training loss: 0.12081913650035858\n",
      "Training loss: 0.12329591810703278\n",
      "Training loss: 0.13074247539043427\n",
      "Training loss: 0.12541039288043976\n",
      "Training loss: 0.11581164598464966\n",
      "Training loss: 0.11645528674125671\n",
      "Training loss: 0.11290937662124634\n",
      "Training loss: 0.11863034218549728\n",
      "Training loss: 0.12104250490665436\n",
      "Training loss: 0.12248211354017258\n",
      "Training loss: 0.12333507835865021\n",
      "Training loss: 0.11386825889348984\n",
      "Training loss: 0.11660683155059814\n",
      "Training loss: 0.10904736071825027\n",
      "Training loss: 0.1151968240737915\n",
      "Training loss: 0.12864501774311066\n",
      "Training loss: 0.1219048947095871\n",
      "Training loss: 0.12490997463464737\n",
      "Training loss: 0.11958608776330948\n",
      "Training loss: 0.12058685719966888\n",
      "Training loss: 0.11557184904813766\n",
      "Training loss: 0.11174589395523071\n",
      "Training loss: 0.12640443444252014\n",
      "Training loss: 0.11472643911838531\n",
      "Training loss: 0.11675667017698288\n",
      "Training loss: 0.1258665770292282\n",
      "Training loss: 0.11306650936603546\n",
      "Training loss: 0.11676181852817535\n",
      "Training loss: 0.12015978246927261\n",
      "Training loss: 0.11409308016300201\n",
      "Training loss: 0.11011864989995956\n",
      "Training loss: 0.11305543780326843\n",
      "Training loss: 0.11539795249700546\n",
      "Training loss: 0.11284651607275009\n",
      "Training loss: 0.12016128748655319\n",
      "Training loss: 0.1372617781162262\n",
      "Training loss: 0.11635418981313705\n",
      "Training loss: 0.11545031517744064\n",
      "Training loss: 0.11179444938898087\n",
      "Training loss: 0.11827772110700607\n",
      "Training loss: 0.1102520003914833\n",
      "Training loss: 0.1108969897031784\n",
      "Training loss: 0.10979651659727097\n",
      "Training loss: 0.11728887259960175\n",
      "Training loss: 0.11823803931474686\n",
      "Training loss: 0.1137487143278122\n",
      "Training loss: 0.1122211292386055\n",
      "Training loss: 0.11480101197957993\n",
      "Training loss: 0.10953530669212341\n",
      "Training loss: 0.11745010316371918\n",
      "Training loss: 0.10506097227334976\n",
      "Training loss: 0.10912016779184341\n",
      "Training loss: 0.1094442754983902\n",
      "Training loss: 0.11077078431844711\n",
      "Training loss: 0.11093167215585709\n",
      "Training loss: 0.10861422121524811\n",
      "Training loss: 0.11406862735748291\n",
      "Training loss: 0.11626484990119934\n",
      "Training loss: 0.11696477979421616\n",
      "Training loss: 0.10715792328119278\n",
      "Training loss: 0.11646109819412231\n",
      "Training loss: 0.11324016004800797\n",
      "Training loss: 0.11537516862154007\n",
      "Training loss: 0.11293896287679672\n",
      "Training loss: 0.11325763911008835\n",
      "Training loss: 0.11510729044675827\n",
      "Training loss: 0.10665632039308548\n",
      "Training loss: 0.11045080423355103\n",
      "Training loss: 0.11123968660831451\n",
      "Training loss: 0.11319153755903244\n",
      "Training loss: 0.11469266563653946\n",
      "Training loss: 0.12315817177295685\n",
      "Training loss: 0.11030464619398117\n",
      "Training loss: 0.11146379262208939\n",
      "Training loss: 0.11394120007753372\n",
      "Training loss: 0.11798133701086044\n",
      "Training loss: 0.1122257262468338\n",
      "Training loss: 0.11168566346168518\n",
      "Training loss: 0.10763265192508698\n",
      "Training loss: 0.11241122335195541\n",
      "Training loss: 0.1132856011390686\n",
      "Training loss: 0.11550775170326233\n",
      "Training loss: 0.11456048488616943\n",
      "Training loss: 0.11071687191724777\n",
      "Training loss: 0.1089443638920784\n",
      "Training loss: 0.11261601746082306\n",
      "Training loss: 0.11188087612390518\n",
      "Training loss: 0.1036614403128624\n",
      "Training loss: 0.11276353895664215\n",
      "Training loss: 0.10996748507022858\n",
      "Training loss: 0.10967427492141724\n",
      "Training loss: 0.10937518626451492\n",
      "Training loss: 0.11435739696025848\n",
      "Training loss: 0.11384281516075134\n",
      "Training loss: 0.11790840327739716\n",
      "Training loss: 0.10707753151655197\n",
      "Training loss: 0.11113627254962921\n",
      "Training loss: 0.10688064247369766\n",
      "Training loss: 0.11450662463903427\n",
      "Training loss: 0.1001967340707779\n",
      "Training loss: 0.11620501428842545\n",
      "Training loss: 0.11358913779258728\n",
      "Training loss: 0.10612981766462326\n",
      "Training loss: 0.10929170995950699\n",
      "Training loss: 0.1071067824959755\n",
      "Training loss: 0.11499253660440445\n",
      "Training loss: 0.11533211916685104\n",
      "Training loss: 0.10870973020792007\n",
      "Training loss: 0.10825620591640472\n",
      "Training loss: 0.10876744985580444\n",
      "Training loss: 0.1113755851984024\n",
      "Training loss: 0.1127062737941742\n",
      "Training loss: 0.11051055788993835\n",
      "Training loss: 0.11192863434553146\n",
      "Training loss: 0.11583642661571503\n",
      "Training loss: 0.10826391726732254\n",
      "Training loss: 0.1183081716299057\n",
      "Training loss: 0.11148015409708023\n",
      "Training loss: 0.10904525220394135\n",
      "Training loss: 0.11598634719848633\n",
      "Training loss: 0.1123267188668251\n",
      "Training loss: 0.10871439427137375\n",
      "Training loss: 0.11583893746137619\n",
      "Training loss: 0.10503245145082474\n",
      "Training loss: 0.11663425713777542\n",
      "Training loss: 0.11122878640890121\n",
      "Training loss: 0.10533758252859116\n",
      "Training loss: 0.10917401313781738\n",
      "Training loss: 0.11598693579435349\n",
      "Training loss: 0.11623714119195938\n",
      "Training loss: 0.10903007537126541\n",
      "Training loss: 0.11377747356891632\n",
      "Training loss: 0.11073282361030579\n",
      "Training loss: 0.1108175739645958\n",
      "Training loss: 0.11962677538394928\n",
      "Training loss: 0.10926368832588196\n",
      "Training loss: 0.11581940948963165\n",
      "Training loss: 0.10662653297185898\n",
      "Training loss: 0.11747558414936066\n",
      "Training loss: 0.12485525012016296\n",
      "Training loss: 0.11032606661319733\n",
      "Training loss: 0.10667536407709122\n",
      "Training loss: 0.11326049268245697\n",
      "Training loss: 0.10712028294801712\n",
      "Training loss: 0.11501358449459076\n",
      "Training loss: 0.1121131181716919\n",
      "Training loss: 0.10654937475919724\n",
      "Training loss: 0.10869822651147842\n",
      "Training loss: 0.10913971811532974\n",
      "Training loss: 0.11166688054800034\n",
      "Training loss: 0.11296620965003967\n",
      "Training loss: 0.10925010591745377\n",
      "Training loss: 0.1425887495279312\n",
      "Training loss: 0.11037634313106537\n",
      "Training loss: 0.11181734502315521\n",
      "Training loss: 0.11010123044252396\n",
      "Training loss: 0.11746761947870255\n",
      "Training loss: 0.11342176049947739\n",
      "Training loss: 0.10507455468177795\n",
      "Training loss: 0.11187709122896194\n",
      "Training loss: 0.11725123226642609\n",
      "Training loss: 0.11955219507217407\n",
      "Training loss: 0.11234839260578156\n",
      "Training loss: 0.10878454148769379\n",
      "Training loss: 0.11804701387882233\n",
      "Training loss: 0.11413321644067764\n",
      "Training loss: 0.11010941863059998\n",
      "Training loss: 0.1122443899512291\n",
      "Training loss: 0.10963629931211472\n",
      "Training loss: 0.11232508718967438\n",
      "Training loss: 0.1072387769818306\n",
      "Training loss: 0.11050888150930405\n",
      "Training loss: 0.11652220040559769\n",
      "Training loss: 0.11327127367258072\n",
      "Training loss: 0.11176400631666183\n",
      "Training loss: 0.11573689430952072\n",
      "Training loss: 0.10882715880870819\n",
      "Training loss: 0.11045845597982407\n",
      "Training loss: 0.11717035621404648\n",
      "Training loss: 0.10650063306093216\n",
      "Training loss: 0.10970556735992432\n",
      "Training loss: 0.10957200080156326\n",
      "Training loss: 0.11296011507511139\n",
      "Training loss: 0.11347664892673492\n",
      "Training loss: 0.11472790688276291\n",
      "Training loss: 0.11053194105625153\n",
      "Training loss: 0.11124332994222641\n",
      "Training loss: 0.10582650452852249\n",
      "Training loss: 0.22630061209201813\n",
      "Training loss: 0.10966696590185165\n",
      "Training loss: 0.11478648334741592\n",
      "Training loss: 0.10522180050611496\n",
      "Training loss: 0.11004337668418884\n",
      "Training loss: 0.11223359405994415\n",
      "Training loss: 0.12094613164663315\n",
      "Training loss: 0.10787414759397507\n",
      "Training loss: 0.11409399658441544\n",
      "Training loss: 0.10964073985815048\n",
      "Training loss: 0.10786333680152893\n",
      "Training loss: 0.11018705368041992\n",
      "Training loss: 0.11510562896728516\n",
      "Training loss: 0.10747431963682175\n",
      "Training loss: 0.11078561097383499\n",
      "Training loss: 0.1046944260597229\n",
      "Training loss: 0.10957583785057068\n",
      "Training loss: 0.11933209747076035\n",
      "Training loss: 0.1114082783460617\n",
      "Training loss: 0.1174948439002037\n",
      "Training loss: 0.1154683530330658\n",
      "Training loss: 0.1111222431063652\n",
      "Training loss: 0.1215638741850853\n",
      "Training loss: 0.11138841509819031\n",
      "Training loss: 0.11261244863271713\n",
      "Training loss: 0.11691945791244507\n",
      "Training loss: 0.11476349085569382\n",
      "Training loss: 0.11564892530441284\n",
      "Training loss: 0.1211099773645401\n",
      "Training loss: 0.12090154737234116\n",
      "Training loss: 0.11316709965467453\n",
      "Training loss: 0.12015295773744583\n",
      "Training loss: 0.1563553363084793\n",
      "Training loss: 0.12791699171066284\n",
      "Training loss: 0.11221326142549515\n",
      "Training loss: 0.11150306463241577\n",
      "Training loss: 0.12034185975790024\n",
      "Training loss: 0.11944244801998138\n",
      "Training loss: 0.11415599286556244\n",
      "Training loss: 0.1190018281340599\n",
      "Training loss: 0.11422638595104218\n",
      "Training loss: 0.11786532402038574\n",
      "Training loss: 0.12000706791877747\n",
      "Training loss: 0.12048275023698807\n",
      "Training loss: 0.1148378774523735\n",
      "Training loss: 0.12904615700244904\n",
      "Training loss: 0.10710269212722778\n",
      "Training loss: 0.11682561039924622\n",
      "Training loss: 0.10732036828994751\n",
      "Training loss: 0.10928799957036972\n",
      "Training loss: 0.11841617524623871\n",
      "Training loss: 0.1119004338979721\n",
      "Training loss: 0.10615390539169312\n",
      "Training loss: 0.1126929521560669\n",
      "Training loss: 0.11248751729726791\n",
      "Training loss: 0.1306271255016327\n",
      "Training loss: 0.11786459386348724\n",
      "Training loss: 0.1100141853094101\n",
      "Training loss: 0.1120670884847641\n",
      "Training loss: 0.11660970002412796\n",
      "Training loss: 0.11746876686811447\n",
      "Training loss: 0.11173105984926224\n",
      "Training loss: 0.10636075586080551\n",
      "Training loss: 0.11166688799858093\n",
      "Training loss: 0.10652924329042435\n",
      "Training loss: 0.11524379998445511\n",
      "Training loss: 0.1151907667517662\n",
      "Training loss: 0.11089987307786942\n",
      "Training loss: 0.10973381251096725\n",
      "Training loss: 0.13109639286994934\n",
      "Training loss: 0.11785218864679337\n",
      "Training loss: 0.10527364909648895\n",
      "Training loss: 0.11124841868877411\n",
      "Training loss: 0.11692117154598236\n",
      "Training loss: 0.11317843943834305\n",
      "Training loss: 0.11303628236055374\n",
      "Training loss: 0.11452064663171768\n",
      "Training loss: 0.10475821048021317\n",
      "Training loss: 0.11302664875984192\n",
      "Training loss: 0.11524242907762527\n",
      "Training loss: 0.10499826818704605\n",
      "Training loss: 0.112376868724823\n",
      "Training loss: 0.10991478711366653\n",
      "Training loss: 0.10787505656480789\n",
      "Training loss: 0.11762619763612747\n",
      "Training loss: 0.12237628549337387\n",
      "Training loss: 0.10992089658975601\n",
      "Training loss: 0.11791157722473145\n",
      "Training loss: 0.11747843027114868\n",
      "Training loss: 0.12272165715694427\n",
      "Training loss: 0.11191929131746292\n",
      "Training loss: 0.11335169523954391\n",
      "Training loss: 0.11237481236457825\n",
      "Training loss: 0.11213143169879913\n",
      "Training loss: 0.12100724130868912\n",
      "Training loss: 0.11294540017843246\n",
      "Training loss: 0.11071161925792694\n",
      "Training loss: 0.10871128737926483\n",
      "Training loss: 0.10616841167211533\n",
      "Training loss: 0.11298705637454987\n",
      "Training loss: 0.11742491275072098\n",
      "Training loss: 0.11048606038093567\n",
      "Training loss: 0.11320119351148605\n",
      "Training loss: 0.10807772725820541\n",
      "Training loss: 0.1255207359790802\n",
      "Training loss: 0.11047862470149994\n",
      "Training loss: 0.10552722960710526\n",
      "Training loss: 0.10737335681915283\n",
      "Training loss: 0.12325242161750793\n",
      "Training loss: 0.11150781810283661\n",
      "Training loss: 0.10706689953804016\n",
      "Training loss: 0.11663010716438293\n",
      "Training loss: 0.11622905731201172\n",
      "Training loss: 0.10476618260145187\n",
      "Training loss: 0.10572246462106705\n",
      "Training loss: 0.11346212029457092\n",
      "Training loss: 0.11196132004261017\n",
      "Training loss: 0.11774486303329468\n",
      "Training loss: 0.11014200001955032\n",
      "Training loss: 0.1064845621585846\n",
      "Training loss: 0.11822274327278137\n",
      "Training loss: 0.10615427047014236\n",
      "Training loss: 0.10554925352334976\n",
      "Training loss: 0.10446273535490036\n",
      "Training loss: 0.11850173026323318\n",
      "Training loss: 0.10891585052013397\n",
      "Training loss: 0.11157941818237305\n",
      "Training loss: 0.1130128800868988\n",
      "Training loss: 0.11272959411144257\n",
      "Training loss: 0.1135183572769165\n",
      "Training loss: 0.11697982251644135\n",
      "Training loss: 0.10232001543045044\n",
      "Training loss: 0.11455133557319641\n",
      "Training loss: 0.1188679188489914\n",
      "Training loss: 0.10760754346847534\n",
      "Training loss: 0.11329439282417297\n",
      "Training loss: 0.11584685742855072\n",
      "Training loss: 0.11017360538244247\n",
      "Training loss: 0.10885544866323471\n",
      "Training loss: 0.11646666377782822\n",
      "Training loss: 0.11276794970035553\n",
      "Training loss: 0.11661355197429657\n",
      "Training loss: 0.11139551550149918\n",
      "Training loss: 0.10544577986001968\n",
      "Training loss: 0.11289375275373459\n",
      "Training loss: 0.11148642748594284\n",
      "Training loss: 0.11617723852396011\n",
      "Training loss: 0.11485839635133743\n",
      "Training loss: 0.10990030318498611\n",
      "Training loss: 0.10826950520277023\n",
      "Training loss: 0.11712681502103806\n",
      "Training loss: 0.11395110934972763\n",
      "Training loss: 0.10931529849767685\n",
      "Training loss: 0.12325067818164825\n",
      "Training loss: 0.11546427011489868\n",
      "Training loss: 0.11780598014593124\n",
      "Training loss: 0.12018268555402756\n",
      "Training loss: 0.11564113944768906\n",
      "Training loss: 0.11575667560100555\n",
      "Training loss: 0.11115157604217529\n",
      "Training loss: 0.11832340061664581\n",
      "Training loss: 0.10435070097446442\n",
      "Training loss: 0.11274240911006927\n",
      "Training loss: 0.10921698808670044\n",
      "Training loss: 0.10936515033245087\n",
      "Training loss: 0.11023437976837158\n",
      "Training loss: 0.11016525328159332\n",
      "Training loss: 0.11417701095342636\n",
      "Training loss: 0.10554768145084381\n",
      "Training loss: 0.11543381214141846\n",
      "Training loss: 0.11104682087898254\n",
      "Training loss: 0.10410428047180176\n",
      "Training loss: 0.10572309046983719\n",
      "Training loss: 0.10427463799715042\n",
      "Training loss: 0.10998065769672394\n",
      "Training loss: 0.1071232259273529\n",
      "Training loss: 0.1075419932603836\n",
      "Training loss: 0.10480109602212906\n",
      "Training loss: 0.10620658844709396\n",
      "Training loss: 0.10785944014787674\n",
      "Training loss: 0.10403148829936981\n",
      "Training loss: 0.1115066409111023\n",
      "Training loss: 0.11661915481090546\n",
      "Training loss: 0.12177767604589462\n",
      "Training loss: 0.11000315099954605\n",
      "Training loss: 0.10945503413677216\n",
      "Training loss: 0.10617681592702866\n",
      "Training loss: 0.1119670495390892\n",
      "Training loss: 0.11313191056251526\n",
      "Training loss: 0.10634522885084152\n",
      "Training loss: 0.11073343455791473\n",
      "Training loss: 0.11512964218854904\n",
      "Training loss: 0.1054433062672615\n",
      "Training loss: 0.11074282228946686\n",
      "Training loss: 0.11953429132699966\n",
      "Training loss: 0.10939186811447144\n",
      "Training loss: 0.10844496637582779\n",
      "Training loss: 0.11460542678833008\n",
      "Training loss: 0.12739239633083344\n",
      "Training loss: 0.10463981330394745\n",
      "Training loss: 0.10767599940299988\n",
      "Training loss: 0.11396928131580353\n",
      "Training loss: 0.11689795553684235\n",
      "Training loss: 0.11635001003742218\n",
      "Training loss: 0.11321461945772171\n",
      "Training loss: 0.11047610640525818\n",
      "Training loss: 0.11581462621688843\n",
      "Training loss: 0.10927294194698334\n",
      "Training loss: 0.11074782907962799\n",
      "Training loss: 0.11215922236442566\n",
      "Training loss: 0.11272867769002914\n",
      "Training loss: 0.10927198082208633\n",
      "Training loss: 0.11076347529888153\n",
      "Training loss: 0.10985641926527023\n",
      "Training loss: 0.11874797940254211\n",
      "Training loss: 0.11853720247745514\n",
      "Training loss: 0.10867126286029816\n",
      "Training loss: 0.10861125588417053\n",
      "Training loss: 0.10711093246936798\n",
      "Training loss: 0.11105707287788391\n",
      "Training loss: 0.11952868103981018\n",
      "Training loss: 0.1129368245601654\n",
      "Training loss: 0.11075584590435028\n",
      "Training loss: 0.11081572622060776\n",
      "Training loss: 0.11230569332838058\n",
      "Training loss: 0.10095846652984619\n",
      "Training loss: 0.1032879501581192\n",
      "Training loss: 0.11080057919025421\n",
      "Training loss: 0.10609297454357147\n",
      "Training loss: 0.11249583959579468\n",
      "Training loss: 0.11259330064058304\n",
      "Training loss: 0.1094624400138855\n",
      "Training loss: 0.10358487069606781\n",
      "Training loss: 0.10387898981571198\n",
      "Training loss: 0.12314781546592712\n",
      "Training loss: 0.1170661449432373\n",
      "Training loss: 0.10426788777112961\n",
      "Training loss: 0.10676369816064835\n",
      "Training loss: 0.10945163667201996\n",
      "Training loss: 0.10508079826831818\n",
      "Training loss: 0.11015807092189789\n",
      "Training loss: 0.1134665459394455\n",
      "Training loss: 0.11502086371183395\n",
      "Training loss: 0.1143377348780632\n",
      "Training loss: 0.11120003461837769\n",
      "Training loss: 0.105891652405262\n",
      "Training loss: 0.10294505208730698\n",
      "Training loss: 0.10711848735809326\n",
      "Training loss: 0.11192625761032104\n",
      "Training loss: 0.10647924244403839\n",
      "Training loss: 0.1054614707827568\n",
      "Training loss: 0.11337082833051682\n",
      "Training loss: 0.1026063859462738\n",
      "Training loss: 0.10657363384962082\n",
      "Training loss: 0.10434819757938385\n",
      "Training loss: 0.11261335015296936\n",
      "Training loss: 0.10292685776948929\n",
      "Training loss: 0.11221436411142349\n",
      "Training loss: 0.112604521214962\n",
      "Training loss: 0.11452475190162659\n",
      "Training loss: 0.11724629253149033\n",
      "Training loss: 0.10925468802452087\n",
      "Training loss: 0.10861318558454514\n",
      "Training loss: 0.10426629334688187\n",
      "Training loss: 0.10670991986989975\n",
      "Training loss: 0.10567895323038101\n",
      "Training loss: 0.1048356369137764\n",
      "Training loss: 0.10285723209381104\n",
      "Training loss: 0.10940860956907272\n",
      "Training loss: 0.10533316433429718\n",
      "Training loss: 0.11142594367265701\n",
      "Training loss: 0.108062744140625\n",
      "Training loss: 0.106650710105896\n",
      "Training loss: 0.10289278626441956\n",
      "Training loss: 0.10657795518636703\n",
      "Training loss: 0.10194076597690582\n",
      "Training loss: 0.10331493616104126\n",
      "Training loss: 0.1031031385064125\n",
      "Training loss: 0.10215910524129868\n",
      "Training loss: 0.10461145639419556\n",
      "Training loss: 0.10965846478939056\n",
      "Training loss: 0.1029914990067482\n",
      "Training loss: 0.10233407467603683\n",
      "Training loss: 0.10085707157850266\n",
      "Training loss: 0.104122593998909\n",
      "Training loss: 0.10103610157966614\n",
      "Training loss: 0.10411415994167328\n",
      "Training loss: 0.10538538545370102\n",
      "Training loss: 0.0982792004942894\n",
      "Training loss: 0.1116265282034874\n",
      "Training loss: 0.10492237657308578\n",
      "Training loss: 0.11178795993328094\n",
      "Training loss: 0.10660597681999207\n",
      "Training loss: 0.12292805314064026\n",
      "Training loss: 0.10064799338579178\n",
      "Training loss: 0.1106010377407074\n",
      "Training loss: 0.10604743659496307\n",
      "Training loss: 0.11772412806749344\n",
      "Training loss: 0.11548564583063126\n",
      "Training loss: 0.11038163304328918\n",
      "Training loss: 0.11272918432950974\n",
      "Training loss: 0.11570693552494049\n",
      "Training loss: 0.1132647916674614\n",
      "Training loss: 0.10725829005241394\n",
      "Training loss: 0.11112431436777115\n",
      "Training loss: 0.10531304031610489\n",
      "Training loss: 0.11335400491952896\n",
      "Training loss: 0.1093020960688591\n",
      "Training loss: 0.11520412564277649\n",
      "Training loss: 0.108346126973629\n",
      "Training loss: 0.11299788951873779\n",
      "Training loss: 0.11023664474487305\n",
      "Training loss: 0.10948856920003891\n",
      "Training loss: 0.10554640740156174\n",
      "Training loss: 0.12137692421674728\n",
      "Training loss: 0.10666096210479736\n",
      "Training loss: 0.11408324539661407\n",
      "Training loss: 0.1062089130282402\n",
      "Training loss: 0.11059106141328812\n",
      "Training loss: 0.11005910485982895\n",
      "Training loss: 0.12001793086528778\n",
      "Training loss: 0.11241801083087921\n",
      "Training loss: 0.11263822764158249\n",
      "Training loss: 0.11161775141954422\n",
      "Training loss: 0.10727962851524353\n",
      "Training loss: 0.11834318935871124\n",
      "Training loss: 0.1109628826379776\n",
      "Training loss: 0.11575693637132645\n",
      "Training loss: 0.10913494229316711\n",
      "Training loss: 0.11273687332868576\n",
      "Training loss: 0.10953313112258911\n",
      "Training loss: 0.10844656825065613\n",
      "Training loss: 0.10946156829595566\n",
      "Training loss: 0.11642222106456757\n",
      "Training loss: 0.10962193459272385\n",
      "Training loss: 0.1094854325056076\n",
      "Training loss: 0.11104343831539154\n",
      "Training loss: 0.11428213864564896\n",
      "Training loss: 0.10594100505113602\n",
      "Training loss: 0.10563196241855621\n",
      "Training loss: 0.10347149521112442\n",
      "Training loss: 0.10703112930059433\n",
      "Training loss: 0.106464684009552\n",
      "Training loss: 0.11194386333227158\n",
      "Training loss: 0.10138116031885147\n",
      "Training loss: 0.10727489739656448\n",
      "Training loss: 0.10360515117645264\n",
      "Training loss: 0.10547717660665512\n",
      "Training loss: 0.10016071051359177\n",
      "Training loss: 0.1056588888168335\n",
      "Training loss: 0.10380873829126358\n",
      "Training loss: 0.10294253379106522\n",
      "Training loss: 0.10904059559106827\n",
      "Training loss: 0.10117831826210022\n",
      "Training loss: 0.10132860392332077\n",
      "Training loss: 0.10789986699819565\n",
      "Training loss: 0.11123470962047577\n",
      "Training loss: 0.10715699940919876\n",
      "Training loss: 0.10009642690420151\n",
      "Training loss: 0.09658177942037582\n",
      "Training loss: 0.11306235194206238\n",
      "Training loss: 0.10070675611495972\n",
      "Training loss: 0.09977412968873978\n",
      "Training loss: 0.09818524122238159\n",
      "Training loss: 0.10816885530948639\n",
      "Training loss: 0.09997937083244324\n",
      "Training loss: 0.09576807916164398\n",
      "Training loss: 0.1101970300078392\n",
      "Training loss: 0.10191123187541962\n",
      "Training loss: 0.10335474461317062\n",
      "Training loss: 0.10062787681818008\n",
      "Training loss: 0.09797733277082443\n",
      "Training loss: 0.10612261295318604\n",
      "Training loss: 0.10683839023113251\n",
      "Training loss: 0.10356774181127548\n",
      "Training loss: 0.10844913125038147\n",
      "Training loss: 0.10254646092653275\n",
      "Training loss: 0.10453498363494873\n",
      "Training loss: 0.10785051435232162\n",
      "Training loss: 0.10434984415769577\n",
      "Training loss: 0.10964380949735641\n",
      "Training loss: 0.10197986662387848\n",
      "Training loss: 0.11096437275409698\n",
      "Training loss: 0.10495109856128693\n",
      "Training loss: 0.10399632155895233\n",
      "Training loss: 0.09712478518486023\n",
      "Training loss: 0.10228075832128525\n",
      "Training loss: 0.1046474352478981\n",
      "Training loss: 0.10727206617593765\n",
      "Training loss: 0.11204598844051361\n",
      "Training loss: 0.11964692175388336\n",
      "Training loss: 0.1091599315404892\n",
      "Training loss: 0.10258589684963226\n",
      "Training loss: 0.11056021600961685\n",
      "Training loss: 0.1056130975484848\n",
      "Training loss: 0.10735953599214554\n",
      "Training loss: 0.11194521933794022\n",
      "Training loss: 0.10151620954275131\n",
      "Training loss: 0.10159987956285477\n",
      "Training loss: 0.10057886689901352\n",
      "Training loss: 0.10383550822734833\n",
      "Training loss: 0.10506102442741394\n",
      "Training loss: 0.10491570085287094\n",
      "Training loss: 0.1021566092967987\n",
      "Training loss: 0.11607029289007187\n",
      "Training loss: 0.1158016100525856\n",
      "Training loss: 0.1042221263051033\n",
      "Training loss: 0.10634392499923706\n",
      "Training loss: 0.10620392113924026\n",
      "Training loss: 0.11158581078052521\n",
      "Training loss: 0.0972026139497757\n",
      "Training loss: 0.11049868911504745\n",
      "Training loss: 0.10617689043283463\n",
      "Training loss: 0.11331692337989807\n",
      "Training loss: 0.1097278818488121\n",
      "Training loss: 0.1084728091955185\n",
      "Training loss: 0.10722282528877258\n",
      "Training loss: 0.10423335433006287\n",
      "Training loss: 0.10737231373786926\n",
      "Training loss: 0.10495995730161667\n",
      "Training loss: 0.10352271795272827\n",
      "Training loss: 0.1102236732840538\n",
      "Training loss: 0.10839097201824188\n",
      "Training loss: 0.10836833715438843\n",
      "Training loss: 0.10608307272195816\n",
      "Training loss: 0.11049607396125793\n",
      "Training loss: 0.1066618338227272\n",
      "Training loss: 0.10907188057899475\n",
      "Training loss: 0.1088588684797287\n",
      "Training loss: 0.10668683797121048\n",
      "Training loss: 0.11439592391252518\n",
      "Training loss: 0.10838578641414642\n",
      "Training loss: 0.1049514189362526\n",
      "Training loss: 0.10385193675756454\n",
      "Training loss: 0.10975131392478943\n",
      "Training loss: 0.11501264572143555\n",
      "Training loss: 0.10445139557123184\n",
      "Training loss: 0.10907872766256332\n",
      "Training loss: 0.10218480229377747\n",
      "Training loss: 0.10432321578264236\n",
      "Training loss: 0.1086890771985054\n",
      "Training loss: 0.10738930851221085\n",
      "Training loss: 0.11281169205904007\n",
      "Training loss: 0.10814804583787918\n",
      "Training loss: 0.09947500377893448\n",
      "Training loss: 0.11068160831928253\n",
      "Training loss: 0.12155908346176147\n",
      "Training loss: 0.1061430349946022\n",
      "Training loss: 0.10878355801105499\n",
      "Training loss: 0.10546614229679108\n",
      "Training loss: 0.10492242872714996\n",
      "Training loss: 0.10320613533258438\n",
      "Training loss: 0.10766137391328812\n",
      "Training loss: 0.1136525347828865\n",
      "Training loss: 0.11386176943778992\n",
      "Training loss: 0.10354888439178467\n",
      "Training loss: 0.11076394468545914\n",
      "Training loss: 0.11197122931480408\n",
      "Training loss: 0.10788038372993469\n",
      "Training loss: 0.10063361376523972\n",
      "Training loss: 0.10667961090803146\n",
      "Training loss: 0.1081172451376915\n",
      "Training loss: 0.1134016364812851\n",
      "Training loss: 0.11068485677242279\n",
      "Training loss: 0.11543425172567368\n",
      "Training loss: 0.10643130540847778\n",
      "Training loss: 0.10488251596689224\n",
      "Training loss: 0.10912619531154633\n",
      "Training loss: 0.11575593054294586\n",
      "Training loss: 0.11190355569124222\n",
      "Training loss: 0.10978560149669647\n",
      "Training loss: 0.11469084769487381\n",
      "Training loss: 0.11438938230276108\n",
      "Training loss: 0.11256841570138931\n",
      "Training loss: 0.11470749974250793\n",
      "Training loss: 0.10240136831998825\n",
      "Training loss: 0.107584148645401\n",
      "Training loss: 0.11494365334510803\n",
      "Training loss: 0.10972510278224945\n",
      "Training loss: 0.10538551956415176\n",
      "Training loss: 0.1076379045844078\n",
      "Training loss: 0.11381082236766815\n",
      "Training loss: 0.11114984005689621\n",
      "Training loss: 0.11096066236495972\n",
      "Training loss: 0.10704102367162704\n",
      "Training loss: 0.10847755521535873\n",
      "Training loss: 0.10650119185447693\n",
      "Training loss: 0.10612676292657852\n",
      "Training loss: 0.10956982523202896\n",
      "Training loss: 0.1088317334651947\n",
      "Training loss: 0.10657716542482376\n",
      "Training loss: 0.10592500120401382\n",
      "Training loss: 0.10101177543401718\n",
      "Training loss: 0.10744460672140121\n",
      "Training loss: 0.10732898861169815\n",
      "Training loss: 0.12154743820428848\n",
      "Training loss: 0.10591917484998703\n",
      "Training loss: 0.10684625059366226\n",
      "Training loss: 0.11652685701847076\n",
      "Training loss: 0.10741367936134338\n",
      "Training loss: 0.10574513673782349\n",
      "Training loss: 0.10310102999210358\n",
      "Training loss: 0.105924092233181\n",
      "Training loss: 0.10243312269449234\n",
      "Training loss: 0.10476062446832657\n",
      "Training loss: 0.11004983633756638\n",
      "Training loss: 0.10719787329435349\n",
      "Training loss: 0.1142384484410286\n",
      "Training loss: 0.10341189056634903\n",
      "Training loss: 0.1072617694735527\n",
      "Training loss: 0.11558812111616135\n",
      "Training loss: 0.10163193196058273\n",
      "Training loss: 0.1097252368927002\n",
      "Training loss: 0.1057085171341896\n",
      "Training loss: 0.10436129570007324\n",
      "Training loss: 0.11195207387208939\n",
      "Training loss: 0.10117526352405548\n",
      "Training loss: 0.10402681678533554\n",
      "Training loss: 0.10167037695646286\n",
      "Training loss: 0.1077800840139389\n",
      "Training loss: 0.10308606177568436\n",
      "Training loss: 0.11009163409471512\n",
      "Training loss: 0.10831544548273087\n",
      "Training loss: 0.1001788079738617\n",
      "Training loss: 0.10067880153656006\n",
      "Training loss: 0.11111439764499664\n",
      "Training loss: 0.103640116751194\n",
      "Training loss: 0.10741280019283295\n",
      "Training loss: 0.10287665575742722\n",
      "Training loss: 0.10439908504486084\n",
      "Training loss: 0.102020263671875\n",
      "Training loss: 0.10940524190664291\n",
      "Training loss: 0.1018902137875557\n",
      "Training loss: 0.109375961124897\n",
      "Training loss: 0.10132942348718643\n",
      "Training loss: 0.10010550171136856\n",
      "Training loss: 0.10347738862037659\n",
      "Training loss: 0.10781529545783997\n",
      "Training loss: 0.10814210027456284\n",
      "Training loss: 0.1033971756696701\n",
      "Training loss: 0.11097709834575653\n",
      "Training loss: 0.10604842007160187\n",
      "Training loss: 0.10209133476018906\n",
      "Training loss: 0.10685709118843079\n",
      "Training loss: 0.12832118570804596\n",
      "Training loss: 0.09963890165090561\n",
      "Training loss: 0.11822537332773209\n",
      "Training loss: 0.09899545460939407\n",
      "Training loss: 0.10099052637815475\n",
      "Training loss: 0.1041371300816536\n",
      "Training loss: 0.10591506212949753\n",
      "Training loss: 0.10551594942808151\n",
      "Training loss: 0.10558417439460754\n",
      "Training loss: 0.10684415698051453\n",
      "Training loss: 0.10188266634941101\n",
      "Training loss: 0.100584976375103\n",
      "Training loss: 0.10116668790578842\n",
      "Training loss: 0.10285362601280212\n",
      "Training loss: 0.10797750949859619\n",
      "Training loss: 0.10830902308225632\n",
      "Training loss: 0.09975580126047134\n",
      "Training loss: 0.10522770136594772\n",
      "Training loss: 0.10445195436477661\n",
      "Training loss: 0.10384207963943481\n",
      "Training loss: 0.09718053042888641\n",
      "Training loss: 0.10393617302179337\n",
      "Training loss: 0.10665781050920486\n",
      "Training loss: 0.09947417676448822\n",
      "Training loss: 0.10473781824111938\n",
      "Training loss: 0.09714806824922562\n",
      "Training loss: 0.10068276524543762\n",
      "Training loss: 0.10374251008033752\n",
      "Training loss: 0.10559810698032379\n",
      "Training loss: 0.10933724790811539\n",
      "Training loss: 0.10052158683538437\n",
      "Training loss: 0.10401361435651779\n",
      "Training loss: 0.10215950757265091\n",
      "Training loss: 0.10183341801166534\n",
      "Training loss: 0.10192815959453583\n",
      "Training loss: 0.1078028604388237\n",
      "Training loss: 0.10010407865047455\n",
      "Training loss: 0.10287395119667053\n",
      "Training loss: 0.09819374233484268\n",
      "Training loss: 0.10604816675186157\n",
      "Training loss: 0.1042180061340332\n",
      "Training loss: 0.10121282190084457\n",
      "Training loss: 0.10917218774557114\n",
      "Training loss: 0.1023898646235466\n",
      "Training loss: 0.10622994601726532\n",
      "Training loss: 0.10657405108213425\n",
      "Training loss: 0.11266425251960754\n",
      "Training loss: 0.10594706237316132\n",
      "Training loss: 0.10840179026126862\n",
      "Training loss: 0.10550139844417572\n",
      "Training loss: 0.10357698798179626\n",
      "Training loss: 0.1030697226524353\n",
      "Training loss: 0.10171588510274887\n",
      "Training loss: 0.10680627077817917\n",
      "Training loss: 0.09645398706197739\n",
      "Training loss: 0.10149865597486496\n",
      "Training loss: 0.10416995733976364\n",
      "Training loss: 0.09904434531927109\n",
      "Training loss: 0.10316213965415955\n",
      "Training loss: 0.10868804156780243\n",
      "Training loss: 0.104180708527565\n",
      "Training loss: 0.10353029519319534\n",
      "Training loss: 0.10208334773778915\n",
      "Training loss: 0.09375210106372833\n",
      "Training loss: 0.1055692657828331\n",
      "Training loss: 0.10069690644741058\n",
      "Training loss: 0.10289406776428223\n",
      "Training loss: 0.11441416293382645\n",
      "Training loss: 0.10651601105928421\n",
      "Training loss: 0.10787377506494522\n",
      "Training loss: 0.11060778051614761\n",
      "Training loss: 0.10214182734489441\n",
      "Training loss: 0.0940031036734581\n",
      "Training loss: 0.09947127103805542\n",
      "Training loss: 0.09637518227100372\n",
      "Training loss: 0.10394662618637085\n",
      "Training loss: 0.10690814256668091\n",
      "Training loss: 0.09711872041225433\n",
      "Training loss: 0.10936962068080902\n",
      "Training loss: 0.09767362475395203\n",
      "Training loss: 0.0974225178360939\n",
      "Training loss: 0.10635394603013992\n",
      "Training loss: 0.09757834672927856\n",
      "Training loss: 0.10703104734420776\n",
      "Training loss: 0.09670446068048477\n",
      "Training loss: 0.10487410426139832\n",
      "Training loss: 0.10644327849149704\n",
      "Training loss: 0.10772555321455002\n",
      "Training loss: 0.09939047694206238\n",
      "Training loss: 0.10617100447416306\n",
      "Training loss: 0.10305433720350266\n",
      "Training loss: 0.10151957720518112\n",
      "Training loss: 0.11109621077775955\n",
      "Training loss: 0.09828239679336548\n",
      "Training loss: 0.10397364944219589\n",
      "Training loss: 0.10089260339736938\n",
      "Training loss: 0.1071927547454834\n",
      "Training loss: 0.10317253321409225\n",
      "Training loss: 0.10652681440114975\n",
      "Training loss: 0.10213650017976761\n",
      "Training loss: 0.10312721878290176\n",
      "Training loss: 0.11262571811676025\n",
      "Training loss: 0.10059253126382828\n",
      "Training loss: 0.10196670144796371\n",
      "Training loss: 0.10274780541658401\n",
      "Training loss: 0.09686949104070663\n",
      "Training loss: 0.10169889777898788\n",
      "Training loss: 0.10337089747190475\n",
      "Training loss: 0.10500749200582504\n",
      "Training loss: 0.10341691970825195\n",
      "Training loss: 0.11031248420476913\n",
      "Training loss: 0.09555189311504364\n",
      "Training loss: 0.09838879853487015\n",
      "Training loss: 0.10283926874399185\n",
      "Training loss: 0.0975528359413147\n",
      "Training loss: 0.09923835843801498\n",
      "Training loss: 0.09767501056194305\n",
      "Training loss: 0.10294531285762787\n",
      "Training loss: 0.10515578091144562\n",
      "Training loss: 0.10600419342517853\n",
      "Training loss: 0.11095548421144485\n",
      "Training loss: 0.10115522891283035\n",
      "Training loss: 0.09683285653591156\n",
      "Training loss: 0.09892003238201141\n",
      "Training loss: 0.10020571947097778\n",
      "Training loss: 0.10061832517385483\n",
      "Training loss: 0.10935664176940918\n",
      "Training loss: 0.109368696808815\n",
      "Training loss: 0.10083023458719254\n",
      "Training loss: 0.10454889386892319\n",
      "Training loss: 0.10086330771446228\n",
      "Training loss: 0.10619743913412094\n",
      "Training loss: 0.10052713751792908\n",
      "Training loss: 0.0993533656001091\n",
      "Training loss: 0.10223733633756638\n",
      "Training loss: 0.09974265843629837\n",
      "Training loss: 0.10629749298095703\n",
      "Training loss: 0.10479340702295303\n",
      "Training loss: 0.09372959285974503\n",
      "Training loss: 0.09792079031467438\n",
      "Training loss: 0.09662358462810516\n",
      "Training loss: 0.09992482513189316\n",
      "Training loss: 0.10489467531442642\n",
      "Training loss: 0.10285364836454391\n",
      "Training loss: 0.10033585131168365\n",
      "Training loss: 0.10019394010305405\n",
      "Training loss: 0.09902247786521912\n",
      "Training loss: 0.11071030050516129\n",
      "Training loss: 0.10857465863227844\n",
      "Training loss: 0.09947554022073746\n",
      "Training loss: 0.09628832340240479\n",
      "Training loss: 0.10068492591381073\n",
      "Training loss: 0.10245922207832336\n",
      "Training loss: 0.10639815777540207\n",
      "Training loss: 0.09989787638187408\n",
      "Training loss: 0.10105226933956146\n",
      "Training loss: 0.104301817715168\n",
      "Training loss: 0.10608948022127151\n",
      "Training loss: 0.10367956757545471\n",
      "Training loss: 0.10293684899806976\n",
      "Training loss: 0.09900861978530884\n",
      "Training loss: 0.11176300048828125\n",
      "Training loss: 0.10311921685934067\n",
      "Training loss: 0.10289628803730011\n",
      "Training loss: 0.10610436648130417\n",
      "Training loss: 0.107109434902668\n",
      "Training loss: 0.1035296842455864\n",
      "Training loss: 0.10737396776676178\n",
      "Training loss: 0.10321245342493057\n",
      "Training loss: 0.11281196773052216\n",
      "Training loss: 0.10349185764789581\n",
      "Training loss: 0.1076308861374855\n",
      "Training loss: 0.12032971531152725\n",
      "Training loss: 0.10791492462158203\n",
      "Training loss: 0.10091504454612732\n",
      "Training loss: 0.11073122173547745\n",
      "Training loss: 0.10371023416519165\n",
      "Training loss: 0.10685508698225021\n",
      "Training loss: 0.10354044288396835\n",
      "Training loss: 0.10756325721740723\n",
      "Training loss: 0.1046544536948204\n",
      "Training loss: 0.10247945040464401\n",
      "Training loss: 0.10033678263425827\n",
      "Training loss: 0.10483750700950623\n",
      "Training loss: 0.10808627307415009\n",
      "Training loss: 0.1048477366566658\n",
      "Training loss: 0.09310219436883926\n",
      "Training loss: 0.10835834592580795\n",
      "Training loss: 0.11111265420913696\n",
      "Training loss: 0.10088536888360977\n",
      "Training loss: 0.108228400349617\n",
      "Training loss: 0.09806093573570251\n",
      "Training loss: 0.10419711470603943\n",
      "Training loss: 0.10540512949228287\n",
      "Training loss: 0.10496712476015091\n",
      "Training loss: 0.10844486206769943\n",
      "Training loss: 0.10215947777032852\n",
      "Training loss: 0.10407791286706924\n",
      "Training loss: 0.10471455752849579\n",
      "Training loss: 0.0977267473936081\n",
      "Training loss: 0.10350412130355835\n",
      "Training loss: 0.10585813969373703\n",
      "Training loss: 0.10152366012334824\n",
      "Training loss: 0.10069591552019119\n",
      "Training loss: 0.10547749698162079\n",
      "Training loss: 0.0995352640748024\n",
      "Training loss: 0.11267158389091492\n",
      "Training loss: 0.09825460612773895\n",
      "Training loss: 0.09907550364732742\n",
      "Training loss: 0.10421387106180191\n",
      "Training loss: 0.10149475187063217\n",
      "Training loss: 0.09983879327774048\n",
      "Training loss: 0.10105665773153305\n",
      "Training loss: 0.10629048198461533\n",
      "Training loss: 0.104219950735569\n",
      "Training loss: 0.10540134459733963\n",
      "Training loss: 0.10139977931976318\n",
      "Training loss: 0.09829413890838623\n",
      "Training loss: 0.1006602868437767\n",
      "Training loss: 0.10436096787452698\n",
      "Training loss: 0.10336975008249283\n",
      "Training loss: 0.10317965596914291\n",
      "Training loss: 0.10680270195007324\n",
      "Training loss: 0.09628752619028091\n",
      "Training loss: 0.1151510700583458\n",
      "Training loss: 0.10281599313020706\n",
      "Training loss: 0.09566167742013931\n",
      "Training loss: 0.10148527473211288\n",
      "Training loss: 0.09775938838720322\n",
      "Training loss: 0.10268089920282364\n",
      "Training loss: 0.09962359815835953\n",
      "Training loss: 0.09915807098150253\n",
      "Training loss: 0.10213900357484818\n",
      "Training loss: 0.11464386433362961\n",
      "Training loss: 0.10183654725551605\n",
      "Training loss: 0.1075214073061943\n",
      "Training loss: 0.10229647159576416\n",
      "Training loss: 0.1043405681848526\n",
      "Training loss: 0.10482744872570038\n",
      "Training loss: 0.10081364214420319\n",
      "Training loss: 0.10266973078250885\n",
      "Training loss: 0.10089913755655289\n",
      "Training loss: 0.10280518233776093\n",
      "Training loss: 0.1025720089673996\n",
      "Training loss: 0.1009567603468895\n",
      "Training loss: 0.09706489741802216\n",
      "Training loss: 0.09815666079521179\n",
      "Training loss: 0.09979717433452606\n",
      "Training loss: 0.094316765666008\n",
      "Training loss: 0.10420002043247223\n",
      "Training loss: 0.10456547141075134\n",
      "Training loss: 0.09788031131029129\n",
      "Training loss: 0.09796103090047836\n",
      "Training loss: 0.10936172306537628\n",
      "Training loss: 0.10235387831926346\n",
      "Training loss: 0.1019270122051239\n",
      "Training loss: 0.10783349722623825\n",
      "Training loss: 0.10066097974777222\n",
      "Training loss: 0.1037992388010025\n",
      "Training loss: 0.10247258841991425\n",
      "Training loss: 0.10010800510644913\n",
      "Training loss: 0.09831371158361435\n",
      "Training loss: 0.10236478596925735\n",
      "Training loss: 0.10537823289632797\n",
      "Training loss: 0.09374954551458359\n",
      "Training loss: 0.10428081452846527\n",
      "Training loss: 0.09897875785827637\n",
      "Training loss: 0.09373821318149567\n",
      "Training loss: 0.10834331810474396\n",
      "Training loss: 0.10457485169172287\n",
      "Training loss: 0.09965917468070984\n",
      "Training loss: 0.0990828424692154\n",
      "Training loss: 0.10122212022542953\n",
      "Training loss: 0.1094176322221756\n",
      "Training loss: 0.10615924000740051\n",
      "Training loss: 0.0985257551074028\n",
      "Training loss: 0.10477534681558609\n",
      "Training loss: 0.09754326939582825\n",
      "Training loss: 0.10764363408088684\n",
      "Training loss: 0.10185876488685608\n",
      "Training loss: 0.09899038821458817\n",
      "Training loss: 0.09700566530227661\n",
      "Training loss: 0.09996046125888824\n",
      "Training loss: 0.11255822330713272\n",
      "Training loss: 0.09803465008735657\n",
      "Training loss: 0.10689392685890198\n",
      "Training loss: 0.10247015953063965\n",
      "Training loss: 0.10123512148857117\n",
      "Training loss: 0.10392913967370987\n",
      "Training loss: 0.09951376914978027\n",
      "Training loss: 0.09771204739809036\n",
      "Training loss: 0.10385900735855103\n",
      "Training loss: 0.10289604961872101\n",
      "Training loss: 0.0967913344502449\n",
      "Training loss: 0.09866771101951599\n",
      "Training loss: 0.10300653427839279\n",
      "Training loss: 0.10149849951267242\n",
      "Training loss: 0.101832814514637\n",
      "Training loss: 0.10160073637962341\n",
      "Training loss: 0.10030202567577362\n",
      "Training loss: 0.09978295117616653\n",
      "Training loss: 0.10054074227809906\n",
      "Training loss: 0.10321690142154694\n",
      "Training loss: 0.10500191152095795\n",
      "Training loss: 0.1053706556558609\n",
      "Training loss: 0.0996285080909729\n",
      "Training loss: 0.10189136117696762\n",
      "Training loss: 0.10535262525081635\n",
      "Training loss: 0.10468249768018723\n",
      "Training loss: 0.10161761194467545\n",
      "Training loss: 0.10612034797668457\n",
      "Training loss: 0.09770525991916656\n",
      "Training loss: 0.10042636096477509\n",
      "Training loss: 0.10891613364219666\n",
      "Training loss: 0.10845237970352173\n",
      "Training loss: 0.11240284889936447\n",
      "Training loss: 0.11504017561674118\n",
      "Training loss: 0.10928955674171448\n",
      "Training loss: 0.10152499377727509\n",
      "Training loss: 0.09718901664018631\n",
      "Training loss: 0.10992767661809921\n",
      "Training loss: 0.10171393305063248\n",
      "Training loss: 0.09600602835416794\n",
      "Training loss: 0.1095581129193306\n",
      "Training loss: 0.10022252798080444\n",
      "Training loss: 0.10879293084144592\n",
      "Training loss: 0.10664103180170059\n",
      "Training loss: 0.10343023389577866\n",
      "Training loss: 0.1022142842411995\n",
      "Training loss: 0.1011396050453186\n",
      "Training loss: 0.10458948463201523\n",
      "Training loss: 0.09844076633453369\n",
      "Training loss: 0.09614298492670059\n",
      "Training loss: 0.10157271474599838\n",
      "Training loss: 0.10867058485746384\n",
      "Training loss: 0.09242377430200577\n",
      "Training loss: 0.10367999970912933\n",
      "Training loss: 0.10339301824569702\n",
      "Training loss: 0.10471824556589127\n",
      "Training loss: 0.09766051918268204\n",
      "Training loss: 0.10016277432441711\n",
      "Training loss: 0.11211029440164566\n",
      "Training loss: 0.10434640198945999\n",
      "Training loss: 0.10119357705116272\n",
      "Training loss: 0.09890320897102356\n",
      "Training loss: 0.0984206348657608\n",
      "Training loss: 0.10330647975206375\n",
      "Training loss: 0.10288264602422714\n",
      "Training loss: 0.10344172269105911\n",
      "Training loss: 0.09972723573446274\n",
      "Training loss: 0.09948567301034927\n",
      "Training loss: 0.10340964794158936\n",
      "Training loss: 0.10136497020721436\n",
      "Training loss: 0.10028425604104996\n",
      "Training loss: 0.09866183996200562\n",
      "Training loss: 0.10197733342647552\n",
      "Training loss: 0.10193399339914322\n",
      "Training loss: 0.10340844094753265\n",
      "Training loss: 0.09608997404575348\n",
      "Training loss: 0.10049424320459366\n",
      "Training loss: 0.10007298737764359\n",
      "Training loss: 0.10723531991243362\n",
      "Training loss: 0.10077245533466339\n",
      "Training loss: 0.0999031513929367\n",
      "Training loss: 0.09918169677257538\n",
      "Training loss: 0.10592979937791824\n",
      "Training loss: 0.09725134074687958\n",
      "Training loss: 0.09673623740673065\n",
      "Training loss: 0.09982955455780029\n",
      "Training loss: 0.10132157802581787\n",
      "Training loss: 0.09801464527845383\n",
      "Training loss: 0.10902233421802521\n",
      "Training loss: 0.09936952590942383\n",
      "Training loss: 0.10159505158662796\n",
      "Training loss: 0.10165350139141083\n",
      "Training loss: 0.1024371087551117\n",
      "Training loss: 0.09789910167455673\n",
      "Training loss: 0.10600613802671432\n",
      "Training loss: 0.10115793347358704\n",
      "Training loss: 0.09469279646873474\n",
      "Training loss: 0.1015489399433136\n",
      "Training loss: 0.10119295865297318\n",
      "Training loss: 0.1091943010687828\n",
      "Training loss: 0.10051515698432922\n",
      "Training loss: 0.09761783480644226\n",
      "Training loss: 0.09899226576089859\n",
      "Training loss: 0.0955684632062912\n",
      "Training loss: 0.09739964455366135\n",
      "Training loss: 0.10292874276638031\n",
      "Training loss: 0.09875643253326416\n",
      "Training loss: 0.10141479223966599\n",
      "Training loss: 0.09213864803314209\n",
      "Training loss: 0.09739578515291214\n",
      "Training loss: 0.10619702935218811\n",
      "Training loss: 0.10140249878168106\n",
      "Training loss: 0.10772933810949326\n",
      "Training loss: 0.09912525862455368\n",
      "Training loss: 0.09978921711444855\n",
      "Training loss: 0.09519414603710175\n",
      "Training loss: 0.1062706857919693\n",
      "Training loss: 0.0947108194231987\n",
      "Training loss: 0.095417320728302\n",
      "Training loss: 0.10305410623550415\n",
      "Training loss: 0.09668419510126114\n",
      "Training loss: 0.09560468047857285\n",
      "Training loss: 0.09676843881607056\n",
      "Training loss: 0.10061348974704742\n",
      "Training loss: 0.094744011759758\n",
      "Training loss: 0.10323027521371841\n",
      "Training loss: 0.10478653758764267\n",
      "Training loss: 0.0980185717344284\n",
      "Training loss: 0.09832792729139328\n",
      "Training loss: 0.09245430678129196\n",
      "Training loss: 0.09469188004732132\n",
      "Training loss: 0.0971500352025032\n",
      "Training loss: 0.09543899446725845\n",
      "Training loss: 0.10393193364143372\n",
      "Training loss: 0.09373625367879868\n",
      "Training loss: 0.09683658927679062\n",
      "Training loss: 0.09643632918596268\n",
      "Training loss: 0.10667356103658676\n",
      "Training loss: 0.10877390950918198\n",
      "Training loss: 0.10645605623722076\n",
      "Training loss: 0.10433875024318695\n",
      "Training loss: 0.10627495497465134\n",
      "Training loss: 0.10579539090394974\n",
      "Training loss: 0.10337754338979721\n",
      "Training loss: 0.10350719839334488\n",
      "Training loss: 0.10486932843923569\n",
      "Training loss: 0.09836068004369736\n",
      "Training loss: 0.09535843878984451\n",
      "Training loss: 0.10273382812738419\n",
      "Training loss: 0.10352076590061188\n",
      "Training loss: 0.0959606021642685\n",
      "Training loss: 0.09677489101886749\n",
      "Training loss: 0.10644488781690598\n",
      "Training loss: 0.09576047956943512\n",
      "Training loss: 0.0972241759300232\n",
      "Training loss: 0.108999103307724\n",
      "Training loss: 0.09767404198646545\n",
      "Training loss: 0.09648732841014862\n",
      "Training loss: 0.10414644330739975\n",
      "Training loss: 0.09624569863080978\n",
      "Training loss: 0.09915515035390854\n",
      "Training loss: 0.10237065702676773\n",
      "Training loss: 0.09463461488485336\n",
      "Training loss: 0.10089671611785889\n",
      "Training loss: 0.1033557802438736\n",
      "Training loss: 0.0945000872015953\n",
      "Training loss: 0.10014446824789047\n",
      "Training loss: 0.09646736830472946\n",
      "Training loss: 0.09926509857177734\n",
      "Training loss: 0.09596169739961624\n",
      "Training loss: 0.10026782006025314\n",
      "Training loss: 0.10130254924297333\n",
      "Training loss: 0.09802974760532379\n",
      "Training loss: 0.10619239509105682\n",
      "Training loss: 0.09538096189498901\n",
      "Training loss: 0.1027650311589241\n",
      "Training loss: 0.10091333091259003\n",
      "Training loss: 0.10327548533678055\n",
      "Training loss: 0.09606370329856873\n",
      "Training loss: 0.09911289811134338\n",
      "Training loss: 0.11033044010400772\n",
      "Training loss: 0.11411955207586288\n",
      "Training loss: 0.10417748242616653\n",
      "Training loss: 0.09774105995893478\n",
      "Training loss: 0.10711301118135452\n",
      "Training loss: 0.1070442646741867\n",
      "Training loss: 0.10582762956619263\n",
      "Training loss: 0.10593956708908081\n",
      "Training loss: 0.09934941679239273\n",
      "Training loss: 0.09515676647424698\n",
      "Training loss: 0.09707841277122498\n",
      "Training loss: 0.09756621718406677\n",
      "Training loss: 0.1060837134718895\n",
      "Training loss: 0.10048035532236099\n",
      "Training loss: 0.09816396981477737\n",
      "Training loss: 0.10155012458562851\n",
      "Training loss: 0.10122060030698776\n",
      "Training loss: 0.09793314337730408\n",
      "Training loss: 0.09476455301046371\n",
      "Training loss: 0.09737665951251984\n",
      "Training loss: 0.10260497033596039\n",
      "Training loss: 0.09986243396997452\n",
      "Training loss: 0.09348153322935104\n",
      "Training loss: 0.10040925443172455\n",
      "Training loss: 0.0925646647810936\n",
      "Training loss: 0.10394097864627838\n",
      "Training loss: 0.09972085058689117\n",
      "Training loss: 0.09160418808460236\n",
      "Training loss: 0.10066671669483185\n",
      "Training loss: 0.10528828948736191\n",
      "Training loss: 0.09529350697994232\n",
      "Training loss: 0.09770340472459793\n",
      "Training loss: 0.09851274639368057\n",
      "Training loss: 0.09981226921081543\n",
      "Training loss: 0.10415694117546082\n",
      "Training loss: 0.09700056910514832\n",
      "Training loss: 0.09953521937131882\n",
      "Training loss: 0.10634337365627289\n",
      "Training loss: 0.10490183532238007\n",
      "Training loss: 0.10205890983343124\n",
      "Training loss: 0.09933802485466003\n",
      "Training loss: 0.0971207320690155\n",
      "Training loss: 0.10135432332754135\n",
      "Training loss: 0.09592922776937485\n",
      "Training loss: 0.10657308250665665\n",
      "Training loss: 0.09389030933380127\n",
      "Training loss: 0.09881100803613663\n",
      "Training loss: 0.1016816645860672\n",
      "Training loss: 0.10867530107498169\n",
      "Training loss: 0.10256640613079071\n",
      "Training loss: 0.10763267427682877\n",
      "Training loss: 0.11042457073926926\n",
      "Training loss: 0.10063344985246658\n",
      "Training loss: 0.09790686517953873\n",
      "Training loss: 0.09246278554201126\n",
      "Training loss: 0.10095963627099991\n",
      "Training loss: 0.0976296216249466\n",
      "Training loss: 0.10341985523700714\n",
      "Training loss: 0.10397616028785706\n",
      "Training loss: 0.09897111356258392\n",
      "Training loss: 0.09809213876724243\n",
      "Training loss: 0.0953136757016182\n",
      "Training loss: 0.1047687977552414\n",
      "Training loss: 0.09857500344514847\n",
      "Training loss: 0.09685462713241577\n",
      "Training loss: 0.10203379392623901\n",
      "Training loss: 0.10233522951602936\n",
      "Training loss: 0.09916076809167862\n",
      "Training loss: 0.0969519093632698\n",
      "Training loss: 0.10137195885181427\n",
      "Training loss: 0.09531479328870773\n",
      "Training loss: 0.09982497990131378\n",
      "Training loss: 0.09824245423078537\n",
      "Training loss: 0.10242278128862381\n",
      "Training loss: 0.10009189695119858\n",
      "Training loss: 0.10357306152582169\n",
      "Training loss: 0.09577762335538864\n",
      "Training loss: 0.09780729562044144\n",
      "Training loss: 0.09901491552591324\n",
      "Training loss: 0.09667186439037323\n",
      "Training loss: 0.10238813608884811\n",
      "Training loss: 0.0985589474439621\n",
      "Training loss: 0.1000572070479393\n",
      "Training loss: 0.0978318601846695\n",
      "Training loss: 0.1006738692522049\n",
      "Training loss: 0.09253036975860596\n",
      "Training loss: 0.09635277092456818\n",
      "Training loss: 0.09281296283006668\n",
      "Training loss: 0.09671725332736969\n",
      "Training loss: 0.09846752882003784\n",
      "Training loss: 0.10139359533786774\n",
      "Training loss: 0.10586417466402054\n",
      "Training loss: 0.09336783736944199\n",
      "Training loss: 0.09970343112945557\n",
      "Training loss: 0.11622881144285202\n",
      "Training loss: 0.0953981876373291\n",
      "Training loss: 0.09661398828029633\n",
      "Training loss: 0.10096250474452972\n",
      "Training loss: 0.09994200617074966\n",
      "Training loss: 0.09370745718479156\n",
      "Training loss: 0.09592381119728088\n",
      "Training loss: 0.09672797471284866\n",
      "Training loss: 0.10218559950590134\n",
      "Training loss: 0.09650682657957077\n",
      "Training loss: 0.09714388847351074\n",
      "Training loss: 0.1087660938501358\n",
      "Training loss: 0.09873387962579727\n",
      "Training loss: 0.09241451323032379\n",
      "Training loss: 0.097684845328331\n",
      "Training loss: 0.10370718687772751\n",
      "Training loss: 0.09529878944158554\n",
      "Training loss: 0.09614120423793793\n",
      "Training loss: 0.10045367479324341\n",
      "Training loss: 0.09890221059322357\n",
      "Training loss: 0.10110495239496231\n",
      "Training loss: 0.10415871441364288\n",
      "Training loss: 0.10510309040546417\n",
      "Training loss: 0.1006496399641037\n",
      "Training loss: 0.09992299973964691\n",
      "Training loss: 0.10018502920866013\n",
      "Training loss: 0.09256134182214737\n",
      "Training loss: 0.09511477500200272\n",
      "Training loss: 0.09843941777944565\n",
      "Training loss: 0.09716103971004486\n",
      "Training loss: 0.10093307495117188\n",
      "Training loss: 0.09561827033758163\n",
      "Training loss: 0.09095831215381622\n",
      "Training loss: 0.09826096892356873\n",
      "Training loss: 0.09402850270271301\n",
      "Training loss: 0.09911293536424637\n",
      "Training loss: 0.09448795020580292\n",
      "Training loss: 0.09150021523237228\n",
      "Training loss: 0.09981507062911987\n",
      "Training loss: 0.10581266134977341\n",
      "Training loss: 0.1067061722278595\n",
      "Training loss: 0.10019417852163315\n",
      "Training loss: 0.09343935549259186\n",
      "Training loss: 0.09873852133750916\n",
      "Training loss: 0.09613464772701263\n",
      "Training loss: 0.09981606155633926\n",
      "Training loss: 0.09595295041799545\n",
      "Training loss: 0.09946504980325699\n",
      "Training loss: 0.10102996230125427\n",
      "Training loss: 0.10124745219945908\n",
      "Training loss: 0.10534673929214478\n",
      "Training loss: 0.09381704032421112\n",
      "Training loss: 0.09600763022899628\n",
      "Training loss: 0.10237231850624084\n",
      "Training loss: 0.10003454983234406\n",
      "Training loss: 0.10721676051616669\n",
      "Training loss: 0.09804093837738037\n",
      "Training loss: 0.09398486465215683\n",
      "Training loss: 0.09754960238933563\n",
      "Training loss: 0.09750677645206451\n",
      "Training loss: 0.0998372882604599\n",
      "Training loss: 0.10636619478464127\n",
      "Training loss: 0.09983054548501968\n",
      "Training loss: 0.10384601354598999\n",
      "Training loss: 0.10256090760231018\n",
      "Training loss: 0.09497957676649094\n",
      "Training loss: 0.09976646304130554\n",
      "Training loss: 0.10369857400655746\n",
      "Training loss: 0.1033962145447731\n",
      "Training loss: 0.10421230643987656\n",
      "Training loss: 0.1067098006606102\n",
      "Training loss: 0.1033373698592186\n",
      "Training loss: 0.10088420659303665\n",
      "Training loss: 0.10640965402126312\n",
      "Training loss: 0.10400305688381195\n",
      "Training loss: 0.10083810240030289\n",
      "Training loss: 0.10274142026901245\n",
      "Training loss: 0.10684049129486084\n",
      "Training loss: 0.10847532004117966\n",
      "Training loss: 0.10727860033512115\n",
      "Training loss: 0.09926995635032654\n",
      "Training loss: 0.10236749053001404\n",
      "Training loss: 0.09794950485229492\n",
      "Training loss: 0.103099025785923\n",
      "Training loss: 0.09962800145149231\n",
      "Training loss: 0.10133874416351318\n",
      "Training loss: 0.09879773110151291\n",
      "Training loss: 0.1036529615521431\n",
      "Training loss: 0.10207059979438782\n",
      "Training loss: 0.09334489703178406\n",
      "Training loss: 0.10623260587453842\n",
      "Training loss: 0.10213102400302887\n",
      "Training loss: 0.10222756862640381\n",
      "Training loss: 0.09795025736093521\n",
      "Training loss: 0.10272558778524399\n",
      "Training loss: 0.10627568513154984\n",
      "Training loss: 0.1097187027335167\n",
      "Training loss: 0.09674592316150665\n",
      "Training loss: 0.09830524772405624\n",
      "Training loss: 0.09632843732833862\n",
      "Training loss: 0.09051857888698578\n",
      "Training loss: 0.09544374793767929\n",
      "Training loss: 0.09654104709625244\n",
      "Training loss: 0.09348145127296448\n",
      "Training loss: 0.09843567758798599\n",
      "Training loss: 0.10060378909111023\n",
      "Training loss: 0.10063480585813522\n",
      "Training loss: 0.10022236406803131\n",
      "Training loss: 0.0943387970328331\n",
      "Training loss: 0.09568755328655243\n",
      "Training loss: 0.09505932033061981\n",
      "Training loss: 0.09897183626890182\n",
      "Training loss: 0.12632934749126434\n",
      "Training loss: 0.09892632812261581\n",
      "Training loss: 0.09724097698926926\n",
      "Training loss: 0.11554893851280212\n",
      "Training loss: 0.09763303399085999\n",
      "Training loss: 0.10939668118953705\n",
      "Training loss: 0.10327594727277756\n",
      "Training loss: 0.10485153645277023\n",
      "Training loss: 0.10307902097702026\n",
      "Training loss: 0.10505592823028564\n",
      "Training loss: 0.1031523272395134\n",
      "Training loss: 0.09937825053930283\n",
      "Training loss: 0.09946317225694656\n",
      "Training loss: 0.10099057108163834\n",
      "Training loss: 0.10675625503063202\n",
      "Training loss: 0.11504179239273071\n",
      "Training loss: 0.10328885167837143\n",
      "Training loss: 0.10518872737884521\n",
      "Training loss: 0.10114194452762604\n",
      "Training loss: 0.10537321865558624\n",
      "Training loss: 0.11328914761543274\n",
      "Training loss: 0.09889186173677444\n",
      "Training loss: 0.10753903537988663\n",
      "Training loss: 0.10292204469442368\n",
      "Training loss: 0.10430053621530533\n",
      "Training loss: 0.10592584311962128\n",
      "Training loss: 0.11271054297685623\n",
      "Training loss: 0.10474420338869095\n",
      "Training loss: 0.09315192699432373\n",
      "Training loss: 0.10454614460468292\n",
      "Training loss: 0.1074298843741417\n",
      "Training loss: 0.10433098673820496\n",
      "Training loss: 0.10422632843255997\n",
      "Training loss: 0.09735213965177536\n",
      "Training loss: 0.10610217601060867\n",
      "Training loss: 0.10794216394424438\n",
      "Training loss: 0.11255320906639099\n",
      "Training loss: 0.10529676824808121\n",
      "Training loss: 0.10621142387390137\n",
      "Training loss: 0.09723562002182007\n",
      "Training loss: 0.10901666432619095\n",
      "Training loss: 0.09943458437919617\n",
      "Training loss: 0.09907181560993195\n",
      "Training loss: 0.09443096071481705\n",
      "Training loss: 0.10079141706228256\n",
      "Training loss: 0.09998151659965515\n",
      "Training loss: 0.10101916640996933\n",
      "Training loss: 0.10034555941820145\n",
      "Training loss: 0.10169830173254013\n",
      "Training loss: 0.09797823429107666\n",
      "Training loss: 0.09889043122529984\n",
      "Training loss: 0.10099455714225769\n",
      "Training loss: 0.09675230830907822\n",
      "Training loss: 0.10289991647005081\n",
      "Training loss: 0.10558782517910004\n",
      "Training loss: 0.10018381476402283\n",
      "Training loss: 0.10016460716724396\n",
      "Training loss: 0.09286908060312271\n",
      "Training loss: 0.09367793798446655\n",
      "Training loss: 0.09371192753314972\n",
      "Training loss: 0.09800703823566437\n",
      "Training loss: 0.09716121852397919\n",
      "Training loss: 0.09457392990589142\n",
      "Training loss: 0.10242169350385666\n",
      "Training loss: 0.10222528874874115\n",
      "Training loss: 0.09204229712486267\n",
      "Training loss: 0.09783522039651871\n",
      "Training loss: 0.09644389897584915\n",
      "Training loss: 0.10380581766366959\n",
      "Training loss: 0.09861341863870621\n",
      "Training loss: 0.10623400658369064\n",
      "Training loss: 0.09634017199277878\n",
      "Training loss: 0.10240082442760468\n",
      "Training loss: 0.10448405891656876\n",
      "Training loss: 0.09443912655115128\n",
      "Training loss: 0.10157063603401184\n",
      "Training loss: 0.11864466965198517\n",
      "Training loss: 0.10199009627103806\n",
      "Training loss: 0.09823912382125854\n",
      "Training loss: 0.10314104706048965\n",
      "Training loss: 0.09775383025407791\n",
      "Training loss: 0.10086344927549362\n",
      "Training loss: 0.11422262340784073\n",
      "Training loss: 0.10301942378282547\n",
      "Training loss: 0.10580062866210938\n",
      "Training loss: 0.10392377525568008\n",
      "Training loss: 0.10659731924533844\n",
      "Training loss: 0.10714162141084671\n",
      "Training loss: 0.09888383746147156\n",
      "Training loss: 0.10619746893644333\n",
      "Training loss: 0.10211608558893204\n",
      "Training loss: 0.09636605530977249\n",
      "Training loss: 0.1101984903216362\n",
      "Training loss: 0.10257256031036377\n",
      "Training loss: 0.10273056477308273\n",
      "Training loss: 0.09889520704746246\n",
      "Training loss: 0.09594374895095825\n",
      "Training loss: 0.10152573138475418\n",
      "Training loss: 0.10576539486646652\n",
      "Training loss: 0.10414859652519226\n",
      "Training loss: 0.1027497723698616\n",
      "Training loss: 0.10557291656732559\n",
      "Training loss: 0.10429766029119492\n",
      "Training loss: 0.09726633131504059\n",
      "Training loss: 0.1017415001988411\n",
      "Training loss: 0.09269241243600845\n",
      "Training loss: 0.10232309252023697\n",
      "Training loss: 0.10026311129331589\n",
      "Training loss: 0.10102544724941254\n",
      "Training loss: 0.10076846182346344\n",
      "Training loss: 0.10196676850318909\n",
      "Training loss: 0.09215542674064636\n",
      "Training loss: 0.09827274084091187\n",
      "Training loss: 0.10252853482961655\n",
      "Training loss: 0.09633959084749222\n",
      "Training loss: 0.09381792694330215\n",
      "Training loss: 0.10137288272380829\n",
      "Training loss: 0.09587876498699188\n",
      "Training loss: 0.09573734551668167\n",
      "Training loss: 0.09519123286008835\n",
      "Training loss: 0.09095193445682526\n",
      "Training loss: 0.0927618071436882\n",
      "Training loss: 0.09583725035190582\n",
      "Training loss: 0.10343525558710098\n",
      "Training loss: 0.1010780930519104\n",
      "Training loss: 0.09356030821800232\n",
      "Training loss: 0.10149107873439789\n",
      "Training loss: 0.09485267847776413\n",
      "Training loss: 0.09380274266004562\n",
      "Training loss: 0.10725583881139755\n",
      "Training loss: 0.09877166152000427\n",
      "Training loss: 0.10111811012029648\n",
      "Training loss: 0.10700346529483795\n",
      "Training loss: 0.09671333432197571\n",
      "Training loss: 0.09725850075483322\n",
      "Training loss: 0.10263200104236603\n",
      "Training loss: 0.0957569032907486\n",
      "Training loss: 0.10208690911531448\n",
      "Training loss: 0.10082855075597763\n",
      "Training loss: 0.09457723051309586\n",
      "Training loss: 0.10275015980005264\n",
      "Training loss: 0.09516642242670059\n",
      "Training loss: 0.10274244099855423\n",
      "Training loss: 0.09163766354322433\n",
      "Training loss: 0.0953298807144165\n",
      "Training loss: 0.10040941834449768\n",
      "Training loss: 0.1036476194858551\n",
      "Training loss: 0.09918346256017685\n",
      "Training loss: 0.10322488099336624\n",
      "Training loss: 0.0979408323764801\n",
      "Training loss: 0.09885942190885544\n",
      "Training loss: 0.0936155691742897\n",
      "Training loss: 0.09232357889413834\n",
      "Training loss: 0.1000022441148758\n",
      "Training loss: 0.10156543552875519\n",
      "Training loss: 0.10163553059101105\n",
      "Training loss: 0.10213009268045425\n",
      "Training loss: 0.09489893913269043\n",
      "Training loss: 0.10171778500080109\n",
      "Training loss: 0.09513480961322784\n",
      "Training loss: 0.09579851478338242\n",
      "Training loss: 0.10079510509967804\n",
      "Training loss: 0.09897448122501373\n",
      "Training loss: 0.09464798122644424\n",
      "Training loss: 0.09197050333023071\n",
      "Training loss: 0.09564803540706635\n",
      "Training loss: 0.09389760345220566\n",
      "Training loss: 0.09260382503271103\n",
      "Training loss: 0.09099717438220978\n",
      "Training loss: 0.09823621064424515\n",
      "Training loss: 0.09445470571517944\n",
      "Training loss: 0.09847322851419449\n",
      "Training loss: 0.09764771163463593\n",
      "Training loss: 0.0901990458369255\n",
      "Training loss: 0.09219798445701599\n",
      "Training loss: 0.09819699823856354\n",
      "Training loss: 0.10123168677091599\n",
      "Training loss: 0.0949271097779274\n",
      "Training loss: 0.09951408207416534\n",
      "Training loss: 0.10346497595310211\n",
      "Training loss: 0.09362643212080002\n",
      "Training loss: 0.10888725519180298\n",
      "Training loss: 0.09575235098600388\n",
      "Training loss: 0.09478949010372162\n",
      "Training loss: 0.10709167271852493\n",
      "Training loss: 0.0983259379863739\n",
      "Training loss: 0.09641057252883911\n",
      "Training loss: 0.10124745965003967\n",
      "Training loss: 0.10551628470420837\n",
      "Training loss: 0.10639340430498123\n",
      "Training loss: 0.10571575164794922\n",
      "Training loss: 0.10454954206943512\n",
      "Training loss: 0.10395735502243042\n",
      "Training loss: 0.10366933047771454\n",
      "Training loss: 0.09563526511192322\n",
      "Training loss: 0.09611938893795013\n",
      "Training loss: 0.10663919150829315\n",
      "Training loss: 0.10302627831697464\n",
      "Training loss: 0.09940848499536514\n",
      "Training loss: 0.09676899760961533\n",
      "Training loss: 0.09546906501054764\n",
      "Training loss: 0.09418292343616486\n",
      "Training loss: 0.09554313868284225\n",
      "Training loss: 0.09444575011730194\n",
      "Training loss: 0.09897950291633606\n",
      "Training loss: 0.09300271421670914\n",
      "Training loss: 0.10004810988903046\n",
      "Training loss: 0.09531407803297043\n",
      "Training loss: 0.0982854813337326\n",
      "Training loss: 0.0945436954498291\n",
      "Training loss: 0.09563691169023514\n",
      "Training loss: 0.09365633130073547\n",
      "Training loss: 0.09517180919647217\n",
      "Training loss: 0.1026100292801857\n",
      "Training loss: 0.10481249541044235\n",
      "Training loss: 0.09551199525594711\n",
      "Training loss: 0.09719125181436539\n",
      "Training loss: 0.09850428253412247\n",
      "Training loss: 0.09871219843626022\n",
      "Training loss: 0.10204584151506424\n",
      "Training loss: 0.10260938107967377\n",
      "Training loss: 0.0993107408285141\n",
      "Training loss: 0.09030116349458694\n",
      "Training loss: 0.10077036917209625\n",
      "Training loss: 0.09605434536933899\n",
      "Training loss: 0.09864877164363861\n",
      "Training loss: 0.09817379713058472\n",
      "Training loss: 0.09997916966676712\n",
      "Training loss: 0.10781577229499817\n",
      "Training loss: 0.10126592218875885\n",
      "Training loss: 0.10099834948778152\n",
      "Training loss: 0.09505222737789154\n",
      "Training loss: 0.10001060366630554\n",
      "Training loss: 0.10228048264980316\n",
      "Training loss: 0.1000543162226677\n",
      "Training loss: 0.10633347928524017\n",
      "Training loss: 0.09992245584726334\n",
      "Training loss: 0.09641715884208679\n",
      "Training loss: 0.10086532682180405\n",
      "Training loss: 0.1065601035952568\n",
      "Training loss: 0.11161249130964279\n",
      "Training loss: 0.11303761601448059\n",
      "Training loss: 0.09511061757802963\n",
      "Training loss: 0.0936380922794342\n",
      "Training loss: 0.10626815259456635\n",
      "Training loss: 0.09729478508234024\n",
      "Training loss: 0.09570227563381195\n",
      "Training loss: 0.09459391981363297\n",
      "Training loss: 0.10484572499990463\n",
      "Training loss: 0.10495985299348831\n",
      "Training loss: 0.09892061352729797\n",
      "Training loss: 0.10661518573760986\n",
      "Training loss: 0.10085631906986237\n",
      "Training loss: 0.09928825497627258\n",
      "Training loss: 0.10455559194087982\n",
      "Training loss: 0.10733552277088165\n",
      "Training loss: 0.10188876837491989\n",
      "Training loss: 0.10547291487455368\n",
      "Training loss: 0.10306493192911148\n",
      "Training loss: 0.10069509595632553\n",
      "Training loss: 0.09261893481016159\n",
      "Training loss: 0.09733892977237701\n",
      "Training loss: 0.09984591603279114\n",
      "Training loss: 0.09699955582618713\n",
      "Training loss: 0.1064944863319397\n",
      "Training loss: 0.10217169672250748\n",
      "Training loss: 0.10240837186574936\n",
      "Training loss: 0.10763353109359741\n",
      "Training loss: 0.10599002987146378\n",
      "Training loss: 0.10446912050247192\n",
      "Training loss: 0.11558972299098969\n",
      "Training loss: 0.10786568373441696\n",
      "Training loss: 0.10198363661766052\n",
      "Training loss: 0.10319601744413376\n",
      "Training loss: 0.10616929084062576\n",
      "Training loss: 0.10251118987798691\n",
      "Training loss: 0.10401201248168945\n",
      "Training loss: 0.10536853224039078\n",
      "Training loss: 0.09890148788690567\n",
      "Training loss: 0.10133775323629379\n",
      "Training loss: 0.09751801192760468\n",
      "Training loss: 0.11376100033521652\n",
      "Training loss: 0.10310433804988861\n",
      "Training loss: 0.10197564214468002\n",
      "Training loss: 0.10341228544712067\n",
      "Training loss: 0.107694111764431\n",
      "Training loss: 0.09936939179897308\n",
      "Training loss: 0.09959493577480316\n",
      "Training loss: 0.1073787584900856\n",
      "Training loss: 0.10769397020339966\n",
      "Training loss: 0.1016666516661644\n",
      "Training loss: 0.11067301779985428\n",
      "Training loss: 0.09910206496715546\n",
      "Training loss: 0.10669344663619995\n",
      "Training loss: 0.10521434247493744\n",
      "Training loss: 0.09798353910446167\n",
      "Training loss: 0.10277099162340164\n",
      "Training loss: 0.09726990759372711\n",
      "Training loss: 0.10380581021308899\n",
      "Training loss: 0.09917349368333817\n",
      "Training loss: 0.09955047070980072\n",
      "Training loss: 0.09982488304376602\n",
      "Training loss: 0.09967629611492157\n",
      "Training loss: 0.10661760717630386\n",
      "Training loss: 0.09978640079498291\n",
      "Training loss: 0.09879807382822037\n",
      "Training loss: 0.10731994360685349\n",
      "Training loss: 0.11171316355466843\n",
      "Training loss: 0.10541985183954239\n",
      "Training loss: 0.09859578311443329\n",
      "Training loss: 0.10236428678035736\n",
      "Training loss: 0.09942981600761414\n",
      "Training loss: 0.09936625510454178\n",
      "Training loss: 0.10647980868816376\n",
      "Training loss: 0.10225541144609451\n",
      "Training loss: 0.10015788674354553\n",
      "Training loss: 0.09225407242774963\n",
      "Training loss: 0.10085327923297882\n",
      "Training loss: 0.10294797271490097\n",
      "Training loss: 0.10401371866464615\n",
      "Training loss: 0.102519690990448\n",
      "Training loss: 0.10169344395399094\n",
      "Training loss: 0.10617455095052719\n",
      "Training loss: 0.09680548310279846\n",
      "Training loss: 0.10236664861440659\n",
      "Training loss: 0.10154478996992111\n",
      "Training loss: 0.10185732692480087\n",
      "Training loss: 0.10730066150426865\n",
      "Training loss: 0.10235574841499329\n",
      "Training loss: 0.09617665410041809\n",
      "Training loss: 0.0994381308555603\n",
      "Training loss: 0.101430244743824\n",
      "Training loss: 0.0989522635936737\n",
      "Training loss: 0.09707481414079666\n",
      "Training loss: 0.10096342861652374\n",
      "Training loss: 0.1214337944984436\n",
      "Training loss: 0.09630517661571503\n",
      "Training loss: 0.09285473823547363\n",
      "Training loss: 0.09745702892541885\n",
      "Training loss: 0.10777970403432846\n",
      "Training loss: 0.09931350499391556\n",
      "Training loss: 0.10247204452753067\n",
      "Training loss: 0.10033275932073593\n",
      "Training loss: 0.10215234011411667\n",
      "Training loss: 0.10255037993192673\n",
      "Training loss: 0.10162418335676193\n",
      "Training loss: 0.09936773777008057\n",
      "Training loss: 0.10791594535112381\n",
      "Training loss: 0.10772779583930969\n",
      "Training loss: 0.10628186166286469\n",
      "Training loss: 0.09792347252368927\n",
      "Training loss: 0.09862067550420761\n",
      "Training loss: 0.10366372764110565\n",
      "Training loss: 0.10039069503545761\n",
      "Training loss: 0.09954208880662918\n",
      "Training loss: 0.10184863209724426\n",
      "Training loss: 0.0977606251835823\n",
      "Training loss: 0.095454640686512\n",
      "Training loss: 0.10003920644521713\n",
      "Training loss: 0.09776239097118378\n",
      "Training loss: 0.09564652293920517\n",
      "Training loss: 0.10206688940525055\n",
      "Training loss: 0.10409345477819443\n",
      "Training loss: 0.09445037692785263\n",
      "Training loss: 0.09501201659440994\n",
      "Training loss: 0.09837404638528824\n",
      "Training loss: 0.09672818332910538\n",
      "Training loss: 0.10243114829063416\n",
      "Training loss: 0.09613088518381119\n",
      "Training loss: 0.0967811569571495\n",
      "Training loss: 0.10463134944438934\n",
      "Training loss: 0.08956271409988403\n",
      "Training loss: 0.09075183421373367\n",
      "Training loss: 0.10045818239450455\n",
      "Training loss: 0.09098158776760101\n",
      "Training loss: 0.10127843171358109\n",
      "Training loss: 0.09282500296831131\n",
      "Training loss: 0.11119135469198227\n",
      "Training loss: 0.09148642420768738\n",
      "Training loss: 0.09361272305250168\n",
      "Training loss: 0.09364780783653259\n",
      "Training loss: 0.09443621337413788\n",
      "Training loss: 0.09885254502296448\n",
      "Training loss: 0.09661094844341278\n",
      "Training loss: 0.09798010438680649\n",
      "Training loss: 0.10504104942083359\n",
      "Training loss: 0.0966954305768013\n",
      "Training loss: 0.09014637023210526\n",
      "Training loss: 0.09120917320251465\n",
      "Training loss: 0.09881184250116348\n",
      "Training loss: 0.09367375820875168\n",
      "Training loss: 0.10042048990726471\n",
      "Training loss: 0.09472773224115372\n",
      "Training loss: 0.1007956862449646\n",
      "Training loss: 0.09687578678131104\n",
      "Training loss: 0.10470423847436905\n",
      "Training loss: 0.09845923632383347\n",
      "Training loss: 0.10354569554328918\n",
      "Training loss: 0.10074687004089355\n",
      "Training loss: 0.10172458738088608\n",
      "Training loss: 0.09492586553096771\n",
      "Training loss: 0.09747202694416046\n",
      "Training loss: 0.09377061575651169\n",
      "Training loss: 0.1023624837398529\n",
      "Training loss: 0.09772085398435593\n",
      "Training loss: 0.1013377234339714\n",
      "Training loss: 0.09800253063440323\n",
      "Training loss: 0.10757936537265778\n",
      "Training loss: 0.09725474566221237\n",
      "Training loss: 0.09564489871263504\n",
      "Training loss: 0.10223057866096497\n",
      "Training loss: 0.10677777230739594\n",
      "Training loss: 0.09589241445064545\n",
      "Training loss: 0.09890434890985489\n",
      "Training loss: 0.10046040266752243\n",
      "Training loss: 0.09708279371261597\n",
      "Training loss: 0.10735303163528442\n",
      "Training loss: 0.09884510189294815\n",
      "Training loss: 0.0955805554986\n",
      "Training loss: 0.10230936110019684\n",
      "Training loss: 0.09478586912155151\n",
      "Training loss: 0.10119014978408813\n",
      "Training loss: 0.09626682102680206\n",
      "Training loss: 0.09239418059587479\n",
      "Training loss: 0.09775549173355103\n",
      "Training loss: 0.09900309890508652\n",
      "Training loss: 0.10505491495132446\n",
      "Training loss: 0.09707602113485336\n",
      "Training loss: 0.10976818948984146\n",
      "Training loss: 0.09948412328958511\n",
      "Training loss: 0.0946359634399414\n",
      "Training loss: 0.09464112669229507\n",
      "Training loss: 0.08717885613441467\n",
      "Training loss: 0.09960287064313889\n",
      "Training loss: 0.10905449092388153\n",
      "Training loss: 0.10181151330471039\n",
      "Training loss: 0.10002106428146362\n",
      "Training loss: 0.09708857536315918\n",
      "Training loss: 0.09612394124269485\n",
      "Training loss: 0.0889982208609581\n",
      "Training loss: 0.09352997690439224\n",
      "Training loss: 0.09856332838535309\n",
      "Training loss: 0.09900553524494171\n",
      "Training loss: 0.10007280111312866\n",
      "Training loss: 0.08911584317684174\n",
      "Training loss: 0.09791670739650726\n",
      "Training loss: 0.08949965238571167\n",
      "Training loss: 0.09224259853363037\n",
      "Training loss: 0.09366350620985031\n",
      "Training loss: 0.09615958482027054\n",
      "Training loss: 0.09380961209535599\n",
      "Training loss: 0.09561251103878021\n",
      "Training loss: 0.09327851980924606\n",
      "Training loss: 0.09351479262113571\n",
      "Training loss: 0.09053861349821091\n",
      "Training loss: 0.09987591952085495\n",
      "Training loss: 0.08963543176651001\n",
      "Training loss: 0.09782138466835022\n",
      "Training loss: 0.08750497549772263\n",
      "Training loss: 0.09230911731719971\n",
      "Training loss: 0.10048749297857285\n",
      "Training loss: 0.09911289811134338\n",
      "Training loss: 0.10257526487112045\n",
      "Training loss: 0.09954310208559036\n",
      "Training loss: 0.08930811285972595\n",
      "Training loss: 0.09360045939683914\n",
      "Training loss: 0.09667814522981644\n",
      "Training loss: 0.09707978367805481\n",
      "Training loss: 0.08981617540121078\n",
      "Training loss: 0.09794909507036209\n",
      "Training loss: 0.0936778113245964\n",
      "Training loss: 0.09894191473722458\n",
      "Training loss: 0.09844132512807846\n",
      "Training loss: 0.09133175015449524\n",
      "Training loss: 0.09103940427303314\n",
      "Training loss: 0.0866679698228836\n",
      "Training loss: 0.09544762223958969\n",
      "Training loss: 0.09425830841064453\n",
      "Training loss: 0.09487888962030411\n",
      "Training loss: 0.09759446978569031\n",
      "Training loss: 0.09708290547132492\n",
      "Training loss: 0.1064402163028717\n",
      "Training loss: 0.0963798388838768\n",
      "Training loss: 0.09651079773902893\n",
      "Training loss: 0.09641018509864807\n",
      "Training loss: 0.1018102616071701\n",
      "Training loss: 0.09040607511997223\n",
      "Training loss: 0.09623808413743973\n",
      "Training loss: 0.09378360956907272\n",
      "Training loss: 0.10905183851718903\n",
      "Training loss: 0.0915306955575943\n",
      "Training loss: 0.09982864558696747\n",
      "Training loss: 0.09290455281734467\n",
      "Training loss: 0.09929462522268295\n",
      "Training loss: 0.09833844751119614\n",
      "Training loss: 0.09385489672422409\n",
      "Training loss: 0.09441988915205002\n",
      "Training loss: 0.09745985269546509\n",
      "Training loss: 0.09835461527109146\n",
      "Training loss: 0.10596746951341629\n",
      "Training loss: 0.10202763229608536\n",
      "Training loss: 0.09999478608369827\n",
      "Training loss: 0.09987087547779083\n",
      "Training loss: 0.10697902739048004\n",
      "Training loss: 0.09317845851182938\n",
      "Training loss: 0.09314607083797455\n",
      "Training loss: 0.09512147307395935\n",
      "Training loss: 0.10246146470308304\n",
      "Training loss: 0.09922516345977783\n",
      "Training loss: 0.09342579543590546\n",
      "Training loss: 0.09632977098226547\n",
      "Training loss: 0.10771702975034714\n",
      "Training loss: 0.09489921480417252\n",
      "Training loss: 0.09704123437404633\n",
      "Training loss: 0.09887780249118805\n",
      "Training loss: 0.12080788612365723\n",
      "Training loss: 0.09950590133666992\n",
      "Training loss: 0.09810999035835266\n",
      "Training loss: 0.10662303864955902\n",
      "Training loss: 0.10302576422691345\n",
      "Training loss: 0.09819063544273376\n",
      "Training loss: 0.10024568438529968\n",
      "Training loss: 0.10232166945934296\n",
      "Training loss: 0.09986874461174011\n",
      "Training loss: 0.09862811118364334\n",
      "Training loss: 0.09170595556497574\n",
      "Training loss: 0.09231880307197571\n",
      "Training loss: 0.09971978515386581\n",
      "Training loss: 0.0997573658823967\n",
      "Training loss: 0.100506491959095\n",
      "Training loss: 0.09877198189496994\n",
      "Training loss: 0.09676606208086014\n",
      "Training loss: 0.10480134189128876\n",
      "Training loss: 0.09946515411138535\n",
      "Training loss: 0.0945792868733406\n",
      "Training loss: 0.09449048340320587\n",
      "Training loss: 0.10008787363767624\n",
      "Training loss: 0.10022114217281342\n",
      "Training loss: 0.09679611027240753\n",
      "Training loss: 0.090200275182724\n",
      "Training loss: 0.09787974506616592\n",
      "Training loss: 0.09346206486225128\n",
      "Training loss: 0.09998303651809692\n",
      "Training loss: 0.09733765572309494\n",
      "Training loss: 0.10260535031557083\n",
      "Training loss: 0.08931327611207962\n",
      "Training loss: 0.10270971804857254\n",
      "Training loss: 0.09747587144374847\n",
      "Training loss: 0.10532829910516739\n",
      "Training loss: 0.08984674513339996\n",
      "Training loss: 0.09355733543634415\n",
      "Training loss: 0.09739013016223907\n",
      "Training loss: 0.10362200438976288\n",
      "Training loss: 0.09572941064834595\n",
      "Training loss: 0.09612973779439926\n",
      "Training loss: 0.09921811521053314\n",
      "Training loss: 0.09477543085813522\n",
      "Training loss: 0.09710383415222168\n",
      "Training loss: 0.09668953716754913\n",
      "Training loss: 0.09115629643201828\n",
      "Training loss: 0.09620704501867294\n",
      "Training loss: 0.09262658655643463\n",
      "Training loss: 0.09431812912225723\n",
      "Training loss: 0.10051945596933365\n",
      "Training loss: 0.09236149489879608\n",
      "Training loss: 0.09207621961832047\n",
      "Training loss: 0.0895276591181755\n",
      "Training loss: 0.09247813373804092\n",
      "Training loss: 0.09815949201583862\n",
      "Training loss: 0.0977814719080925\n",
      "Training loss: 0.09529319405555725\n",
      "Training loss: 0.09574776142835617\n",
      "Training loss: 0.09035094827413559\n",
      "Training loss: 0.10375101864337921\n",
      "Training loss: 0.09211088716983795\n",
      "Training loss: 0.097219318151474\n",
      "Training loss: 0.0981070026755333\n",
      "Training loss: 0.08874493837356567\n",
      "Training loss: 0.09522520005702972\n",
      "Training loss: 0.09170063585042953\n",
      "Training loss: 0.0901038646697998\n",
      "Training loss: 0.09334839880466461\n",
      "Training loss: 0.09671865403652191\n",
      "Training loss: 0.09185252338647842\n",
      "Training loss: 0.10658056288957596\n",
      "Training loss: 0.09790868312120438\n",
      "Training loss: 0.10766705870628357\n",
      "Training loss: 0.09929097443819046\n",
      "Training loss: 0.1004800945520401\n",
      "Training loss: 0.10616104304790497\n",
      "Training loss: 0.0966513380408287\n",
      "Training loss: 0.09205136448144913\n",
      "Training loss: 0.09321186691522598\n",
      "Training loss: 0.10090094804763794\n",
      "Training loss: 0.09321028739213943\n",
      "Training loss: 0.09804581105709076\n",
      "Training loss: 0.09409870952367783\n",
      "Training loss: 0.09799185395240784\n",
      "Training loss: 0.09966140240430832\n",
      "Training loss: 0.0954638347029686\n",
      "Training loss: 0.0947604775428772\n",
      "Training loss: 0.09560572355985641\n",
      "Training loss: 0.10643932968378067\n",
      "Training loss: 0.09305652230978012\n",
      "Training loss: 0.09407022595405579\n",
      "Training loss: 0.09558457881212234\n",
      "Training loss: 0.08705455809831619\n",
      "Training loss: 0.08975233137607574\n",
      "Training loss: 0.09430339187383652\n",
      "Training loss: 0.09603973478078842\n",
      "Training loss: 0.10574124753475189\n",
      "Training loss: 0.10199102014303207\n",
      "Training loss: 0.09899972379207611\n",
      "Training loss: 0.10088391602039337\n",
      "Training loss: 0.08818437904119492\n",
      "Training loss: 0.08980463445186615\n",
      "Training loss: 0.08831106126308441\n",
      "Training loss: 0.10809201747179031\n",
      "Training loss: 0.1003287211060524\n",
      "Training loss: 0.09499164670705795\n",
      "Training loss: 0.08625253289937973\n",
      "Training loss: 0.09095077961683273\n",
      "Training loss: 0.09820275753736496\n",
      "Training loss: 0.09688854217529297\n",
      "Training loss: 0.08893629908561707\n",
      "Training loss: 0.09035499393939972\n",
      "Training loss: 0.09663361310958862\n",
      "Training loss: 0.09904752671718597\n",
      "Training loss: 0.09425657987594604\n",
      "Training loss: 0.09578438848257065\n",
      "Training loss: 0.08854146301746368\n",
      "Training loss: 0.10221482068300247\n",
      "Training loss: 0.09542940557003021\n",
      "Training loss: 0.09030357003211975\n",
      "Training loss: 0.09484393894672394\n",
      "Training loss: 0.09271111339330673\n",
      "Training loss: 0.09994598478078842\n",
      "Training loss: 0.09062618762254715\n",
      "Training loss: 0.08611558377742767\n",
      "Training loss: 0.08742937445640564\n",
      "Training loss: 0.10078594833612442\n",
      "Training loss: 0.08899541199207306\n",
      "Training loss: 0.08739209175109863\n",
      "Training loss: 0.08654599636793137\n",
      "Training loss: 0.09191504865884781\n",
      "Training loss: 0.09648266434669495\n",
      "Training loss: 0.09672341495752335\n",
      "Training loss: 0.09796153008937836\n",
      "Training loss: 0.08693736791610718\n",
      "Training loss: 0.09596860408782959\n",
      "Training loss: 0.09569881111383438\n",
      "Training loss: 0.09828061610460281\n",
      "Training loss: 0.10006241500377655\n",
      "Training loss: 0.09061768651008606\n",
      "Training loss: 0.09650696814060211\n",
      "Training loss: 0.09249978512525558\n",
      "Training loss: 0.0904153361916542\n",
      "Training loss: 0.09081690013408661\n",
      "Training loss: 0.10100318491458893\n",
      "Training loss: 0.09212084859609604\n",
      "Training loss: 0.09928179532289505\n",
      "Training loss: 0.08988681435585022\n",
      "Training loss: 0.09113602340221405\n",
      "Training loss: 0.0910259559750557\n",
      "Training loss: 0.08733680844306946\n",
      "Training loss: 0.09006614238023758\n",
      "Training loss: 0.09056875854730606\n",
      "Training loss: 0.08983878791332245\n",
      "Training loss: 0.09378043562173843\n",
      "Training loss: 0.08845395594835281\n",
      "Training loss: 0.0865386351943016\n",
      "Training loss: 0.08881073445081711\n",
      "Training loss: 0.08901651948690414\n",
      "Training loss: 0.08559691905975342\n",
      "Training loss: 0.09281044453382492\n",
      "Training loss: 0.0865466445684433\n",
      "Training loss: 0.09016717970371246\n",
      "Training loss: 0.09114927798509598\n",
      "Training loss: 0.09246010333299637\n",
      "Training loss: 0.09498391300439835\n",
      "Training loss: 0.09389972686767578\n",
      "Training loss: 0.10009779781103134\n",
      "Training loss: 0.10009434074163437\n",
      "Training loss: 0.09058628976345062\n",
      "Training loss: 0.10091384500265121\n",
      "Training loss: 0.09611322730779648\n",
      "Training loss: 0.09272892028093338\n",
      "Training loss: 0.09335906058549881\n",
      "Training loss: 0.09047917276620865\n",
      "Training loss: 0.09072857350111008\n",
      "Training loss: 0.08982284367084503\n",
      "Training loss: 0.08773034811019897\n",
      "Training loss: 0.09548568725585938\n",
      "Training loss: 0.0935898870229721\n",
      "Training loss: 0.09103241562843323\n",
      "Training loss: 0.09152664989233017\n",
      "Training loss: 0.09247028082609177\n",
      "Training loss: 0.09111660718917847\n",
      "Training loss: 0.09397517889738083\n",
      "Training loss: 0.10036106407642365\n",
      "Training loss: 0.09171372652053833\n",
      "Training loss: 0.09665410220623016\n",
      "Training loss: 0.0983467623591423\n",
      "Training loss: 0.10006083548069\n",
      "Training loss: 0.09518995136022568\n",
      "Training loss: 0.09640161693096161\n",
      "Training loss: 0.0961848720908165\n",
      "Training loss: 0.10017533600330353\n",
      "Training loss: 0.10396768152713776\n",
      "Training loss: 0.09984079748392105\n",
      "Training loss: 0.09705173224210739\n",
      "Training loss: 0.09903732687234879\n",
      "Training loss: 0.09347334504127502\n",
      "Training loss: 0.09220512956380844\n",
      "Training loss: 0.09923206269741058\n",
      "Training loss: 0.09846016019582748\n",
      "Training loss: 0.09607482701539993\n",
      "Training loss: 0.1131971925497055\n",
      "Training loss: 0.09688019007444382\n",
      "Training loss: 0.10324782133102417\n",
      "Training loss: 0.09871404618024826\n",
      "Training loss: 0.09778536856174469\n",
      "Training loss: 0.09796666353940964\n",
      "Training loss: 0.1010439321398735\n",
      "Training loss: 0.09493383020162582\n",
      "Training loss: 0.09529058635234833\n",
      "Training loss: 0.09881503880023956\n",
      "Training loss: 0.09997856616973877\n",
      "Training loss: 0.09666053205728531\n",
      "Training loss: 0.09631317853927612\n",
      "Training loss: 0.09839467704296112\n",
      "Training loss: 0.10251523554325104\n",
      "Training loss: 0.10337810218334198\n",
      "Training loss: 0.09359912574291229\n",
      "Training loss: 0.09681754559278488\n",
      "Training loss: 0.10223361849784851\n",
      "Training loss: 0.0952458381652832\n",
      "Training loss: 0.09315125644207001\n",
      "Training loss: 0.09492027759552002\n",
      "Training loss: 0.09278634190559387\n",
      "Training loss: 0.09726881235837936\n",
      "Training loss: 0.09263407438993454\n",
      "Training loss: 0.09513092786073685\n",
      "Training loss: 0.1003076434135437\n",
      "Training loss: 0.0995253473520279\n",
      "Training loss: 0.09145069122314453\n",
      "Training loss: 0.08935536444187164\n",
      "Training loss: 0.10219675302505493\n",
      "Training loss: 0.09472339600324631\n",
      "Training loss: 0.09428702294826508\n",
      "Training loss: 0.10361120104789734\n",
      "Training loss: 0.08815332502126694\n",
      "Training loss: 0.09402487426996231\n",
      "Training loss: 0.09031816571950912\n",
      "Training loss: 0.08979137986898422\n",
      "Training loss: 0.08820217102766037\n",
      "Training loss: 0.0947786346077919\n",
      "Training loss: 0.08712036907672882\n",
      "Training loss: 0.08931221812963486\n",
      "Training loss: 0.08425237238407135\n",
      "Training loss: 0.09678791463375092\n",
      "Training loss: 0.09372887760400772\n",
      "Training loss: 0.08686289936304092\n",
      "Training loss: 0.09141845256090164\n",
      "Training loss: 0.09216497093439102\n",
      "Training loss: 0.09097538143396378\n",
      "Training loss: 0.08880677074193954\n",
      "Training loss: 0.10037228465080261\n",
      "Training loss: 0.0922383964061737\n",
      "Training loss: 0.08981636166572571\n",
      "Training loss: 0.0879407525062561\n",
      "Training loss: 0.10698582231998444\n",
      "Training loss: 0.09205135703086853\n",
      "Training loss: 0.088786281645298\n",
      "Training loss: 0.09648170322179794\n",
      "Training loss: 0.0876460000872612\n",
      "Training loss: 0.09822645038366318\n",
      "Training loss: 0.09193532168865204\n",
      "Training loss: 0.09123818576335907\n",
      "Training loss: 0.09659122675657272\n",
      "Training loss: 0.10451838374137878\n",
      "Training loss: 0.09330637753009796\n",
      "Training loss: 0.08810322731733322\n",
      "Training loss: 0.09262144565582275\n",
      "Training loss: 0.09677508473396301\n",
      "Training loss: 0.1002776101231575\n",
      "Training loss: 0.09543817490339279\n",
      "Training loss: 0.09695548564195633\n",
      "Training loss: 0.1052011102437973\n",
      "Training loss: 0.09026143699884415\n",
      "Training loss: 0.09300369769334793\n",
      "Training loss: 0.09182218462228775\n",
      "Training loss: 0.09301190078258514\n",
      "Training loss: 0.09065505117177963\n",
      "Training loss: 0.09096825122833252\n",
      "Training loss: 0.08863144367933273\n",
      "Training loss: 0.09526985883712769\n",
      "Training loss: 0.0861581563949585\n",
      "Training loss: 0.09150522202253342\n",
      "Training loss: 0.09211896359920502\n",
      "Training loss: 0.09756822139024734\n",
      "Training loss: 0.08888666331768036\n",
      "Training loss: 0.08428430557250977\n",
      "Training loss: 0.0883050188422203\n",
      "Training loss: 0.08768356591463089\n",
      "Training loss: 0.09334409236907959\n",
      "Training loss: 0.08793309330940247\n",
      "Training loss: 0.08660530298948288\n",
      "Training loss: 0.08287538588047028\n",
      "Training loss: 0.08883107453584671\n",
      "Training loss: 0.09103697538375854\n",
      "Training loss: 0.09454989433288574\n",
      "Training loss: 0.0870688185095787\n",
      "Training loss: 0.09160207957029343\n",
      "Training loss: 0.08890023827552795\n",
      "Training loss: 0.09395790845155716\n",
      "Training loss: 0.09468010067939758\n",
      "Training loss: 0.08954556286334991\n",
      "Training loss: 0.08608192950487137\n",
      "Training loss: 0.09059940278530121\n",
      "Training loss: 0.09314709901809692\n",
      "Training loss: 0.09650275111198425\n",
      "Training loss: 0.08596587926149368\n",
      "Training loss: 0.08748531341552734\n",
      "Training loss: 0.10267390310764313\n",
      "Training loss: 0.09183675050735474\n",
      "Training loss: 0.08939233422279358\n",
      "Training loss: 0.0897749736905098\n",
      "Training loss: 0.09303771704435349\n",
      "Training loss: 0.09438343346118927\n",
      "Training loss: 0.09261482208967209\n",
      "Training loss: 0.08797746151685715\n",
      "Training loss: 0.08735865354537964\n",
      "Training loss: 0.09465078264474869\n",
      "Training loss: 0.0938531905412674\n",
      "Training loss: 0.08537928760051727\n",
      "Training loss: 0.09018395841121674\n",
      "Training loss: 0.09136859327554703\n",
      "Training loss: 0.09262338280677795\n",
      "Training loss: 0.09352216124534607\n",
      "Training loss: 0.09118623286485672\n",
      "Training loss: 0.09548544883728027\n",
      "Training loss: 0.08749904483556747\n",
      "Training loss: 0.08593106269836426\n",
      "Training loss: 0.09329116344451904\n",
      "Training loss: 0.08958835154771805\n",
      "Training loss: 0.08851686865091324\n",
      "Training loss: 0.09689926356077194\n",
      "Training loss: 0.08496242761611938\n",
      "Training loss: 0.09448117762804031\n",
      "Training loss: 0.08401471376419067\n",
      "Training loss: 0.08260015398263931\n",
      "Training loss: 0.08432891964912415\n",
      "Training loss: 0.09091702103614807\n",
      "Training loss: 0.08337713032960892\n",
      "Training loss: 0.09387791901826859\n",
      "Training loss: 0.08991358429193497\n",
      "Training loss: 0.09127636253833771\n",
      "Training loss: 0.09532976895570755\n",
      "Training loss: 0.09109596163034439\n",
      "Training loss: 0.0900798812508583\n",
      "Training loss: 0.08783511817455292\n",
      "Training loss: 0.08157978951931\n",
      "Training loss: 0.0926596149802208\n",
      "Training loss: 0.09117932617664337\n",
      "Training loss: 0.0876481831073761\n",
      "Training loss: 0.09036418050527573\n",
      "Training loss: 0.09255154430866241\n",
      "Training loss: 0.09026286005973816\n",
      "Training loss: 0.0838349238038063\n",
      "Training loss: 0.08893612027168274\n",
      "Training loss: 0.09339366108179092\n",
      "Training loss: 0.09087620675563812\n",
      "Training loss: 0.08723355829715729\n",
      "Training loss: 0.08787093311548233\n",
      "Training loss: 0.08726495504379272\n",
      "Training loss: 0.08362657576799393\n",
      "Training loss: 0.0846763551235199\n",
      "Training loss: 0.09624134749174118\n",
      "Training loss: 0.08906158059835434\n",
      "Training loss: 0.08575423806905746\n",
      "Training loss: 0.08544790744781494\n",
      "Training loss: 0.09333169460296631\n",
      "Training loss: 0.08277799189090729\n",
      "Training loss: 0.09091641753911972\n",
      "Training loss: 0.08952856808900833\n",
      "Training loss: 0.08983279019594193\n",
      "Training loss: 0.09597130864858627\n",
      "Training loss: 0.08923928439617157\n",
      "Training loss: 0.08591321855783463\n",
      "Training loss: 0.08755036443471909\n",
      "Training loss: 0.097439244389534\n",
      "Training loss: 0.08981636166572571\n",
      "Training loss: 0.08917812258005142\n",
      "Training loss: 0.08943761140108109\n",
      "Training loss: 0.08861877024173737\n",
      "Training loss: 0.09016682952642441\n",
      "Training loss: 0.08734395354986191\n",
      "Training loss: 0.08714205771684647\n",
      "Training loss: 0.08818142861127853\n",
      "Training loss: 0.09135253727436066\n",
      "Training loss: 0.08915936946868896\n",
      "Training loss: 0.0923139750957489\n",
      "Training loss: 0.0892745852470398\n",
      "Training loss: 0.08713457733392715\n",
      "Training loss: 0.08177604526281357\n",
      "Training loss: 0.08736582845449448\n",
      "Training loss: 0.09276564419269562\n",
      "Training loss: 0.08559858798980713\n",
      "Training loss: 0.08415959030389786\n",
      "Training loss: 0.08154328167438507\n",
      "Training loss: 0.08912364393472672\n",
      "Training loss: 0.0859157145023346\n",
      "Training loss: 0.08616326004266739\n",
      "Training loss: 0.09432865679264069\n",
      "Training loss: 0.08778052777051926\n",
      "Training loss: 0.08940361440181732\n",
      "Training loss: 0.08943627774715424\n",
      "Training loss: 0.08592643588781357\n",
      "Training loss: 0.09170246869325638\n",
      "Training loss: 0.08509112149477005\n",
      "Training loss: 0.09543541073799133\n",
      "Training loss: 0.08494092524051666\n",
      "Training loss: 0.08719882369041443\n",
      "Training loss: 0.08324887603521347\n",
      "Training loss: 0.09107519686222076\n",
      "Training loss: 0.0979493111371994\n",
      "Training loss: 0.08782219141721725\n",
      "Training loss: 0.09411992132663727\n",
      "Training loss: 0.08755350857973099\n",
      "Training loss: 0.09179060906171799\n",
      "Training loss: 0.0831286609172821\n",
      "Training loss: 0.09091363102197647\n",
      "Training loss: 0.08825080841779709\n",
      "Training loss: 0.08928704261779785\n",
      "Training loss: 0.09075553715229034\n",
      "Training loss: 0.09333190321922302\n",
      "Training loss: 0.09959935396909714\n",
      "Training loss: 0.09677934646606445\n",
      "Training loss: 0.09222937375307083\n",
      "Training loss: 0.09047325700521469\n",
      "Training loss: 0.09159056097269058\n",
      "Training loss: 0.08613178879022598\n",
      "Training loss: 0.09108016639947891\n",
      "Training loss: 0.08864859491586685\n",
      "Training loss: 0.08955681324005127\n",
      "Training loss: 0.08315819501876831\n",
      "Training loss: 0.09945124387741089\n",
      "Training loss: 0.08776122331619263\n",
      "Training loss: 0.09032903611660004\n",
      "Training loss: 0.08264940977096558\n",
      "Training loss: 0.09268434345722198\n",
      "Training loss: 0.09308154135942459\n",
      "Training loss: 0.08358123898506165\n",
      "Training loss: 0.08887942880392075\n",
      "Training loss: 0.09480919688940048\n",
      "Training loss: 0.08664052933454514\n",
      "Training loss: 0.09261684864759445\n",
      "Training loss: 0.0849488377571106\n",
      "Training loss: 0.08349453657865524\n",
      "Training loss: 0.0843285545706749\n",
      "Training loss: 0.08510397374629974\n",
      "Training loss: 0.09653732180595398\n",
      "Training loss: 0.08399705588817596\n",
      "Training loss: 0.08974452316761017\n",
      "Training loss: 0.0904281809926033\n",
      "Training loss: 0.0902925580739975\n",
      "Training loss: 0.0898163765668869\n",
      "Training loss: 0.08455309271812439\n",
      "Training loss: 0.08201559633016586\n",
      "Training loss: 0.08872737735509872\n",
      "Training loss: 0.08267565816640854\n",
      "Training loss: 0.08740947395563126\n",
      "Training loss: 0.08569181710481644\n",
      "Training loss: 0.08521375805139542\n",
      "Training loss: 0.08555344492197037\n",
      "Training loss: 0.08524667471647263\n",
      "Training loss: 0.0908036082983017\n",
      "Training loss: 0.09301257133483887\n",
      "Training loss: 0.08987357467412949\n",
      "Training loss: 0.08313217014074326\n",
      "Training loss: 0.0908856987953186\n",
      "Training loss: 0.0890074148774147\n",
      "Training loss: 0.09126828610897064\n",
      "Training loss: 0.09031617641448975\n",
      "Training loss: 0.0942503958940506\n",
      "Training loss: 0.08965146541595459\n",
      "Training loss: 0.09139017015695572\n",
      "Training loss: 0.08949427306652069\n",
      "Training loss: 0.08716820180416107\n",
      "Training loss: 0.08812617510557175\n",
      "Training loss: 0.0949774757027626\n",
      "Training loss: 0.08657206594944\n",
      "Training loss: 0.09220881760120392\n",
      "Training loss: 0.08783572912216187\n",
      "Training loss: 0.09122820943593979\n",
      "Training loss: 0.0850079134106636\n",
      "Training loss: 0.08556774258613586\n",
      "Training loss: 0.09965834766626358\n",
      "Training loss: 0.0905870795249939\n",
      "Training loss: 0.0916062667965889\n",
      "Training loss: 0.09684910625219345\n",
      "Training loss: 0.10027558356523514\n",
      "Training loss: 0.09085822105407715\n",
      "Training loss: 0.09335796535015106\n",
      "Training loss: 0.09878814220428467\n",
      "Training loss: 0.09429875016212463\n",
      "Training loss: 0.09333447366952896\n",
      "Training loss: 0.08703901618719101\n",
      "Training loss: 0.09547098726034164\n",
      "Training loss: 0.09777296334505081\n",
      "Training loss: 0.09101929515600204\n",
      "Training loss: 0.08705101907253265\n",
      "Training loss: 0.09312719106674194\n",
      "Training loss: 0.09474501758813858\n",
      "Training loss: 0.08995071798563004\n",
      "Training loss: 0.08954060077667236\n",
      "Training loss: 0.08856328576803207\n",
      "Training loss: 0.0919128805398941\n",
      "Training loss: 0.08560923486948013\n",
      "Training loss: 0.09390914440155029\n",
      "Training loss: 0.09398338198661804\n",
      "Training loss: 0.09046610444784164\n",
      "Training loss: 0.0875648483633995\n",
      "Training loss: 0.09117096662521362\n",
      "Training loss: 0.09791257232427597\n",
      "Training loss: 0.09142062813043594\n",
      "Training loss: 0.0901179239153862\n",
      "Training loss: 0.08801090717315674\n",
      "Training loss: 0.09330615401268005\n",
      "Training loss: 0.09303267300128937\n",
      "Training loss: 0.0902228131890297\n",
      "Training loss: 0.08826147764921188\n",
      "Training loss: 0.08796527236700058\n",
      "Training loss: 0.09367407113313675\n",
      "Training loss: 0.09354563057422638\n",
      "Training loss: 0.0896850973367691\n",
      "Training loss: 0.09172789752483368\n",
      "Training loss: 0.09311649948358536\n",
      "Training loss: 0.09200633317232132\n",
      "Training loss: 0.09210021793842316\n",
      "Training loss: 0.08945509791374207\n",
      "Training loss: 0.09017682075500488\n",
      "Training loss: 0.09023795276880264\n",
      "Training loss: 0.09174071252346039\n",
      "Training loss: 0.09353332966566086\n",
      "Training loss: 0.0900408923625946\n",
      "Training loss: 0.09018650650978088\n",
      "Training loss: 0.0883990228176117\n",
      "Training loss: 0.09074273705482483\n",
      "Training loss: 0.08627728372812271\n",
      "Training loss: 0.0912708267569542\n",
      "Training loss: 0.08943431824445724\n",
      "Training loss: 0.08697463572025299\n",
      "Training loss: 0.09133221954107285\n",
      "Training loss: 0.09699814766645432\n",
      "Training loss: 0.09176792949438095\n",
      "Training loss: 0.09007975459098816\n",
      "Training loss: 0.08828762173652649\n",
      "Training loss: 0.09562378376722336\n",
      "Training loss: 0.09454074501991272\n",
      "Training loss: 0.08833342045545578\n",
      "Training loss: 0.09082545340061188\n",
      "Training loss: 0.09266968816518784\n",
      "Training loss: 0.08770133554935455\n",
      "Training loss: 0.08486904203891754\n",
      "Training loss: 0.09153230488300323\n",
      "Training loss: 0.09042564034461975\n",
      "Training loss: 0.08832532167434692\n",
      "Training loss: 0.08879964798688889\n",
      "Training loss: 0.08634580671787262\n",
      "Training loss: 0.09215766936540604\n",
      "Training loss: 0.08523212373256683\n",
      "Training loss: 0.08684343099594116\n",
      "Training loss: 0.08556322753429413\n",
      "Training loss: 0.091868095099926\n",
      "Training loss: 0.096678227186203\n",
      "Training loss: 0.08608213812112808\n",
      "Training loss: 0.09196183830499649\n",
      "Training loss: 0.08717266470193863\n",
      "Training loss: 0.09361125528812408\n",
      "Training loss: 0.0920107439160347\n",
      "Training loss: 0.08898850530385971\n",
      "Training loss: 0.09026307612657547\n",
      "Training loss: 0.09363363683223724\n",
      "Training loss: 0.08607004582881927\n",
      "Training loss: 0.09437241405248642\n",
      "Training loss: 0.09006038308143616\n",
      "Training loss: 0.09117113053798676\n",
      "Training loss: 0.09373952448368073\n",
      "Training loss: 0.08820781111717224\n",
      "Training loss: 0.09729126840829849\n",
      "Training loss: 0.09605848044157028\n",
      "Training loss: 0.09115389734506607\n",
      "Training loss: 0.08728531748056412\n",
      "Training loss: 0.0897081270813942\n",
      "Training loss: 0.08602484315633774\n",
      "Training loss: 0.08195792138576508\n",
      "Training loss: 0.09235062450170517\n",
      "Training loss: 0.09751913696527481\n",
      "Training loss: 0.09349165856838226\n",
      "Training loss: 0.09046564996242523\n",
      "Training loss: 0.08980882912874222\n",
      "Training loss: 0.08886001259088516\n",
      "Training loss: 0.09348563104867935\n",
      "Training loss: 0.09399192780256271\n",
      "Training loss: 0.10095380246639252\n",
      "Training loss: 0.08650463074445724\n",
      "Training loss: 0.09540640562772751\n",
      "Training loss: 0.10547056049108505\n",
      "Training loss: 0.09651952981948853\n",
      "Training loss: 0.09295430034399033\n",
      "Training loss: 0.10021545737981796\n",
      "Training loss: 0.09795628488063812\n",
      "Training loss: 0.09701602905988693\n",
      "Training loss: 0.08970535546541214\n",
      "Training loss: 0.0896439254283905\n",
      "Training loss: 0.0994616150856018\n",
      "Training loss: 0.09592218697071075\n",
      "Training loss: 0.09149481356143951\n",
      "Training loss: 0.0869370549917221\n",
      "Training loss: 0.0931885689496994\n",
      "Training loss: 0.09981569647789001\n",
      "Training loss: 0.09895183145999908\n",
      "Training loss: 0.0942419171333313\n",
      "Training loss: 0.09495861083269119\n",
      "Training loss: 0.09296683967113495\n",
      "Training loss: 0.08919994533061981\n",
      "Training loss: 0.09214232861995697\n",
      "Training loss: 0.08972165733575821\n",
      "Training loss: 0.09435521066188812\n",
      "Training loss: 0.08895694464445114\n",
      "Training loss: 0.09259282052516937\n",
      "Training loss: 0.08917196840047836\n",
      "Training loss: 0.08878771215677261\n",
      "Training loss: 0.10135301202535629\n",
      "Training loss: 0.09546440094709396\n",
      "Training loss: 0.09444225579500198\n",
      "Training loss: 0.08661721646785736\n",
      "Training loss: 0.08990664780139923\n",
      "Training loss: 0.08809439837932587\n",
      "Training loss: 0.08873773366212845\n",
      "Training loss: 0.09157266467809677\n",
      "Training loss: 0.09205776453018188\n",
      "Training loss: 0.08961665630340576\n",
      "Training loss: 0.08999411761760712\n",
      "Training loss: 0.09042293578386307\n",
      "Training loss: 0.08810319006443024\n",
      "Training loss: 0.09223300218582153\n",
      "Training loss: 0.093128502368927\n",
      "Training loss: 0.0942639708518982\n",
      "Training loss: 0.0875873789191246\n",
      "Training loss: 0.09444574266672134\n",
      "Training loss: 0.08863259106874466\n",
      "Training loss: 0.08977364748716354\n",
      "Training loss: 0.09044717252254486\n",
      "Training loss: 0.09723199903964996\n",
      "Training loss: 0.08421671390533447\n",
      "Training loss: 0.09195562452077866\n",
      "Training loss: 0.09043289721012115\n",
      "Training loss: 0.0837254747748375\n",
      "Training loss: 0.09285105764865875\n",
      "Training loss: 0.08158595114946365\n",
      "Training loss: 0.08981459587812424\n",
      "Training loss: 0.09113455563783646\n",
      "Training loss: 0.09603945165872574\n",
      "Training loss: 0.0934114009141922\n",
      "Training loss: 0.08959955722093582\n",
      "Training loss: 0.09059938788414001\n",
      "Training loss: 0.09124422073364258\n",
      "Training loss: 0.09642962366342545\n",
      "Training loss: 0.0893435925245285\n",
      "Training loss: 0.08726773411035538\n",
      "Training loss: 0.09159897267818451\n",
      "Training loss: 0.08893364667892456\n",
      "Training loss: 0.08680889755487442\n",
      "Training loss: 0.09508487582206726\n",
      "Training loss: 0.09746130555868149\n",
      "Training loss: 0.08793654292821884\n",
      "Training loss: 0.09297465533018112\n",
      "Training loss: 0.08575121313333511\n",
      "Training loss: 0.08993270993232727\n",
      "Training loss: 0.0925513505935669\n",
      "Training loss: 0.09563765674829483\n",
      "Training loss: 0.09854821860790253\n",
      "Training loss: 0.0948360413312912\n",
      "Training loss: 0.09322895109653473\n",
      "Training loss: 0.09390251338481903\n",
      "Training loss: 0.08769580721855164\n",
      "Training loss: 0.09255985915660858\n",
      "Training loss: 0.0936901867389679\n",
      "Training loss: 0.09209690243005753\n",
      "Training loss: 0.09560036659240723\n",
      "Training loss: 0.09554184228181839\n",
      "Training loss: 0.09259791672229767\n",
      "Training loss: 0.09165282547473907\n",
      "Training loss: 0.09746447205543518\n",
      "Training loss: 0.09035694599151611\n",
      "Training loss: 0.0903659462928772\n",
      "Training loss: 0.08808358013629913\n",
      "Training loss: 0.08673017472028732\n",
      "Training loss: 0.08652252703905106\n",
      "Training loss: 0.08951821178197861\n",
      "Training loss: 0.09425567835569382\n",
      "Training loss: 0.09058117866516113\n",
      "Training loss: 0.09031162410974503\n",
      "Training loss: 0.08821631968021393\n",
      "Training loss: 0.08476182073354721\n",
      "Training loss: 0.08846104890108109\n",
      "Training loss: 0.09026754647493362\n",
      "Training loss: 0.08669622987508774\n",
      "Training loss: 0.08914963155984879\n",
      "Training loss: 0.092662513256073\n",
      "Training loss: 0.08858814835548401\n",
      "Training loss: 0.09393011778593063\n",
      "Training loss: 0.08356758207082748\n",
      "Training loss: 0.09417255222797394\n",
      "Training loss: 0.0926220491528511\n",
      "Training loss: 0.09283235669136047\n",
      "Training loss: 0.09649952501058578\n",
      "Training loss: 0.0955086350440979\n",
      "Training loss: 0.08957745879888535\n",
      "Training loss: 0.0926133245229721\n",
      "Training loss: 0.08978599309921265\n",
      "Training loss: 0.09045311063528061\n",
      "Training loss: 0.08724180608987808\n",
      "Training loss: 0.09047383815050125\n",
      "Training loss: 0.09383069723844528\n",
      "Training loss: 0.08837494999170303\n",
      "Training loss: 0.0948210135102272\n",
      "Training loss: 0.08417429029941559\n",
      "Training loss: 0.09053655713796616\n",
      "Training loss: 0.09233603626489639\n",
      "Training loss: 0.08958669006824493\n",
      "Training loss: 0.0868283286690712\n",
      "Training loss: 0.09055567532777786\n",
      "Training loss: 0.09043220430612564\n",
      "Training loss: 0.08834390342235565\n",
      "Training loss: 0.08875656127929688\n",
      "Training loss: 0.08562085032463074\n",
      "Training loss: 0.09073172509670258\n",
      "Training loss: 0.08903303742408752\n",
      "Training loss: 0.08583160489797592\n",
      "Training loss: 0.0832788273692131\n",
      "Training loss: 0.08217625319957733\n",
      "Training loss: 0.09064733982086182\n",
      "Training loss: 0.09067053347826004\n",
      "Training loss: 0.09180687367916107\n",
      "Training loss: 0.09404926002025604\n",
      "Training loss: 0.0844166949391365\n",
      "Training loss: 0.08787637203931808\n",
      "Training loss: 0.08693800866603851\n",
      "Training loss: 0.08322056382894516\n",
      "Training loss: 0.0869162455201149\n",
      "Training loss: 0.0868135541677475\n",
      "Training loss: 0.08591104298830032\n",
      "Training loss: 0.0852213203907013\n",
      "Training loss: 0.09021545201539993\n",
      "Training loss: 0.09016913920640945\n",
      "Training loss: 0.08491363376379013\n",
      "Training loss: 0.08782248944044113\n",
      "Training loss: 0.08433566242456436\n",
      "Training loss: 0.08188866078853607\n",
      "Training loss: 0.07890067994594574\n",
      "Training loss: 0.0839829295873642\n",
      "Training loss: 0.0868760272860527\n",
      "Training loss: 0.08216995745897293\n",
      "Training loss: 0.0873834490776062\n",
      "Training loss: 0.08147431910037994\n",
      "Training loss: 0.08690750598907471\n",
      "Training loss: 0.08430532366037369\n",
      "Training loss: 0.0818445160984993\n",
      "Training loss: 0.08486811071634293\n",
      "Training loss: 0.08581790328025818\n",
      "Training loss: 0.09168403595685959\n",
      "Training loss: 0.0875663012266159\n",
      "Training loss: 0.08724450320005417\n",
      "Training loss: 0.09920459240674973\n",
      "Training loss: 0.09188001602888107\n",
      "Training loss: 0.09040875732898712\n",
      "Training loss: 0.09652058780193329\n",
      "Training loss: 0.08663620054721832\n",
      "Training loss: 0.09349478781223297\n",
      "Training loss: 0.08255699276924133\n",
      "Training loss: 0.08624003082513809\n",
      "Training loss: 0.08606553822755814\n",
      "Training loss: 0.08473224937915802\n",
      "Training loss: 0.08416657149791718\n",
      "Training loss: 0.08422005921602249\n",
      "Training loss: 0.0856645256280899\n",
      "Training loss: 0.0869402140378952\n",
      "Training loss: 0.08320476859807968\n",
      "Training loss: 0.08650115132331848\n",
      "Training loss: 0.08673517405986786\n",
      "Training loss: 0.08861526101827621\n",
      "Training loss: 0.08625856041908264\n",
      "Training loss: 0.08922667801380157\n",
      "Training loss: 0.08571465313434601\n",
      "Training loss: 0.09101048856973648\n",
      "Training loss: 0.08391661196947098\n",
      "Training loss: 0.0858408510684967\n",
      "Training loss: 0.08922062814235687\n",
      "Training loss: 0.09146006405353546\n",
      "Training loss: 0.08740253001451492\n",
      "Training loss: 0.09321117401123047\n",
      "Training loss: 0.09165727347135544\n",
      "Training loss: 0.0924467071890831\n",
      "Training loss: 0.08510155975818634\n",
      "Training loss: 0.08936221897602081\n",
      "Training loss: 0.08862756192684174\n",
      "Training loss: 0.08384211361408234\n",
      "Training loss: 0.08769350498914719\n",
      "Training loss: 0.0882069319486618\n",
      "Training loss: 0.08932007849216461\n",
      "Training loss: 0.09364436566829681\n",
      "Training loss: 0.08868693560361862\n",
      "Training loss: 0.0860418751835823\n",
      "Training loss: 0.08516010642051697\n",
      "Training loss: 0.08539777994155884\n",
      "Training loss: 0.08589407801628113\n",
      "Training loss: 0.09333174675703049\n",
      "Training loss: 0.08230356127023697\n",
      "Training loss: 0.0926346480846405\n",
      "Training loss: 0.09068015217781067\n",
      "Training loss: 0.08580514043569565\n",
      "Training loss: 0.09079593420028687\n",
      "Training loss: 0.08872630447149277\n",
      "Training loss: 0.08078941702842712\n",
      "Training loss: 0.08801290392875671\n",
      "Training loss: 0.08190831542015076\n",
      "Training loss: 0.08276455849409103\n",
      "Training loss: 0.08160494267940521\n",
      "Training loss: 0.08541565388441086\n",
      "Training loss: 0.08257270604372025\n",
      "Training loss: 0.08598114550113678\n",
      "Training loss: 0.08512525260448456\n",
      "Training loss: 0.08937720954418182\n",
      "Training loss: 0.08180394768714905\n",
      "Training loss: 0.08810897916555405\n",
      "Training loss: 0.08813722431659698\n",
      "Training loss: 0.08416343480348587\n",
      "Training loss: 0.08373866975307465\n",
      "Training loss: 0.08292420953512192\n",
      "Training loss: 0.08303577452898026\n",
      "Training loss: 0.07739335298538208\n",
      "Training loss: 0.07797075808048248\n",
      "Training loss: 0.08557777851819992\n",
      "Training loss: 0.08529797196388245\n",
      "Training loss: 0.08568401634693146\n",
      "Training loss: 0.08414287120103836\n",
      "Training loss: 0.08625244349241257\n",
      "Training loss: 0.08801960945129395\n",
      "Training loss: 0.08783020824193954\n",
      "Training loss: 0.08235990256071091\n",
      "Training loss: 0.09007047116756439\n",
      "Training loss: 0.0849817544221878\n",
      "Training loss: 0.08022210001945496\n",
      "Training loss: 0.08493252843618393\n",
      "Training loss: 0.08784479647874832\n",
      "Training loss: 0.08428728580474854\n",
      "Training loss: 0.08898128569126129\n",
      "Training loss: 0.07959295809268951\n",
      "Training loss: 0.08528430759906769\n",
      "Training loss: 0.08210572600364685\n",
      "Training loss: 0.08248702436685562\n",
      "Training loss: 0.0821656882762909\n",
      "Training loss: 0.0891944020986557\n",
      "Training loss: 0.08474065363407135\n",
      "Training loss: 0.08926023542881012\n",
      "Training loss: 0.08358573913574219\n",
      "Training loss: 0.08356180787086487\n",
      "Training loss: 0.08280488103628159\n",
      "Training loss: 0.07978396862745285\n",
      "Training loss: 0.0932135134935379\n",
      "Training loss: 0.0908031091094017\n",
      "Training loss: 0.08275525271892548\n",
      "Training loss: 0.08767398446798325\n",
      "Training loss: 0.08225913345813751\n",
      "Training loss: 0.08132711797952652\n",
      "Training loss: 0.09618589282035828\n",
      "Training loss: 0.0847829058766365\n",
      "Training loss: 0.08803258836269379\n",
      "Training loss: 0.08422589302062988\n",
      "Training loss: 0.09221924096345901\n",
      "Training loss: 0.08760326355695724\n",
      "Training loss: 0.08504363894462585\n",
      "Training loss: 0.08764822781085968\n",
      "Training loss: 0.09218677878379822\n",
      "Training loss: 0.08900415152311325\n",
      "Training loss: 0.08413499593734741\n",
      "Training loss: 0.08422259986400604\n",
      "Training loss: 0.09174538403749466\n",
      "Training loss: 0.08773220330476761\n",
      "Training loss: 0.08359738439321518\n",
      "Training loss: 0.0988357737660408\n",
      "Training loss: 0.08854043483734131\n",
      "Training loss: 0.08498542010784149\n",
      "Training loss: 0.08763790130615234\n",
      "Training loss: 0.08147678524255753\n",
      "Training loss: 0.08768013119697571\n",
      "Training loss: 0.08845051378011703\n",
      "Training loss: 0.08298203349113464\n",
      "Training loss: 0.09010474383831024\n",
      "Training loss: 0.08753940463066101\n",
      "Training loss: 0.08353440463542938\n",
      "Training loss: 0.09226778894662857\n",
      "Training loss: 0.08078478276729584\n",
      "Training loss: 0.08465835452079773\n",
      "Training loss: 0.08582711219787598\n",
      "Training loss: 0.08391807228326797\n",
      "Training loss: 0.08968684822320938\n",
      "Training loss: 0.08778093010187149\n",
      "Training loss: 0.08598233759403229\n",
      "Training loss: 0.09126605093479156\n",
      "Training loss: 0.08777690678834915\n",
      "Training loss: 0.08644392341375351\n",
      "Training loss: 0.08861120045185089\n",
      "Training loss: 0.0831088125705719\n",
      "Training loss: 0.09450653940439224\n",
      "Training loss: 0.09884873032569885\n",
      "Training loss: 0.08549372851848602\n",
      "Training loss: 0.08553062379360199\n",
      "Training loss: 0.08310125768184662\n",
      "Training loss: 0.09301186352968216\n",
      "Training loss: 0.08592194318771362\n",
      "Training loss: 0.09427215903997421\n",
      "Training loss: 0.09150084853172302\n",
      "Training loss: 0.08551781624555588\n",
      "Training loss: 0.09062712639570236\n",
      "Training loss: 0.08781757950782776\n",
      "Training loss: 0.08593674749135971\n",
      "Training loss: 0.08867360651493073\n",
      "Training loss: 0.0843203067779541\n",
      "Training loss: 0.08811406046152115\n",
      "Training loss: 0.09187494963407516\n",
      "Training loss: 0.08846083283424377\n",
      "Training loss: 0.08692331612110138\n",
      "Training loss: 0.09212881326675415\n",
      "Training loss: 0.0861843153834343\n",
      "Training loss: 0.09263032674789429\n",
      "Training loss: 0.08025654405355453\n",
      "Training loss: 0.08370477706193924\n",
      "Training loss: 0.08194723725318909\n",
      "Training loss: 0.08930188417434692\n",
      "Training loss: 0.09506526589393616\n",
      "Training loss: 0.07982209324836731\n",
      "Training loss: 0.0848461464047432\n",
      "Training loss: 0.08893219381570816\n",
      "Training loss: 0.084676094353199\n",
      "Training loss: 0.08084780722856522\n",
      "Training loss: 0.09179192036390305\n",
      "Training loss: 0.08266118913888931\n",
      "Training loss: 0.09199827164411545\n",
      "Training loss: 0.08236952871084213\n",
      "Training loss: 0.08320344984531403\n",
      "Training loss: 0.08606602251529694\n",
      "Training loss: 0.07954316586256027\n",
      "Training loss: 0.08061473071575165\n",
      "Training loss: 0.07905761897563934\n",
      "Training loss: 0.08141397684812546\n",
      "Training loss: 0.08254644274711609\n",
      "Training loss: 0.08040351420640945\n",
      "Training loss: 0.08431712538003922\n",
      "Training loss: 0.08166297525167465\n",
      "Training loss: 0.08170618861913681\n",
      "Training loss: 0.08575210720300674\n",
      "Training loss: 0.0857386440038681\n",
      "Training loss: 0.08276065438985825\n",
      "Training loss: 0.08283930271863937\n",
      "Training loss: 0.09303595125675201\n",
      "Training loss: 0.08838997781276703\n",
      "Training loss: 0.08393247425556183\n",
      "Training loss: 0.09031617641448975\n",
      "Training loss: 0.09392662346363068\n",
      "Training loss: 0.08518430590629578\n",
      "Training loss: 0.08915700018405914\n",
      "Training loss: 0.0829121321439743\n",
      "Training loss: 0.08711444586515427\n",
      "Training loss: 0.08739657700061798\n",
      "Training loss: 0.09381097555160522\n",
      "Training loss: 0.08212894946336746\n",
      "Training loss: 0.0826912447810173\n",
      "Training loss: 0.08531079441308975\n",
      "Training loss: 0.08865836262702942\n",
      "Training loss: 0.08386117964982986\n",
      "Training loss: 0.0871787965297699\n",
      "Training loss: 0.08898352831602097\n",
      "Training loss: 0.10476768016815186\n",
      "Training loss: 0.08782706409692764\n",
      "Training loss: 0.07914610952138901\n",
      "Training loss: 0.08450287580490112\n",
      "Training loss: 0.0894683450460434\n",
      "Training loss: 0.08732499182224274\n",
      "Training loss: 0.09023210406303406\n",
      "Training loss: 0.096213199198246\n",
      "Training loss: 0.09173127263784409\n",
      "Training loss: 0.11164814233779907\n",
      "Training loss: 0.08334004133939743\n",
      "Training loss: 0.09424792975187302\n",
      "Training loss: 0.08729734271764755\n",
      "Training loss: 0.08827801793813705\n",
      "Training loss: 0.0936981663107872\n",
      "Training loss: 0.09267204254865646\n",
      "Training loss: 0.0886940136551857\n",
      "Training loss: 0.08595871180295944\n",
      "Training loss: 0.08630052208900452\n",
      "Training loss: 0.09479010850191116\n",
      "Training loss: 0.08440960943698883\n",
      "Training loss: 0.08291251957416534\n",
      "Training loss: 0.08940084278583527\n",
      "Training loss: 0.08872035890817642\n",
      "Training loss: 0.09770242869853973\n",
      "Training loss: 0.089936763048172\n",
      "Training loss: 0.08509156852960587\n",
      "Training loss: 0.08553877472877502\n",
      "Training loss: 0.08939575403928757\n",
      "Training loss: 0.09130965173244476\n",
      "Training loss: 0.0891006588935852\n",
      "Training loss: 0.08622798323631287\n",
      "Training loss: 0.0856931284070015\n",
      "Training loss: 0.08761471509933472\n",
      "Training loss: 0.0868799090385437\n",
      "Training loss: 0.08914806693792343\n",
      "Training loss: 0.08113818615674973\n",
      "Training loss: 0.08222232758998871\n",
      "Training loss: 0.08984246104955673\n",
      "Training loss: 0.08950556814670563\n",
      "Training loss: 0.08465918153524399\n",
      "Training loss: 0.09192579984664917\n",
      "Training loss: 0.08785922080278397\n",
      "Training loss: 0.08328749239444733\n",
      "Training loss: 0.08292977511882782\n",
      "Training loss: 0.08170723915100098\n",
      "Training loss: 0.08567126095294952\n",
      "Training loss: 0.07728627324104309\n",
      "Training loss: 0.08503016084432602\n",
      "Training loss: 0.08968087285757065\n",
      "Training loss: 0.08177658170461655\n",
      "Training loss: 0.08567149192094803\n",
      "Training loss: 0.0816570445895195\n",
      "Training loss: 0.08331172913312912\n",
      "Training loss: 0.09663075953722\n",
      "Training loss: 0.08643462508916855\n",
      "Training loss: 0.08317674696445465\n",
      "Training loss: 0.09109372645616531\n",
      "Training loss: 0.09095843881368637\n",
      "Training loss: 0.08912474662065506\n",
      "Training loss: 0.08249972760677338\n",
      "Training loss: 0.09839824587106705\n",
      "Training loss: 0.08796540647745132\n",
      "Training loss: 0.08576170355081558\n",
      "Training loss: 0.0988512933254242\n",
      "Training loss: 0.0859135091304779\n",
      "Training loss: 0.08856377750635147\n",
      "Training loss: 0.08607230335474014\n",
      "Training loss: 0.09493832290172577\n",
      "Training loss: 0.08431942015886307\n",
      "Training loss: 0.088584303855896\n",
      "Training loss: 0.09141532331705093\n",
      "Training loss: 0.09259810298681259\n",
      "Training loss: 0.09216468781232834\n",
      "Training loss: 0.08864303678274155\n",
      "Training loss: 0.08372709155082703\n",
      "Training loss: 0.08336962759494781\n",
      "Training loss: 0.0908864438533783\n",
      "Training loss: 0.08712022006511688\n",
      "Training loss: 0.09339354932308197\n",
      "Training loss: 0.08538807183504105\n",
      "Training loss: 0.0872260108590126\n",
      "Training loss: 0.0815722644329071\n",
      "Training loss: 0.08859676867723465\n",
      "Training loss: 0.09044308960437775\n",
      "Training loss: 0.09054231643676758\n",
      "Training loss: 0.08529674261808395\n",
      "Training loss: 0.08261027932167053\n",
      "Training loss: 0.08934544026851654\n",
      "Training loss: 0.09448893368244171\n",
      "Training loss: 0.08014681190252304\n",
      "Training loss: 0.08772852271795273\n",
      "Training loss: 0.0906352773308754\n",
      "Training loss: 0.08380106091499329\n",
      "Training loss: 0.07993945479393005\n",
      "Training loss: 0.08963838964700699\n",
      "Training loss: 0.09158749878406525\n",
      "Training loss: 0.0882866159081459\n",
      "Training loss: 0.08783670514822006\n",
      "Training loss: 0.08538393676280975\n",
      "Training loss: 0.08561349660158157\n",
      "Training loss: 0.08214074373245239\n",
      "Training loss: 0.08838298916816711\n",
      "Training loss: 0.081537164747715\n",
      "Training loss: 0.08102501928806305\n",
      "Training loss: 0.08674423396587372\n",
      "Training loss: 0.08604755252599716\n",
      "Training loss: 0.08161451667547226\n",
      "Training loss: 0.08648338168859482\n",
      "Training loss: 0.08942201733589172\n",
      "Training loss: 0.08360029011964798\n",
      "Training loss: 0.08520682156085968\n",
      "Training loss: 0.08411671966314316\n",
      "Training loss: 0.08277557790279388\n",
      "Training loss: 0.0883759930729866\n",
      "Training loss: 0.08219090104103088\n",
      "Training loss: 0.09561733901500702\n",
      "Training loss: 0.08329673111438751\n",
      "Training loss: 0.08767272531986237\n",
      "Training loss: 0.08490006625652313\n",
      "Training loss: 0.08571934700012207\n",
      "Training loss: 0.08707625418901443\n",
      "Training loss: 0.09179266542196274\n",
      "Training loss: 0.09374246001243591\n",
      "Training loss: 0.09808257967233658\n",
      "Training loss: 0.08615931868553162\n",
      "Training loss: 0.09048941731452942\n",
      "Training loss: 0.08983545750379562\n",
      "Training loss: 0.08649363368749619\n",
      "Training loss: 0.08314190059900284\n",
      "Training loss: 0.08623563498258591\n",
      "Training loss: 0.08267271518707275\n",
      "Training loss: 0.08836173266172409\n",
      "Training loss: 0.0903066024184227\n",
      "Training loss: 0.08528713881969452\n",
      "Training loss: 0.08519227802753448\n",
      "Training loss: 0.0898364931344986\n",
      "Training loss: 0.08386064320802689\n",
      "Training loss: 0.08907942473888397\n",
      "Training loss: 0.08420158177614212\n",
      "Training loss: 0.08789148181676865\n",
      "Training loss: 0.08310804516077042\n",
      "Training loss: 0.08749986439943314\n",
      "Training loss: 0.08554086089134216\n",
      "Training loss: 0.08004993945360184\n",
      "Training loss: 0.07907646894454956\n",
      "Training loss: 0.07826706022024155\n",
      "Training loss: 0.07840126752853394\n",
      "Training loss: 0.09531909972429276\n",
      "Training loss: 0.07609201222658157\n",
      "Training loss: 0.0789269283413887\n",
      "Training loss: 0.0866876021027565\n",
      "Training loss: 0.07846426218748093\n",
      "Training loss: 0.07863465696573257\n",
      "Training loss: 0.0812983363866806\n",
      "Training loss: 0.08184521645307541\n",
      "Training loss: 0.08169849961996078\n",
      "Training loss: 0.07842923700809479\n",
      "Training loss: 0.07700143754482269\n",
      "Training loss: 0.07981278747320175\n",
      "Training loss: 0.08302830904722214\n",
      "Training loss: 0.08500882238149643\n",
      "Training loss: 0.08191882818937302\n",
      "Training loss: 0.08441749215126038\n",
      "Training loss: 0.08944133669137955\n",
      "Training loss: 0.08482106775045395\n",
      "Training loss: 0.0899212434887886\n",
      "Training loss: 0.08603876084089279\n",
      "Training loss: 0.07455337792634964\n",
      "Training loss: 0.07937465608119965\n",
      "Training loss: 0.08307430148124695\n",
      "Training loss: 0.08273432403802872\n",
      "Training loss: 0.08100682497024536\n",
      "Training loss: 0.08016893267631531\n",
      "Training loss: 0.08702096343040466\n",
      "Training loss: 0.08388051390647888\n",
      "Training loss: 0.08763612806797028\n",
      "Training loss: 0.0814887061715126\n",
      "Training loss: 0.08368782699108124\n",
      "Training loss: 0.0796264261007309\n",
      "Training loss: 0.08376338332891464\n",
      "Training loss: 0.08919407427310944\n",
      "Training loss: 0.07525550574064255\n",
      "Training loss: 0.08424873650074005\n",
      "Training loss: 0.08130770921707153\n",
      "Training loss: 0.08501136302947998\n",
      "Training loss: 0.07877743244171143\n",
      "Training loss: 0.07909909635782242\n",
      "Training loss: 0.0988447293639183\n",
      "Training loss: 0.08574004471302032\n",
      "Training loss: 0.08775223046541214\n",
      "Training loss: 0.08806496113538742\n",
      "Training loss: 0.08812787383794785\n",
      "Training loss: 0.0824219286441803\n",
      "Training loss: 0.08363820612430573\n",
      "Training loss: 0.08419882506132126\n",
      "Training loss: 0.08552029728889465\n",
      "Training loss: 0.08696386218070984\n",
      "Training loss: 0.08618944138288498\n",
      "Training loss: 0.07971242815256119\n",
      "Training loss: 0.09070349484682083\n",
      "Training loss: 0.08591867983341217\n",
      "Training loss: 0.09223836660385132\n",
      "Training loss: 0.08327838033437729\n",
      "Training loss: 0.08103899657726288\n",
      "Training loss: 0.08667010068893433\n",
      "Training loss: 0.08566676825284958\n",
      "Training loss: 0.0836678296327591\n",
      "Training loss: 0.08748544007539749\n",
      "Training loss: 0.08627288043498993\n",
      "Training loss: 0.08720241487026215\n",
      "Training loss: 0.08777892589569092\n",
      "Training loss: 0.07992063462734222\n",
      "Training loss: 0.07871361821889877\n",
      "Training loss: 0.08842471241950989\n",
      "Training loss: 0.08270616829395294\n",
      "Training loss: 0.08749445527791977\n",
      "Training loss: 0.07743986696004868\n",
      "Training loss: 0.08458297699689865\n",
      "Training loss: 0.08583467453718185\n",
      "Training loss: 0.08202408254146576\n",
      "Training loss: 0.08235461264848709\n",
      "Training loss: 0.08623730391263962\n",
      "Training loss: 0.08047729730606079\n",
      "Training loss: 0.0909743383526802\n",
      "Training loss: 0.0915297418832779\n",
      "Training loss: 0.08401579409837723\n",
      "Training loss: 0.08722422271966934\n",
      "Training loss: 0.0824655294418335\n",
      "Training loss: 0.0864177942276001\n",
      "Training loss: 0.0857040286064148\n",
      "Training loss: 0.08905892819166183\n",
      "Training loss: 0.07890079915523529\n",
      "Training loss: 0.0895053818821907\n",
      "Training loss: 0.077900730073452\n",
      "Training loss: 0.09082577377557755\n",
      "Training loss: 0.09074196219444275\n",
      "Training loss: 0.07884886115789413\n",
      "Training loss: 0.08194396644830704\n",
      "Training loss: 0.08587167412042618\n",
      "Training loss: 0.07710740715265274\n",
      "Training loss: 0.07915642857551575\n",
      "Training loss: 0.09017346054315567\n",
      "Training loss: 0.0834723711013794\n",
      "Training loss: 0.08196165412664413\n",
      "Training loss: 0.08679605275392532\n",
      "Training loss: 0.08357710391283035\n",
      "Training loss: 0.08265019208192825\n",
      "Training loss: 0.08002405613660812\n",
      "Training loss: 0.08526686578989029\n",
      "Training loss: 0.08396125584840775\n",
      "Training loss: 0.07807821780443192\n",
      "Training loss: 0.07970065623521805\n",
      "Training loss: 0.0815628245472908\n",
      "Training loss: 0.08598975837230682\n",
      "Training loss: 0.07901694625616074\n",
      "Training loss: 0.08388972282409668\n",
      "Training loss: 0.08420465886592865\n",
      "Training loss: 0.0862267017364502\n",
      "Training loss: 0.08129452913999557\n",
      "Training loss: 0.0832749456167221\n",
      "Training loss: 0.0834917277097702\n",
      "Training loss: 0.08384454250335693\n",
      "Training loss: 0.09003177285194397\n",
      "Training loss: 0.0922975167632103\n",
      "Training loss: 0.08410673588514328\n",
      "Training loss: 0.08768467605113983\n",
      "Training loss: 0.08714579790830612\n",
      "Training loss: 0.0818692296743393\n",
      "Training loss: 0.08548254519701004\n",
      "Training loss: 0.08316962420940399\n",
      "Training loss: 0.07998862117528915\n",
      "Training loss: 0.0790211632847786\n",
      "Training loss: 0.08105544000864029\n",
      "Training loss: 0.0864369124174118\n",
      "Training loss: 0.07843469083309174\n",
      "Training loss: 0.08273787051439285\n",
      "Training loss: 0.08751051872968674\n",
      "Training loss: 0.08710186183452606\n",
      "Training loss: 0.08773855119943619\n",
      "Training loss: 0.07824591547250748\n",
      "Training loss: 0.07693523168563843\n",
      "Training loss: 0.09293224662542343\n",
      "Training loss: 0.0868697464466095\n",
      "Training loss: 0.07853703200817108\n",
      "Training loss: 0.0872553288936615\n",
      "Training loss: 0.0842997208237648\n",
      "Training loss: 0.0849468857049942\n",
      "Training loss: 0.09191038459539413\n",
      "Training loss: 0.0865560993552208\n",
      "Training loss: 0.08624874800443649\n",
      "Training loss: 0.08584524691104889\n",
      "Training loss: 0.08049456775188446\n",
      "Training loss: 0.08293859660625458\n",
      "Training loss: 0.07715176790952682\n",
      "Training loss: 0.08115039765834808\n",
      "Training loss: 0.0785938948392868\n",
      "Training loss: 0.08789034932851791\n",
      "Training loss: 0.0837395116686821\n",
      "Training loss: 0.0816587284207344\n",
      "Training loss: 0.07885785400867462\n",
      "Training loss: 0.08242085576057434\n",
      "Training loss: 0.08205686509609222\n",
      "Training loss: 0.0788639485836029\n",
      "Training loss: 0.08068645745515823\n",
      "Training loss: 0.08483341336250305\n",
      "Training loss: 0.08132196962833405\n",
      "Training loss: 0.08050910383462906\n",
      "Training loss: 0.0816577598452568\n",
      "Training loss: 0.08232708275318146\n",
      "Training loss: 0.07939688861370087\n",
      "Training loss: 0.07839478552341461\n",
      "Training loss: 0.08078116178512573\n",
      "Training loss: 0.09427643567323685\n",
      "Training loss: 0.07590150833129883\n",
      "Training loss: 0.0815972313284874\n",
      "Training loss: 0.09063665568828583\n",
      "Training loss: 0.0832226425409317\n",
      "Training loss: 0.07905266433954239\n",
      "Training loss: 0.09938943386077881\n",
      "Training loss: 0.09223315864801407\n",
      "Training loss: 0.08768884092569351\n",
      "Training loss: 0.07680061459541321\n",
      "Training loss: 0.0822412297129631\n",
      "Training loss: 0.09009493142366409\n",
      "Training loss: 0.08400632441043854\n",
      "Training loss: 0.08230210840702057\n",
      "Training loss: 0.07826797664165497\n",
      "Training loss: 0.08536094427108765\n",
      "Training loss: 0.08037134259939194\n",
      "Training loss: 0.08388546854257584\n",
      "Training loss: 0.08150327950716019\n",
      "Training loss: 0.08558164536952972\n",
      "Training loss: 0.08539650589227676\n",
      "Training loss: 0.08987338840961456\n",
      "Training loss: 0.08359035849571228\n",
      "Training loss: 0.08075672388076782\n",
      "Training loss: 0.08658745884895325\n",
      "Training loss: 0.0871562585234642\n",
      "Training loss: 0.08152464777231216\n",
      "Training loss: 0.07970388978719711\n",
      "Training loss: 0.07651782035827637\n",
      "Training loss: 0.08532685041427612\n",
      "Training loss: 0.07519416511058807\n",
      "Training loss: 0.07928724586963654\n",
      "Training loss: 0.07586993277072906\n",
      "Training loss: 0.2695331871509552\n",
      "Training loss: 0.08408831059932709\n",
      "Training loss: 0.080755315721035\n",
      "Training loss: 0.08267852663993835\n",
      "Training loss: 0.08357477933168411\n",
      "Training loss: 0.08364398032426834\n",
      "Training loss: 0.08843999356031418\n",
      "Training loss: 0.0816909670829773\n",
      "Training loss: 0.08339311182498932\n",
      "Training loss: 0.08267903327941895\n",
      "Training loss: 0.08548372983932495\n",
      "Training loss: 0.08501407504081726\n",
      "Training loss: 0.0891035944223404\n",
      "Training loss: 0.09202056378126144\n",
      "Training loss: 0.08956482261419296\n",
      "Training loss: 0.08107687532901764\n",
      "Training loss: 0.08926708996295929\n",
      "Training loss: 0.0924391895532608\n",
      "Training loss: 0.08994488418102264\n",
      "Training loss: 0.081688292324543\n",
      "Training loss: 0.08244556188583374\n",
      "Training loss: 0.08308666199445724\n",
      "Training loss: 0.08788923174142838\n",
      "Training loss: 0.09135161340236664\n",
      "Training loss: 0.0847667008638382\n",
      "Training loss: 0.08541662245988846\n",
      "Training loss: 0.08607422560453415\n",
      "Training loss: 0.08177240192890167\n",
      "Training loss: 0.08332107961177826\n",
      "Training loss: 0.08964875340461731\n",
      "Training loss: 0.08558253943920135\n",
      "Training loss: 0.08751652389764786\n",
      "Training loss: 0.08277744799852371\n",
      "Training loss: 0.08965276926755905\n",
      "Training loss: 0.0864131823182106\n",
      "Training loss: 0.08381542563438416\n",
      "Training loss: 0.08260396867990494\n",
      "Training loss: 0.08244027197360992\n",
      "Training loss: 0.0806422159075737\n",
      "Training loss: 0.08458027243614197\n",
      "Training loss: 0.08971468359231949\n",
      "Training loss: 0.0834851786494255\n",
      "Training loss: 0.07766947150230408\n",
      "Training loss: 0.08887382596731186\n",
      "Training loss: 0.08730804920196533\n",
      "Training loss: 0.08535309880971909\n",
      "Training loss: 0.08289525657892227\n",
      "Training loss: 0.08428585529327393\n",
      "Training loss: 0.08251185715198517\n",
      "Training loss: 0.08805734664201736\n",
      "Training loss: 0.08120672404766083\n",
      "Training loss: 0.08459403365850449\n",
      "Training loss: 0.08509572595357895\n",
      "Training loss: 0.08371495455503464\n",
      "Training loss: 0.08787950128316879\n",
      "Training loss: 0.08514337986707687\n",
      "Training loss: 0.08706998080015182\n",
      "Training loss: 0.08734850585460663\n",
      "Training loss: 0.08846971392631531\n",
      "Training loss: 0.08036002516746521\n",
      "Training loss: 0.08988258242607117\n",
      "Training loss: 0.08806919306516647\n",
      "Training loss: 0.08933202177286148\n",
      "Training loss: 0.09056635946035385\n",
      "Training loss: 0.08272945880889893\n",
      "Training loss: 0.08731444925069809\n",
      "Training loss: 0.08436790108680725\n",
      "Training loss: 0.08330037444829941\n",
      "Training loss: 0.08120526373386383\n",
      "Training loss: 0.08798453211784363\n",
      "Training loss: 0.08467700332403183\n",
      "Training loss: 0.0854618176817894\n",
      "Training loss: 0.08419864624738693\n",
      "Training loss: 0.08668459206819534\n",
      "Training loss: 0.08139616250991821\n",
      "Training loss: 0.08803538978099823\n",
      "Training loss: 0.08997975289821625\n",
      "Training loss: 0.08725550025701523\n",
      "Training loss: 0.08417369425296783\n",
      "Training loss: 0.0833020880818367\n",
      "Training loss: 0.097064308822155\n",
      "Training loss: 0.08311118930578232\n",
      "Training loss: 0.07754179835319519\n",
      "Training loss: 0.09912662208080292\n",
      "Training loss: 0.0955030545592308\n",
      "Training loss: 0.08813348412513733\n",
      "Training loss: 0.08431451767683029\n",
      "Training loss: 0.08422629535198212\n",
      "Training loss: 0.0867181271314621\n",
      "Training loss: 0.078243188560009\n",
      "Training loss: 0.08328420668840408\n",
      "Training loss: 0.0878288671374321\n",
      "Training loss: 0.085403673350811\n",
      "Training loss: 0.08162834495306015\n",
      "Training loss: 0.09044791758060455\n",
      "Training loss: 0.0901007279753685\n",
      "Training loss: 0.09346547722816467\n",
      "Training loss: 0.08608082681894302\n",
      "Training loss: 0.08455199748277664\n",
      "Training loss: 0.09011072665452957\n",
      "Training loss: 0.08994196355342865\n",
      "Training loss: 0.09825354814529419\n",
      "Training loss: 0.08303996920585632\n",
      "Training loss: 0.08847183734178543\n",
      "Training loss: 0.08509368449449539\n",
      "Training loss: 0.08123329281806946\n",
      "Training loss: 0.08444622904062271\n",
      "Training loss: 0.09040381014347076\n",
      "Training loss: 0.08721345663070679\n",
      "Training loss: 0.084398552775383\n",
      "Training loss: 0.07993580400943756\n",
      "Training loss: 0.09049005806446075\n",
      "Training loss: 0.0836465060710907\n",
      "Training loss: 0.08673480153083801\n",
      "Training loss: 0.09002576023340225\n",
      "Training loss: 0.08929971605539322\n",
      "Training loss: 0.08535084873437881\n",
      "Training loss: 0.07946359366178513\n",
      "Training loss: 0.08558586239814758\n",
      "Training loss: 0.08284188061952591\n",
      "Training loss: 0.0802600309252739\n",
      "Training loss: 0.08443821966648102\n",
      "Training loss: 0.08443519473075867\n",
      "Training loss: 0.08349408209323883\n",
      "Training loss: 0.08462993055582047\n",
      "Training loss: 0.08101251721382141\n",
      "Training loss: 0.0834130197763443\n",
      "Training loss: 0.07863857597112656\n",
      "Training loss: 0.08566094934940338\n",
      "Training loss: 0.0771787092089653\n",
      "Training loss: 0.08403056114912033\n",
      "Training loss: 0.08909884840250015\n",
      "Training loss: 0.0893961489200592\n",
      "Training loss: 0.07970590144395828\n",
      "Training loss: 0.08150926977396011\n",
      "Training loss: 0.0792599692940712\n",
      "Training loss: 0.08660757541656494\n",
      "Training loss: 0.08266034722328186\n",
      "Training loss: 0.08161544054746628\n",
      "Training loss: 0.08181802183389664\n",
      "Training loss: 0.07902782410383224\n",
      "Training loss: 0.08584026992321014\n",
      "Training loss: 0.0933147743344307\n",
      "Training loss: 0.08880680054426193\n",
      "Training loss: 0.08585833013057709\n",
      "Training loss: 0.08721110969781876\n",
      "Training loss: 0.08631958812475204\n",
      "Training loss: 0.0860041156411171\n",
      "Training loss: 0.07991498708724976\n",
      "Training loss: 0.08543629944324493\n",
      "Training loss: 0.07824637740850449\n",
      "Training loss: 0.08288007974624634\n",
      "Training loss: 0.08918255567550659\n",
      "Training loss: 0.08531608432531357\n",
      "Training loss: 0.08240391314029694\n",
      "Training loss: 0.08324188739061356\n",
      "Training loss: 0.0855034738779068\n",
      "Training loss: 0.08388359844684601\n",
      "Training loss: 0.0817359909415245\n",
      "Training loss: 0.08390021324157715\n",
      "Training loss: 0.08472014963626862\n",
      "Training loss: 0.08426575362682343\n",
      "Training loss: 0.07855860143899918\n",
      "Training loss: 0.08809658885002136\n",
      "Training loss: 0.0825880914926529\n",
      "Training loss: 0.07573632895946503\n",
      "Training loss: 0.08229681104421616\n",
      "Training loss: 0.08667590469121933\n",
      "Training loss: 0.0846913605928421\n",
      "Training loss: 0.08135561645030975\n",
      "Training loss: 0.0784066691994667\n",
      "Training loss: 0.07731302827596664\n",
      "Training loss: 0.08798881620168686\n",
      "Training loss: 0.08953636139631271\n",
      "Training loss: 0.07762739807367325\n",
      "Training loss: 0.08224530518054962\n",
      "Training loss: 0.07867453992366791\n",
      "Training loss: 0.08277270942926407\n",
      "Training loss: 0.0787799209356308\n",
      "Training loss: 0.08562830090522766\n",
      "Training loss: 0.0833163931965828\n",
      "Training loss: 0.07714438438415527\n",
      "Training loss: 0.08638479560613632\n",
      "Training loss: 0.08131780475378036\n",
      "Training loss: 0.0807342380285263\n",
      "Training loss: 0.08202821761369705\n",
      "Training loss: 0.07481829077005386\n",
      "Training loss: 0.08092077076435089\n",
      "Training loss: 0.08048088848590851\n",
      "Training loss: 0.08555936068296432\n",
      "Training loss: 0.08255360275506973\n",
      "Training loss: 0.08283617347478867\n",
      "Training loss: 0.08108150213956833\n",
      "Training loss: 0.0833808034658432\n",
      "Training loss: 0.08221466839313507\n",
      "Training loss: 0.07769086956977844\n",
      "Training loss: 0.08159694075584412\n",
      "Training loss: 0.0815991461277008\n",
      "Training loss: 0.08304587006568909\n",
      "Training loss: 0.08512484282255173\n",
      "Training loss: 0.07860754430294037\n",
      "Training loss: 0.08075622469186783\n",
      "Training loss: 0.08391804248094559\n",
      "Training loss: 0.07988519966602325\n",
      "Training loss: 0.08080887049436569\n",
      "Training loss: 0.07974845170974731\n",
      "Training loss: 0.09183133393526077\n",
      "Training loss: 0.08108209073543549\n",
      "Training loss: 0.08865353465080261\n",
      "Training loss: 0.07673416286706924\n",
      "Training loss: 0.07383162528276443\n",
      "Training loss: 0.07829132676124573\n",
      "Training loss: 0.08437374979257584\n",
      "Training loss: 0.07620230317115784\n",
      "Training loss: 0.08402025699615479\n",
      "Training loss: 0.08306407928466797\n",
      "Training loss: 0.09178189188241959\n",
      "Training loss: 0.0767100378870964\n",
      "Training loss: 0.07851047068834305\n",
      "Training loss: 0.08672391623258591\n",
      "Training loss: 0.08153088390827179\n",
      "Training loss: 0.08448125422000885\n",
      "Training loss: 0.08532796800136566\n",
      "Training loss: 0.08251932263374329\n",
      "Training loss: 0.08066723495721817\n",
      "Training loss: 0.0777926966547966\n",
      "Training loss: 0.08760884404182434\n",
      "Training loss: 0.0853780210018158\n",
      "Training loss: 0.09034654498100281\n",
      "Training loss: 0.0793120339512825\n",
      "Training loss: 0.0841488242149353\n",
      "Training loss: 0.10082048177719116\n",
      "Training loss: 0.0774713084101677\n",
      "Training loss: 0.07892916351556778\n",
      "Training loss: 0.08155502378940582\n",
      "Training loss: 0.08536674082279205\n",
      "Training loss: 0.09176005423069\n",
      "Training loss: 0.08484798669815063\n",
      "Training loss: 0.09180839359760284\n",
      "Training loss: 0.09342214465141296\n",
      "Training loss: 0.07922811061143875\n",
      "Training loss: 0.08164314180612564\n",
      "Training loss: 0.09062396734952927\n",
      "Training loss: 0.08701252192258835\n",
      "Training loss: 0.0864996388554573\n",
      "Training loss: 0.08411774784326553\n",
      "Training loss: 0.08811566233634949\n",
      "Training loss: 0.09019935131072998\n",
      "Training loss: 0.08956050127744675\n",
      "Training loss: 0.08487061411142349\n",
      "Training loss: 0.08751590549945831\n",
      "Training loss: 0.08863720297813416\n",
      "Training loss: 0.08239235728979111\n",
      "Training loss: 0.08872080594301224\n",
      "Training loss: 0.08942520618438721\n",
      "Training loss: 0.08522140979766846\n",
      "Training loss: 0.08395155519247055\n",
      "Training loss: 0.09283957630395889\n",
      "Training loss: 0.08289717137813568\n",
      "Training loss: 0.08887777477502823\n",
      "Training loss: 0.09206389635801315\n",
      "Training loss: 0.08636970818042755\n",
      "Training loss: 0.08310127258300781\n",
      "Training loss: 0.08221845328807831\n",
      "Training loss: 0.08303114771842957\n",
      "Training loss: 0.08405320346355438\n",
      "Training loss: 0.07665881514549255\n",
      "Training loss: 0.0808078944683075\n",
      "Training loss: 0.0816602036356926\n",
      "Training loss: 0.08414726704359055\n",
      "Training loss: 0.08233683556318283\n",
      "Training loss: 0.08649390935897827\n",
      "Training loss: 0.08183998614549637\n",
      "Training loss: 0.07869867980480194\n",
      "Training loss: 0.08352003991603851\n",
      "Training loss: 0.09319328516721725\n",
      "Training loss: 0.08197051286697388\n",
      "Training loss: 0.08634605258703232\n",
      "Training loss: 0.0801459401845932\n",
      "Training loss: 0.07982850074768066\n",
      "Training loss: 0.07731226831674576\n",
      "Training loss: 0.08682264387607574\n",
      "Training loss: 0.08615516871213913\n",
      "Training loss: 0.07896307855844498\n",
      "Training loss: 0.08115527778863907\n",
      "Training loss: 0.09466323256492615\n",
      "Training loss: 0.08716047555208206\n",
      "Training loss: 0.09253290295600891\n",
      "Training loss: 0.08140043914318085\n",
      "Training loss: 0.08524084091186523\n",
      "Training loss: 0.08523990958929062\n",
      "Training loss: 0.08220724016427994\n",
      "Training loss: 0.08031333982944489\n",
      "Training loss: 0.09601372480392456\n",
      "Training loss: 0.08135165274143219\n",
      "Training loss: 0.07902048528194427\n",
      "Training loss: 0.08219394832849503\n",
      "Training loss: 0.08670399338006973\n",
      "Training loss: 0.08260969072580338\n",
      "Training loss: 0.08123175799846649\n",
      "Training loss: 0.08257914334535599\n",
      "Training loss: 0.08514047414064407\n",
      "Training loss: 0.08872227370738983\n",
      "Training loss: 0.07859352976083755\n",
      "Training loss: 0.07933393120765686\n",
      "Training loss: 0.08109413832426071\n",
      "Training loss: 0.08066525310277939\n",
      "Training loss: 0.07980848848819733\n",
      "Training loss: 0.08593539893627167\n",
      "Training loss: 0.07997399568557739\n",
      "Training loss: 0.07842066884040833\n",
      "Training loss: 0.07833094149827957\n",
      "Training loss: 0.08754511922597885\n",
      "Training loss: 0.08848892152309418\n",
      "Training loss: 0.08966439962387085\n",
      "Training loss: 0.0856899693608284\n",
      "Training loss: 0.0784720629453659\n",
      "Training loss: 0.0796012431383133\n",
      "Training loss: 0.0794081762433052\n",
      "Training loss: 0.08347321301698685\n",
      "Training loss: 0.08489944040775299\n",
      "Training loss: 0.08633065223693848\n",
      "Training loss: 0.08139478415250778\n",
      "Training loss: 0.0795992836356163\n",
      "Training loss: 0.09385506808757782\n",
      "Training loss: 0.08270728588104248\n",
      "Training loss: 0.08688129484653473\n",
      "Training loss: 0.08774807304143906\n",
      "Training loss: 0.08388486504554749\n",
      "Training loss: 0.08682309091091156\n",
      "Training loss: 0.08721345663070679\n",
      "Training loss: 0.08425915986299515\n",
      "Training loss: 0.0853615403175354\n",
      "Training loss: 0.0859595388174057\n",
      "Training loss: 0.08718067407608032\n",
      "Training loss: 0.08757234364748001\n",
      "Training loss: 0.08038009703159332\n",
      "Training loss: 0.07705572247505188\n",
      "Training loss: 0.07929035276174545\n",
      "Training loss: 0.07753591239452362\n",
      "Training loss: 0.08087412267923355\n",
      "Training loss: 0.0751459151506424\n",
      "Training loss: 0.09202291071414948\n",
      "Training loss: 0.0834183320403099\n",
      "Training loss: 0.08264339715242386\n",
      "Training loss: 0.07401348650455475\n",
      "Training loss: 0.08001518994569778\n",
      "Training loss: 0.07630394399166107\n",
      "Training loss: 0.08261249214410782\n",
      "Training loss: 0.08051692694425583\n",
      "Training loss: 0.0808432325720787\n",
      "Training loss: 0.08209706097841263\n",
      "Training loss: 0.0800326019525528\n",
      "Training loss: 0.07739537209272385\n",
      "Training loss: 0.08484603464603424\n",
      "Training loss: 0.0826142430305481\n",
      "Training loss: 0.07441765069961548\n",
      "Training loss: 0.08378374576568604\n",
      "Training loss: 0.07499878108501434\n",
      "Training loss: 0.07807941734790802\n",
      "Training loss: 0.08578794449567795\n",
      "Training loss: 0.07927650958299637\n",
      "Training loss: 0.0746714249253273\n",
      "Training loss: 0.08164505660533905\n",
      "Training loss: 0.09032730758190155\n",
      "Training loss: 0.0788315162062645\n",
      "Training loss: 0.0850946381688118\n",
      "Training loss: 0.0808112770318985\n",
      "Training loss: 0.0868794322013855\n",
      "Training loss: 0.07733400166034698\n",
      "Training loss: 0.08525236696004868\n",
      "Training loss: 0.0878259539604187\n",
      "Training loss: 0.08224540203809738\n",
      "Training loss: 0.08342441916465759\n",
      "Training loss: 0.0793759897351265\n",
      "Training loss: 0.08213689178228378\n",
      "Training loss: 0.08313398063182831\n",
      "Training loss: 0.08231881260871887\n",
      "Training loss: 0.08514615893363953\n",
      "Training loss: 0.08488404750823975\n",
      "Training loss: 0.0825405940413475\n",
      "Training loss: 0.08652567863464355\n",
      "Training loss: 0.08412306010723114\n",
      "Training loss: 0.07531070709228516\n",
      "Training loss: 0.0902768149971962\n",
      "Training loss: 0.07964381575584412\n",
      "Training loss: 0.0801633894443512\n",
      "Training loss: 0.07531683892011642\n",
      "Training loss: 0.0887681320309639\n",
      "Training loss: 0.09103617072105408\n",
      "Training loss: 0.08644912391901016\n",
      "Training loss: 0.08011388778686523\n",
      "Training loss: 0.08755537867546082\n",
      "Training loss: 0.07854795455932617\n",
      "Training loss: 0.08870630711317062\n",
      "Training loss: 0.08306077867746353\n",
      "Training loss: 0.07856150716543198\n",
      "Training loss: 0.08223831653594971\n",
      "Training loss: 0.08514557778835297\n",
      "Training loss: 0.07620356231927872\n",
      "Training loss: 0.07771813124418259\n",
      "Training loss: 0.08710746467113495\n",
      "Training loss: 0.0801529735326767\n",
      "Training loss: 0.08572928607463837\n",
      "Training loss: 0.08415812253952026\n",
      "Training loss: 0.08306171745061874\n",
      "Training loss: 0.07968129962682724\n",
      "Training loss: 0.08705717325210571\n",
      "Training loss: 0.08499494194984436\n",
      "Training loss: 0.07933557778596878\n",
      "Training loss: 0.08093446493148804\n",
      "Training loss: 0.0835118368268013\n",
      "Training loss: 0.0888010561466217\n",
      "Training loss: 0.08362139761447906\n",
      "Training loss: 0.09050777554512024\n",
      "Training loss: 0.08268199115991592\n",
      "Training loss: 0.08649025857448578\n",
      "Training loss: 0.08713646233081818\n",
      "Training loss: 0.0869312733411789\n",
      "Training loss: 0.08831723779439926\n",
      "Training loss: 0.07999547570943832\n",
      "Training loss: 0.07984025031328201\n",
      "Training loss: 0.07810381799936295\n",
      "Training loss: 0.08306591957807541\n",
      "Training loss: 0.07769805192947388\n",
      "Training loss: 0.07546432316303253\n",
      "Training loss: 0.07759910821914673\n",
      "Training loss: 0.07861454039812088\n",
      "Training loss: 0.08357945084571838\n",
      "Training loss: 0.07774180173873901\n",
      "Training loss: 0.07598141580820084\n",
      "Training loss: 0.07921187579631805\n",
      "Training loss: 0.08225651830434799\n",
      "Training loss: 0.07893868535757065\n",
      "Training loss: 0.08855405449867249\n",
      "Training loss: 0.08683034777641296\n",
      "Training loss: 0.07855498790740967\n",
      "Training loss: 0.07627233117818832\n",
      "Training loss: 0.08778803050518036\n",
      "Training loss: 0.08174508810043335\n",
      "Training loss: 0.08371816575527191\n",
      "Training loss: 0.0879940390586853\n",
      "Training loss: 0.08767073601484299\n",
      "Training loss: 0.08490949869155884\n",
      "Training loss: 0.07849420607089996\n",
      "Training loss: 0.08127181231975555\n",
      "Training loss: 0.07772701233625412\n",
      "Training loss: 0.08374593406915665\n",
      "Training loss: 0.09035789966583252\n",
      "Training loss: 0.08792319148778915\n",
      "Training loss: 0.08371911942958832\n",
      "Training loss: 0.08305543661117554\n",
      "Training loss: 0.08934629708528519\n",
      "Training loss: 0.09370625764131546\n",
      "Training loss: 0.08026173710823059\n",
      "Training loss: 0.09758938103914261\n",
      "Training loss: 0.0842052474617958\n",
      "Training loss: 0.08532951772212982\n",
      "Training loss: 0.08155778050422668\n",
      "Training loss: 0.08341019600629807\n",
      "Training loss: 0.08337195217609406\n",
      "Training loss: 0.09327078610658646\n",
      "Training loss: 0.09025292843580246\n",
      "Training loss: 0.0939771756529808\n",
      "Training loss: 0.08313615620136261\n",
      "Training loss: 0.083584263920784\n",
      "Training loss: 0.08788526803255081\n",
      "Training loss: 0.08817482739686966\n",
      "Training loss: 0.09174766391515732\n",
      "Training loss: 0.08420167863368988\n",
      "Training loss: 0.08479344099760056\n",
      "Training loss: 0.08303418755531311\n",
      "Training loss: 0.08309120684862137\n",
      "Training loss: 0.0809202715754509\n",
      "Training loss: 0.08592738211154938\n",
      "Training loss: 0.08610858023166656\n",
      "Training loss: 0.0891476422548294\n",
      "Training loss: 0.08688322454690933\n",
      "Training loss: 0.08779111504554749\n",
      "Training loss: 0.09432601928710938\n",
      "Training loss: 0.08313166350126266\n",
      "Training loss: 0.08539294451475143\n",
      "Training loss: 0.08598881959915161\n",
      "Training loss: 0.07805696129798889\n",
      "Training loss: 0.07661518454551697\n",
      "Training loss: 0.0887921005487442\n",
      "Training loss: 0.08520805835723877\n",
      "Training loss: 0.08449607342481613\n",
      "Training loss: 0.08058609813451767\n",
      "Training loss: 0.07955663651227951\n",
      "Training loss: 0.07998963445425034\n",
      "Training loss: 0.09879027307033539\n",
      "Training loss: 0.08162082731723785\n",
      "Training loss: 0.08584342896938324\n",
      "Training loss: 0.09365188330411911\n",
      "Training loss: 0.0870312750339508\n",
      "Training loss: 0.08404509723186493\n",
      "Training loss: 0.09671466052532196\n",
      "Training loss: 0.08983942121267319\n",
      "Training loss: 0.08156508207321167\n",
      "Training loss: 0.0898311659693718\n",
      "Training loss: 0.09563520550727844\n",
      "Training loss: 0.0929635763168335\n",
      "Training loss: 0.08933912217617035\n",
      "Training loss: 0.08967515826225281\n",
      "Training loss: 0.08306188881397247\n",
      "Training loss: 0.07716930657625198\n",
      "Training loss: 0.08715260773897171\n",
      "Training loss: 0.09169503301382065\n",
      "Training loss: 0.07649616152048111\n",
      "Training loss: 0.08192324638366699\n",
      "Training loss: 0.07975302636623383\n",
      "Training loss: 0.08056160807609558\n",
      "Training loss: 0.08637650310993195\n",
      "Training loss: 0.08217180520296097\n",
      "Training loss: 0.07854051142930984\n",
      "Training loss: 0.08164800703525543\n",
      "Training loss: 0.0791134387254715\n",
      "Training loss: 0.08334266394376755\n",
      "Training loss: 0.07773450762033463\n",
      "Training loss: 0.07453355193138123\n",
      "Training loss: 0.07948103547096252\n",
      "Training loss: 0.07879625260829926\n",
      "Training loss: 0.08828689157962799\n",
      "Training loss: 0.08776377886533737\n",
      "Training loss: 0.07685677707195282\n",
      "Training loss: 0.08237031102180481\n",
      "Training loss: 0.07476940751075745\n",
      "Training loss: 0.0840192660689354\n",
      "Training loss: 0.07870935648679733\n",
      "Training loss: 0.08281063288450241\n",
      "Training loss: 0.07359591871500015\n",
      "Training loss: 0.08397267013788223\n",
      "Training loss: 0.08535882830619812\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    log_ps = model(X_train_tensor)\n",
    "    loss = criterion(log_ps, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Training loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96Iw2q1TDh1V"
   },
   "outputs": [],
   "source": [
    "y_pred = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEo3oe7IDh1Y"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2TG6lrDDh1b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "17uSEkzwDh1f",
    "outputId": "9361fd99-77da-4a5d-ff01-45b494a06cef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9637533875338753"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_train_tensor.to('cpu')),np.array( y_pred.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Xkoy-lIWDh1i",
    "outputId": "09500235-1ad2-43d8-ef97-8c022de6d711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test_np = preprocessor.transform(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EJUsPP3okLN"
   },
   "source": [
    "### 5.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "re7L1-wqokLO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0ZqStPdokLQ"
   },
   "outputs": [],
   "source": [
    "clf_logreg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(max_iter=10000, tol=0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InIdUN-ookLU",
    "outputId": "61281c32-da81-400b-8e87-99976626a9a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['area_code'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=10000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.1, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdpsJF3TokLX",
    "outputId": "63736ec9-15a6-4f45-afa5-23131d549321",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7982214694050391 0.7019475021168501\n"
     ]
    }
   ],
   "source": [
    "print(clf_logreg.score(X_train, y_train),clf_logreg.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iruMK7GUokLZ"
   },
   "outputs": [],
   "source": [
    "y_test_log_reg = clf_logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U5hhAfgokLb"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_log_reg})\n",
    "output.to_csv('logistic/output1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKC1oQ_YokLd"
   },
   "source": [
    "The above classification resulted in 57% accuracy in the case of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TO6sujcYokLe"
   },
   "source": [
    "**Class imbalance as a problem suspected, hence going for smote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHNrYLEokLf"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NpYdlA-okLh"
   },
   "outputs": [],
   "source": [
    "# smote in pipeline\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=7)\n",
    "smote_logreg = Pipeline(steps=[('preprocessor', preprocessor),('sampling',sm),\n",
    "                      ('classifier', LogisticRegression(max_iter=10000, tol=0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ffj24y-BokLk",
    "outputId": "5a5d4387-08ff-4417-98c3-60280026777b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                ('sampling',\n",
       "                 SMOTE(k_neighbors=5, n_jobs=None, random_state=7,\n",
       "                       sampling_strategy='minority')),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=10000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.1, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ytILNEWokLm",
    "outputId": "d462fba4-c256-4add-8737-61d3bb1e7f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7641329663349566 0.6655376799322608\n"
     ]
    }
   ],
   "source": [
    "print(smote_logreg.score(X_train, y_train),smote_logreg.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPpBWmoHokLr"
   },
   "outputs": [],
   "source": [
    "y_test_logistic_smote = smote_logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eam5d4uokLt"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_logistic_smote})\n",
    "output.to_csv('logistic/output2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pn8SPEWIokLz"
   },
   "source": [
    "Smote resulted in performance decrease, and the test result was 53%, hence smote was a bad idea. let's try PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d011RcG_okL2"
   },
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh1yzq5zokL3",
    "outputId": "d31e745b-6b8f-4678-f5d7-194cd809ff43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  25 | elapsed:   18.8s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          MinMaxScaler(copy=True,\n",
       "                                                                                                       feature_range=(0,\n",
       "                                                                                                                      1)))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         ['ADSL '\n",
       "                                                                          '1',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2+',\n",
       "                                                                          'BPL',\n",
       "                                                                          'Cable',...\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=10000,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.1, verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'pca__n_components': [10, 50, 100, 200, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca_log_pipe = Pipeline(steps=[('preprocessor', preprocessor),('pca', pca),\n",
    "                      ('classifier', LogisticRegression(max_iter=10000, tol=0.1))])\n",
    "param_grid = {\n",
    "    'pca__n_components': [10, 50, 100, 200, 500]\n",
    "}\n",
    "\n",
    "pca_logistic_grid = GridSearchCV(pca_log_pipe, param_grid, n_jobs=-1,verbose=10)\n",
    "pca_logistic_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpF5dYvPokL6",
    "outputId": "5add536b-314a-42aa-8190-d703f4b2b176",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7654033453313572 0.7061812023708721\n"
     ]
    }
   ],
   "source": [
    "print(pca_logistic_grid.score(X_train, y_train),pca_logistic_grid.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45KQpW9mokL-"
   },
   "outputs": [],
   "source": [
    "y_test_logistic_pca = pca_logistic_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqQuZxxKokMB"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_logistic_pca})\n",
    "output.to_csv('logistic/output3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQmAiaN1okMD"
   },
   "source": [
    "PCA with 500 parameters resulted in a higher test accuracy of 58%, trying with further small principal components as 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZCC_DxTokME"
   },
   "outputs": [],
   "source": [
    "#with PCA 100\n",
    "logistic_pca = Pipeline(steps=[('preprocessor', preprocessor),('pca',PCA(n_components=100)),\n",
    "                      ('classifier', LogisticRegression(max_iter=10000, tol=0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2QHqRWNokMH",
    "outputId": "a1e0c651-8f4f-4213-bcf2-a55f7cfb413c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=100,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=10000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.1, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_pca.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWOXx8ZHokMJ",
    "outputId": "fc8fa8f9-62b6-4aad-fb1a-fe494c6823cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7082362904933305 0.6934801016088061\n"
     ]
    }
   ],
   "source": [
    "print(logistic_pca.score(X_train, y_train),logistic_pca.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SL8eZBvokMM"
   },
   "outputs": [],
   "source": [
    "y_test_logistic_pca2 = logistic_pca.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEOPMYbuokMO"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_logistic_pca2})\n",
    "output.to_csv('logistic/output4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYh7ocznokMP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOdSsGt0okMS"
   },
   "outputs": [],
   "source": [
    "#with PCA 800\n",
    "logistic_pca1 = Pipeline(steps=[('preprocessor', preprocessor),('pca',PCA(n_components=800)),\n",
    "                      ('classifier', LogisticRegression(max_iter=10000, tol=0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08upeozwokMV",
    "outputId": "ae88ceb8-3c46-4c32-f586-4353fe54ff5e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=800,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=10000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.1, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_pca1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljJZ_rM1okMZ",
    "outputId": "b7ad04a4-c14c-40c9-d528-f7c2bc7c03ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7903874655939022 0.6968670618120237\n"
     ]
    }
   ],
   "source": [
    "print(logistic_pca1.score(X_train, y_train),logistic_pca1.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvE0qJbjokMd"
   },
   "outputs": [],
   "source": [
    "y_test_logistic_pca3 = logistic_pca.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_logistic_pca3})\n",
    "output.to_csv('logistic/output5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eovUnGMnokMf"
   },
   "outputs": [],
   "source": [
    "train_vol_dfs = [train_df,broadband_df,outage_df,report_volume_df]\n",
    "test_vol_dfs = [test_df,broadband_df,outage_df,report_volume_df]\n",
    "train_vol_final = reduce(lambda left,right: pd.merge(left,right,on='id'), train_vol_dfs)\n",
    "test_vol_final =  reduce(lambda left,right: pd.merge(left,right,on='id'), test_vol_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_IiheRGokMk"
   },
   "outputs": [],
   "source": [
    "train_vol_final = train_vol_final.set_index('id')\n",
    "X_train_vol, X_val_vol, y_train_vol, y_val_vol = train_test_split(train_vol_final.drop('outage_duration',axis=1), train_vol_final.outage_duration, test_size=0.2,stratify=train_df.outage_duration, random_state=42)\n",
    "\n",
    "X_test_vol = test_vol_final.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxL0xgZMokMp",
    "outputId": "5b829fcd-35bc-4561-cab6-ba207c4f6f32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4723, 17), (1181, 17), (4723,), (1181,), (1477, 17))"
      ]
     },
     "execution_count": 217,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vol.shape, X_val_vol.shape, y_train_vol.shape, y_val_vol.shape, X_test_vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3elGZRAokMr",
    "outputId": "c8616171-bb38-4fee-fecf-41afeaeaf487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['area_code', 'ADSL 1', 'ADSL 2', 'ADSL 2+', 'BPL', 'Cable', 'Fiber 1',\n",
       "       'Fiber 2', 'Fiber High Speed', 'Fiber Ultra', 'Fiber Ultra Max',\n",
       "       'outage_type_1', 'outage_type_2', 'outage_type_3', 'outage_type_4',\n",
       "       'outage_type_5', 'volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 218,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vol.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTnDRIQMokMu",
    "outputId": "a71e1c75-7847-4a4b-b909-9cb73cd33d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['area_code', 'ADSL 1', 'ADSL 2', 'ADSL 2+', 'BPL', 'Cable', 'Fiber 1',\n",
       "       'Fiber 2', 'Fiber High Speed', 'Fiber Ultra', 'Fiber Ultra Max',\n",
       "       'outage_type_1', 'outage_type_2', 'outage_type_3', 'outage_type_4',\n",
       "       'outage_type_5', 'volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 219,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vol.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz1werLIokMx",
    "outputId": "0aed9707-127f-4e80-bb16-2eb6fcdda3cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['area_code'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=10000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.1, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_logreg.fit(X_train_vol,y_train_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIbMF61AokMz",
    "outputId": "2bc809b1-d258-423c-d962-872b6697acc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747194579716282 0.668077900084674\n"
     ]
    }
   ],
   "source": [
    "print(clf_logreg.score(X_train_vol,y_train_vol),clf_logreg.score(X_val_vol,y_val_vol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMxg0MEkokM1"
   },
   "outputs": [],
   "source": [
    "y_test_logistic_vol = clf_logreg.predict(X_test_vol)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_logistic_vol})\n",
    "output.to_csv('logistic/output5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UZoq61dokM5"
   },
   "source": [
    "### 5.3 KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIxg7iT6okM5"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Vr2thnokM7"
   },
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline(steps=[('preprocessor', preprocessor),('pca',PCA(500)),\n",
    "                      ('classifier', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZz_JQv9okM_",
    "outputId": "b23a4d97-83e0-48e0-952e-12db091cb532"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['area_code'])],\n",
       "                                   verbose=False)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=500,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('classifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tlutm4ORokNC",
    "outputId": "39fe3bf8-b1c9-4108-8902-1be58287dbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7899640059284353 0.6850127011007621\n"
     ]
    }
   ],
   "source": [
    "print(knn_pipe.score(X_train,y_train),knn_pipe.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdIxC7rFokNE"
   },
   "outputs": [],
   "source": [
    "y_test_knn = knn_pipe.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_knn})\n",
    "output.to_csv('knn/output1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WsdJBfKokNH"
   },
   "source": [
    "Even the above case resulted in a 58% accuracy, but knn suffers from curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBfQrld7okNH"
   },
   "source": [
    "### 5.3 SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4rpLIj6okNI"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "diMLz82RokNK"
   },
   "outputs": [],
   "source": [
    "svm_basic = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5anqyQmbokNM",
    "outputId": "fb92ec27-b998-440b-e142-730646366f60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fibe...\n",
       "                                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 sparse=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['area_code'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 250,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_basic.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3EWkcuxokNO",
    "outputId": "e7f334e0-579a-475f-917e-cee90bb6a9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7709083209824265 0.6985605419136325\n"
     ]
    }
   ],
   "source": [
    "print(svm_basic.score(X_train,y_train),svm_basic.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmxsy1U2okNQ"
   },
   "source": [
    "**Trying out grid search with different hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FF7luTU2okNQ"
   },
   "outputs": [],
   "source": [
    "param_grid = {'classifier__C': [0.1, 1, 10, 100, 1000],  \n",
    "              'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'classifier__kernel': ['poly', 'rbf', 'sigmoid','linear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dsMkEERokNV"
   },
   "outputs": [],
   "source": [
    "svm_grid = GridSearchCV(svm_basic, param_grid=param_grid, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmgSZYsMokNX",
    "outputId": "27afa820-b0be-42a9-d347-224aff3b03c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 29.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 37.3min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 43.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 51.1min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 55.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 65.2min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 73.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 87.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          MinMaxScaler(copy=True,\n",
       "                                                                                                       feature_range=(0,\n",
       "                                                                                                                      1)))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         ['ADSL '\n",
       "                                                                          '1',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2+',\n",
       "                                                                          'BPL',\n",
       "                                                                          'Cable',...\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.1, 1, 10, 100, 1000],\n",
       "                         'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'classifier__kernel': ['poly', 'rbf', 'sigmoid',\n",
       "                                                'linear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 279,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDOFRD2sokNb",
    "outputId": "c228b59e-c430-44fd-f00d-f2e150da7381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'poly'}"
      ]
     },
     "execution_count": 280,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzEbfCRNokNg",
    "outputId": "4b4b5eeb-fe90-483a-e777-44a1f0dbe032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8827016726656786 0.7002540220152413\n"
     ]
    }
   ],
   "source": [
    "print(svm_grid.score(X_train,y_train),svm_grid.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYvtXzV-okNn"
   },
   "outputs": [],
   "source": [
    "y_test_svm = svm_grid.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_svm})\n",
    "output.to_csv('svm/output1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2oAFu0BokNp"
   },
   "source": [
    "Above output resulted in a 59% percent accuracy which is improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27q0Gm4uokNp"
   },
   "source": [
    "**Using bagging classifier over SVM to see if there is any improvement in performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5X9L28_OokNq"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-eZ2Lg4okNs"
   },
   "outputs": [],
   "source": [
    "svm_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVC(kernel='poly',gamma=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD36u6tOokNu"
   },
   "outputs": [],
   "source": [
    "svm_bagged = BaggingClassifier(svm_basic, n_estimators=80, random_state=314,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MF3podgdokNw"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "Xv =pd.DataFrame(preprocessor.transform(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mg31CHADokN0"
   },
   "outputs": [],
   "source": [
    "svm=BaggingClassifier(SVC(kernel='poly',gamma=1), max_samples=0.4,max_features=0.5, n_estimators=80, n_jobs=-1,random_state=123,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXwEIpwxokN1",
    "outputId": "09643ecf-544a-44ce-9906-d26acd7f43ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   51.8s remaining:   51.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   52.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   52.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                     class_weight=None, coef0=0.0,\n",
       "                                     decision_function_shape='ovr', degree=3,\n",
       "                                     gamma=1, kernel='poly', max_iter=-1,\n",
       "                                     probability=False, random_state=None,\n",
       "                                     shrinking=True, tol=0.001, verbose=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=0.5,\n",
       "                  max_samples=0.4, n_estimators=80, n_jobs=-1, oob_score=False,\n",
       "                  random_state=123, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 342,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kClrCt9bokN6",
    "outputId": "d24dfa2d-95fe-4b76-9610-9f9c2c2bca72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  1.8min remaining:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   26.9s remaining:   26.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313148422612746 0.6900931414055885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   29.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   29.3s finished\n"
     ]
    }
   ],
   "source": [
    "print(svm.score(X,y_train),svm.score(Xv,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmLKqZTaokN8",
    "outputId": "8c9367cb-4c9d-4862-bf22-2109dd8ec646"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   36.7s remaining:   36.7s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   38.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   38.8s finished\n"
     ]
    }
   ],
   "source": [
    "Xt = pd.DataFrame(preprocessor.transform(X_test))\n",
    "y_test_svm_bag = svm.predict(Xt)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_svm_bag})\n",
    "output.to_csv('svm/output2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee6NduGUokN-"
   },
   "source": [
    "50% on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iZxh9KNokN-"
   },
   "source": [
    "### 5.4 Tree Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TetZxq8tokN-"
   },
   "source": [
    "* Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Jn2ObcPokN-"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTcWTnYmokOD"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(oob_score=True,n_jobs=-1)\n",
    "rfc_pipe = Pipeline(steps=[('pre',preprocessor),('rf',rfc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOW-9WrbokOG"
   },
   "outputs": [],
   "source": [
    "param_grid = {'rf__n_estimators': [200, 400, 1000, 1400, 2000],'rf__bootstrap': [True, False],'rf__criterion':['gini','entropy'],'rf__min_samples_leaf': [1, 2, 4,10,20],\n",
    " 'rf__min_samples_split': [2, 5, 10],'rf__max_features':['sqrt','log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kd2ZAdMUokOI"
   },
   "outputs": [],
   "source": [
    "forest_grid = GridSearchCV(rfc_pipe,param_grid,verbose=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrISmNCXokON",
    "outputId": "953c56ac-0c01-4568-a70b-c031368b34a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 36.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 45.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 48.5min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 52.3min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 56.0min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 59.5min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 66.6min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 69.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 72.4min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 75.9min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 83.7min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 91.4min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 97.8min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 101.5min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 105.5min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 112.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 115.1min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 118.1min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 120.4min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 123.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 135.7min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 147.8min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed: 158.5min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed: 166.8min\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed: 174.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 182.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed: 189.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1097 tasks      | elapsed: 194.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 202.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed: 214.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 222.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed: 228.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1344 tasks      | elapsed: 234.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1397 tasks      | elapsed: 238.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 242.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1505 tasks      | elapsed: 245.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 245.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1617 tasks      | elapsed: 246.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1674 tasks      | elapsed: 246.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1733 tasks      | elapsed: 246.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 246.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1853 tasks      | elapsed: 246.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1914 tasks      | elapsed: 246.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 246.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed: 246.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2105 tasks      | elapsed: 246.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2170 tasks      | elapsed: 246.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2237 tasks      | elapsed: 246.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2304 tasks      | elapsed: 246.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2373 tasks      | elapsed: 247.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 247.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2513 tasks      | elapsed: 247.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 247.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2657 tasks      | elapsed: 247.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2730 tasks      | elapsed: 247.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2805 tasks      | elapsed: 247.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 tasks      | elapsed: 247.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2957 tasks      | elapsed: 247.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 247.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('pre',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          MinMaxScaler(copy=True,\n",
       "                                                                                                       feature_range=(0,\n",
       "                                                                                                                      1)))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         ['ADSL '\n",
       "                                                                          '1',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2+',\n",
       "                                                                          'BPL',\n",
       "                                                                          'Cable',\n",
       "                                                                          'Fiber '\n",
       "                                                                          '1...\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'rf__bootstrap': [True, False],\n",
       "                         'rf__criterion': ['gini', 'entropy'],\n",
       "                         'rf__max_features': ['sqrt', 'log2'],\n",
       "                         'rf__min_samples_leaf': [1, 2, 4, 10, 20],\n",
       "                         'rf__min_samples_split': [2, 5, 10],\n",
       "                         'rf__n_estimators': [200, 400, 1000, 1400, 2000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRpqlRmzokOO",
    "outputId": "1b7a7661-a3b1-4de1-84e5-7a09a736f085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pre',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fiber Ultra Max...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='entropy',\n",
       "                                        max_depth=None, max_features='sqrt',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=5,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=200, n_jobs=-1,\n",
       "                                        oob_score=True, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wk0Igl1okOU",
    "outputId": "0a056809-a8cf-40b3-ead2-1edecf2de90c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9697226339191192, 0.7552921253175275)"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid.score(X_train,y_train),forest_grid.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0A5prvTjokOW"
   },
   "outputs": [],
   "source": [
    "y_test_rf = forest_grid.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_rf})\n",
    "output.to_csv('randomforest/output1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYtJnzvSokOX"
   },
   "source": [
    "The random forest resulted in an accuracy of 65%. There seems to be an overfit! Lets tune the parameters by pruning and avoid the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLcS2JdCokOY"
   },
   "source": [
    "**Tuned Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oR0mTobgokOY"
   },
   "outputs": [],
   "source": [
    "param_grid = {'rf__n_estimators': [800,1200,2000],'rf__bootstrap': [True],'rf__criterion':['entropy'],\n",
    " 'rf__max_depth': [2,4,6,8,10,15,20],'rf__max_features':['sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HSmgurRokOb"
   },
   "outputs": [],
   "source": [
    "forest_grid2 = GridSearchCV(rfc_pipe,param_grid,verbose=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_57ex0FokOf",
    "outputId": "9cad2139-6cc6-4b03-95f9-84f7c701a792",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed: 20.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('pre',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          MinMaxScaler(copy=True,\n",
       "                                                                                                       feature_range=(0,\n",
       "                                                                                                                      1)))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         ['ADSL '\n",
       "                                                                          '1',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2',\n",
       "                                                                          'ADSL '\n",
       "                                                                          '2+',\n",
       "                                                                          'BPL',\n",
       "                                                                          'Cable',\n",
       "                                                                          'Fiber '\n",
       "                                                                          '1...\n",
       "                                                               oob_score=True,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'rf__bootstrap': [True], 'rf__criterion': ['entropy'],\n",
       "                         'rf__max_depth': [2, 4, 6, 8, 10, 15, 20],\n",
       "                         'rf__max_features': ['sqrt'],\n",
       "                         'rf__n_estimators': [800, 1200, 2000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G-YxJ6WokOh",
    "outputId": "1e70e946-4c61-45e3-ea85-ede2d9c79a72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__bootstrap': True,\n",
       " 'rf__criterion': 'entropy',\n",
       " 'rf__max_depth': 20,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__n_estimators': 2000}"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxF27N_mokOm",
    "outputId": "ba72e37f-a054-4c0c-bccc-e62e87a6ff7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8384501376243912, 0.7349703640982218)"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid2.score(X_train,y_train),forest_grid2.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "on2NsDmGokOo"
   },
   "outputs": [],
   "source": [
    "y_test_rf2 = forest_grid2.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_rf2})\n",
    "output.to_csv('randomforest/output2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcyfuPs_okOq"
   },
   "source": [
    "The above model resulted in a 62%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_nzh47ookOq"
   },
   "outputs": [],
   "source": [
    "rf_final = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                                        class_weight=None, criterion='entropy',\n",
    "                                        max_depth=None, max_features='sqrt',\n",
    "                                        max_leaf_nodes=None, max_samples=None,\n",
    "                                        min_impurity_decrease=0.0,\n",
    "                                        min_impurity_split=None,\n",
    "                                        min_samples_leaf=1, min_samples_split=5,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        n_estimators=1000, n_jobs=-1,\n",
    "                                        oob_score=True, random_state=None,\n",
    "                                        verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmEUY5_xokOt"
   },
   "outputs": [],
   "source": [
    "rf_final_pipe = Pipeline(steps=[('pre',preprocessor),('rf',rf_final)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7h8t5p6okOu",
    "outputId": "ae8059cd-39aa-4d7e-d8c4-dd5b33576994"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pre',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('scaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['ADSL 1', 'ADSL 2',\n",
       "                                                   'ADSL 2+', 'BPL', 'Cable',\n",
       "                                                   'Fiber 1', 'Fiber 2',\n",
       "                                                   'Fiber High Speed',\n",
       "                                                   'Fiber Ultra',\n",
       "                                                   'Fiber Ultra Max...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='entropy',\n",
       "                                        max_depth=None, max_features='sqrt',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=5,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=1000, n_jobs=-1,\n",
       "                                        oob_score=True, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_final_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0161-zaokOw",
    "outputId": "1f8b25ff-80d0-415c-a655-c44c8af07d77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9720516620791869, 0.7552921253175275)"
      ]
     },
     "execution_count": 130,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_final_pipe.score(X_train,y_train),rf_final_pipe.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9Tc4TExokOz"
   },
   "outputs": [],
   "source": [
    "y_test_final = rf_final_pipe.predict(X_test)\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_final})\n",
    "output.to_csv('randomforest/output3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0G89tz1IokO1"
   },
   "source": [
    "Again the result was 65%, hence proceeding with boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rp8kVF7cokO1"
   },
   "source": [
    "#### 5.4.2 Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgjQJLlYokO2"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1SeeSJ7okO3"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(silent=False, \n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.2,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.5,\n",
    "                      objective='multi:softprob', \n",
    "                      n_estimators=1000, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=4, \n",
    "                      gamma=0.5,\n",
    "                     num_class = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwfAY0M6okO5"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(learning_rate=0.25,colsample_bytree = 0.5,subsample = 0.9,n_estimators=1000,objective='multi:softprob',n_jobs=-1,verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHWco_8iokO7",
    "outputId": "128d97c5-4541-4ed6-98f8-df63ee754f4c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.30906\n",
      "Will train until validation_0-merror hasn't improved in 200 rounds.\n",
      "[1]\tvalidation_0-merror:0.27180\n",
      "[2]\tvalidation_0-merror:0.27180\n",
      "[3]\tvalidation_0-merror:0.27180\n",
      "[4]\tvalidation_0-merror:0.27011\n",
      "[5]\tvalidation_0-merror:0.26926\n",
      "[6]\tvalidation_0-merror:0.26842\n",
      "[7]\tvalidation_0-merror:0.26926\n",
      "[8]\tvalidation_0-merror:0.26757\n",
      "[9]\tvalidation_0-merror:0.26672\n",
      "[10]\tvalidation_0-merror:0.26334\n",
      "[11]\tvalidation_0-merror:0.26418\n",
      "[12]\tvalidation_0-merror:0.26249\n",
      "[13]\tvalidation_0-merror:0.26418\n",
      "[14]\tvalidation_0-merror:0.25910\n",
      "[15]\tvalidation_0-merror:0.25826\n",
      "[16]\tvalidation_0-merror:0.25826\n",
      "[17]\tvalidation_0-merror:0.25656\n",
      "[18]\tvalidation_0-merror:0.25656\n",
      "[19]\tvalidation_0-merror:0.25741\n",
      "[20]\tvalidation_0-merror:0.25741\n",
      "[21]\tvalidation_0-merror:0.25572\n",
      "[22]\tvalidation_0-merror:0.25572\n",
      "[23]\tvalidation_0-merror:0.25656\n",
      "[24]\tvalidation_0-merror:0.25572\n",
      "[25]\tvalidation_0-merror:0.25402\n",
      "[26]\tvalidation_0-merror:0.25233\n",
      "[27]\tvalidation_0-merror:0.25148\n",
      "[28]\tvalidation_0-merror:0.25148\n",
      "[29]\tvalidation_0-merror:0.25233\n",
      "[30]\tvalidation_0-merror:0.25317\n",
      "[31]\tvalidation_0-merror:0.25317\n",
      "[32]\tvalidation_0-merror:0.25402\n",
      "[33]\tvalidation_0-merror:0.25402\n",
      "[34]\tvalidation_0-merror:0.25402\n",
      "[35]\tvalidation_0-merror:0.25402\n",
      "[36]\tvalidation_0-merror:0.25487\n",
      "[37]\tvalidation_0-merror:0.25487\n",
      "[38]\tvalidation_0-merror:0.25317\n",
      "[39]\tvalidation_0-merror:0.25148\n",
      "[40]\tvalidation_0-merror:0.25063\n",
      "[41]\tvalidation_0-merror:0.25063\n",
      "[42]\tvalidation_0-merror:0.25063\n",
      "[43]\tvalidation_0-merror:0.24894\n",
      "[44]\tvalidation_0-merror:0.24894\n",
      "[45]\tvalidation_0-merror:0.24979\n",
      "[46]\tvalidation_0-merror:0.24979\n",
      "[47]\tvalidation_0-merror:0.24979\n",
      "[48]\tvalidation_0-merror:0.24894\n",
      "[49]\tvalidation_0-merror:0.24725\n",
      "[50]\tvalidation_0-merror:0.24640\n",
      "[51]\tvalidation_0-merror:0.24555\n",
      "[52]\tvalidation_0-merror:0.24810\n",
      "[53]\tvalidation_0-merror:0.24810\n",
      "[54]\tvalidation_0-merror:0.24725\n",
      "[55]\tvalidation_0-merror:0.24725\n",
      "[56]\tvalidation_0-merror:0.24640\n",
      "[57]\tvalidation_0-merror:0.24555\n",
      "[58]\tvalidation_0-merror:0.24471\n",
      "[59]\tvalidation_0-merror:0.24301\n",
      "[60]\tvalidation_0-merror:0.24386\n",
      "[61]\tvalidation_0-merror:0.24386\n",
      "[62]\tvalidation_0-merror:0.24386\n",
      "[63]\tvalidation_0-merror:0.24386\n",
      "[64]\tvalidation_0-merror:0.24386\n",
      "[65]\tvalidation_0-merror:0.24386\n",
      "[66]\tvalidation_0-merror:0.24386\n",
      "[67]\tvalidation_0-merror:0.24386\n",
      "[68]\tvalidation_0-merror:0.24386\n",
      "[69]\tvalidation_0-merror:0.24471\n",
      "[70]\tvalidation_0-merror:0.24471\n",
      "[71]\tvalidation_0-merror:0.24217\n",
      "[72]\tvalidation_0-merror:0.24301\n",
      "[73]\tvalidation_0-merror:0.24301\n",
      "[74]\tvalidation_0-merror:0.24471\n",
      "[75]\tvalidation_0-merror:0.24386\n",
      "[76]\tvalidation_0-merror:0.24301\n",
      "[77]\tvalidation_0-merror:0.24217\n",
      "[78]\tvalidation_0-merror:0.24132\n",
      "[79]\tvalidation_0-merror:0.24132\n",
      "[80]\tvalidation_0-merror:0.24132\n",
      "[81]\tvalidation_0-merror:0.24132\n",
      "[82]\tvalidation_0-merror:0.24217\n",
      "[83]\tvalidation_0-merror:0.24217\n",
      "[84]\tvalidation_0-merror:0.24301\n",
      "[85]\tvalidation_0-merror:0.24471\n",
      "[86]\tvalidation_0-merror:0.24471\n",
      "[87]\tvalidation_0-merror:0.24555\n",
      "[88]\tvalidation_0-merror:0.24555\n",
      "[89]\tvalidation_0-merror:0.24471\n",
      "[90]\tvalidation_0-merror:0.24471\n",
      "[91]\tvalidation_0-merror:0.24471\n",
      "[92]\tvalidation_0-merror:0.24471\n",
      "[93]\tvalidation_0-merror:0.24471\n",
      "[94]\tvalidation_0-merror:0.24471\n",
      "[95]\tvalidation_0-merror:0.24555\n",
      "[96]\tvalidation_0-merror:0.24217\n",
      "[97]\tvalidation_0-merror:0.24301\n",
      "[98]\tvalidation_0-merror:0.24217\n",
      "[99]\tvalidation_0-merror:0.24217\n",
      "[100]\tvalidation_0-merror:0.24217\n",
      "[101]\tvalidation_0-merror:0.24047\n",
      "[102]\tvalidation_0-merror:0.24301\n",
      "[103]\tvalidation_0-merror:0.24301\n",
      "[104]\tvalidation_0-merror:0.24047\n",
      "[105]\tvalidation_0-merror:0.23963\n",
      "[106]\tvalidation_0-merror:0.23963\n",
      "[107]\tvalidation_0-merror:0.24047\n",
      "[108]\tvalidation_0-merror:0.24047\n",
      "[109]\tvalidation_0-merror:0.23963\n",
      "[110]\tvalidation_0-merror:0.23793\n",
      "[111]\tvalidation_0-merror:0.23878\n",
      "[112]\tvalidation_0-merror:0.23963\n",
      "[113]\tvalidation_0-merror:0.23963\n",
      "[114]\tvalidation_0-merror:0.23709\n",
      "[115]\tvalidation_0-merror:0.23624\n",
      "[116]\tvalidation_0-merror:0.23539\n",
      "[117]\tvalidation_0-merror:0.23709\n",
      "[118]\tvalidation_0-merror:0.23709\n",
      "[119]\tvalidation_0-merror:0.23878\n",
      "[120]\tvalidation_0-merror:0.23963\n",
      "[121]\tvalidation_0-merror:0.23878\n",
      "[122]\tvalidation_0-merror:0.23878\n",
      "[123]\tvalidation_0-merror:0.23963\n",
      "[124]\tvalidation_0-merror:0.23963\n",
      "[125]\tvalidation_0-merror:0.23963\n",
      "[126]\tvalidation_0-merror:0.24047\n",
      "[127]\tvalidation_0-merror:0.23963\n",
      "[128]\tvalidation_0-merror:0.23963\n",
      "[129]\tvalidation_0-merror:0.23963\n",
      "[130]\tvalidation_0-merror:0.24132\n",
      "[131]\tvalidation_0-merror:0.24132\n",
      "[132]\tvalidation_0-merror:0.24217\n",
      "[133]\tvalidation_0-merror:0.24132\n",
      "[134]\tvalidation_0-merror:0.24132\n",
      "[135]\tvalidation_0-merror:0.24047\n",
      "[136]\tvalidation_0-merror:0.24132\n",
      "[137]\tvalidation_0-merror:0.24217\n",
      "[138]\tvalidation_0-merror:0.24217\n",
      "[139]\tvalidation_0-merror:0.24047\n",
      "[140]\tvalidation_0-merror:0.24132\n",
      "[141]\tvalidation_0-merror:0.24132\n",
      "[142]\tvalidation_0-merror:0.24132\n",
      "[143]\tvalidation_0-merror:0.24047\n",
      "[144]\tvalidation_0-merror:0.24047\n",
      "[145]\tvalidation_0-merror:0.24047\n",
      "[146]\tvalidation_0-merror:0.24047\n",
      "[147]\tvalidation_0-merror:0.23963\n",
      "[148]\tvalidation_0-merror:0.24047\n",
      "[149]\tvalidation_0-merror:0.24217\n",
      "[150]\tvalidation_0-merror:0.23878\n",
      "[151]\tvalidation_0-merror:0.24301\n",
      "[152]\tvalidation_0-merror:0.24217\n",
      "[153]\tvalidation_0-merror:0.24301\n",
      "[154]\tvalidation_0-merror:0.24132\n",
      "[155]\tvalidation_0-merror:0.24132\n",
      "[156]\tvalidation_0-merror:0.24555\n",
      "[157]\tvalidation_0-merror:0.24640\n",
      "[158]\tvalidation_0-merror:0.24725\n",
      "[159]\tvalidation_0-merror:0.24725\n",
      "[160]\tvalidation_0-merror:0.24386\n",
      "[161]\tvalidation_0-merror:0.24471\n",
      "[162]\tvalidation_0-merror:0.24555\n",
      "[163]\tvalidation_0-merror:0.24640\n",
      "[164]\tvalidation_0-merror:0.24555\n",
      "[165]\tvalidation_0-merror:0.24471\n",
      "[166]\tvalidation_0-merror:0.24386\n",
      "[167]\tvalidation_0-merror:0.24301\n",
      "[168]\tvalidation_0-merror:0.24301\n",
      "[169]\tvalidation_0-merror:0.24301\n",
      "[170]\tvalidation_0-merror:0.24386\n",
      "[171]\tvalidation_0-merror:0.24471\n",
      "[172]\tvalidation_0-merror:0.24301\n",
      "[173]\tvalidation_0-merror:0.24132\n",
      "[174]\tvalidation_0-merror:0.23793\n",
      "[175]\tvalidation_0-merror:0.23793\n",
      "[176]\tvalidation_0-merror:0.23793\n",
      "[177]\tvalidation_0-merror:0.23624\n",
      "[178]\tvalidation_0-merror:0.23624\n",
      "[179]\tvalidation_0-merror:0.23624\n",
      "[180]\tvalidation_0-merror:0.23624\n",
      "[181]\tvalidation_0-merror:0.23709\n",
      "[182]\tvalidation_0-merror:0.23709\n",
      "[183]\tvalidation_0-merror:0.23709\n",
      "[184]\tvalidation_0-merror:0.23709\n",
      "[185]\tvalidation_0-merror:0.23878\n",
      "[186]\tvalidation_0-merror:0.23878\n",
      "[187]\tvalidation_0-merror:0.24132\n",
      "[188]\tvalidation_0-merror:0.24132\n",
      "[189]\tvalidation_0-merror:0.24132\n",
      "[190]\tvalidation_0-merror:0.24132\n",
      "[191]\tvalidation_0-merror:0.24132\n",
      "[192]\tvalidation_0-merror:0.24132\n",
      "[193]\tvalidation_0-merror:0.24217\n",
      "[194]\tvalidation_0-merror:0.24217\n",
      "[195]\tvalidation_0-merror:0.24301\n",
      "[196]\tvalidation_0-merror:0.24301\n",
      "[197]\tvalidation_0-merror:0.24301\n",
      "[198]\tvalidation_0-merror:0.24301\n",
      "[199]\tvalidation_0-merror:0.24301\n",
      "[200]\tvalidation_0-merror:0.24301\n",
      "[201]\tvalidation_0-merror:0.24386\n",
      "[202]\tvalidation_0-merror:0.24301\n",
      "[203]\tvalidation_0-merror:0.24217\n",
      "[204]\tvalidation_0-merror:0.24301\n",
      "[205]\tvalidation_0-merror:0.24217\n",
      "[206]\tvalidation_0-merror:0.24132\n",
      "[207]\tvalidation_0-merror:0.24047\n",
      "[208]\tvalidation_0-merror:0.24132\n",
      "[209]\tvalidation_0-merror:0.24301\n",
      "[210]\tvalidation_0-merror:0.24386\n",
      "[211]\tvalidation_0-merror:0.24301\n",
      "[212]\tvalidation_0-merror:0.24386\n",
      "[213]\tvalidation_0-merror:0.24386\n",
      "[214]\tvalidation_0-merror:0.24386\n",
      "[215]\tvalidation_0-merror:0.24471\n",
      "[216]\tvalidation_0-merror:0.24555\n",
      "[217]\tvalidation_0-merror:0.24471\n",
      "[218]\tvalidation_0-merror:0.24640\n",
      "[219]\tvalidation_0-merror:0.24640\n",
      "[220]\tvalidation_0-merror:0.24471\n",
      "[221]\tvalidation_0-merror:0.24555\n",
      "[222]\tvalidation_0-merror:0.24471\n",
      "[223]\tvalidation_0-merror:0.24471\n",
      "[224]\tvalidation_0-merror:0.24471\n",
      "[225]\tvalidation_0-merror:0.24386\n",
      "[226]\tvalidation_0-merror:0.24301\n",
      "[227]\tvalidation_0-merror:0.24301\n",
      "[228]\tvalidation_0-merror:0.24301\n",
      "[229]\tvalidation_0-merror:0.24217\n",
      "[230]\tvalidation_0-merror:0.24386\n",
      "[231]\tvalidation_0-merror:0.24386\n",
      "[232]\tvalidation_0-merror:0.24217\n",
      "[233]\tvalidation_0-merror:0.24132\n",
      "[234]\tvalidation_0-merror:0.24386\n",
      "[235]\tvalidation_0-merror:0.24386\n",
      "[236]\tvalidation_0-merror:0.24386\n",
      "[237]\tvalidation_0-merror:0.24386\n",
      "[238]\tvalidation_0-merror:0.24471\n",
      "[239]\tvalidation_0-merror:0.24217\n",
      "[240]\tvalidation_0-merror:0.24217\n",
      "[241]\tvalidation_0-merror:0.24217\n",
      "[242]\tvalidation_0-merror:0.24217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243]\tvalidation_0-merror:0.24217\n",
      "[244]\tvalidation_0-merror:0.24217\n",
      "[245]\tvalidation_0-merror:0.24217\n",
      "[246]\tvalidation_0-merror:0.24047\n",
      "[247]\tvalidation_0-merror:0.24132\n",
      "[248]\tvalidation_0-merror:0.24132\n",
      "[249]\tvalidation_0-merror:0.24132\n",
      "[250]\tvalidation_0-merror:0.24047\n",
      "[251]\tvalidation_0-merror:0.24047\n",
      "[252]\tvalidation_0-merror:0.24047\n",
      "[253]\tvalidation_0-merror:0.24047\n",
      "[254]\tvalidation_0-merror:0.23878\n",
      "[255]\tvalidation_0-merror:0.24132\n",
      "[256]\tvalidation_0-merror:0.24132\n",
      "[257]\tvalidation_0-merror:0.24132\n",
      "[258]\tvalidation_0-merror:0.24047\n",
      "[259]\tvalidation_0-merror:0.24132\n",
      "[260]\tvalidation_0-merror:0.24132\n",
      "[261]\tvalidation_0-merror:0.24132\n",
      "[262]\tvalidation_0-merror:0.24217\n",
      "[263]\tvalidation_0-merror:0.24132\n",
      "[264]\tvalidation_0-merror:0.24047\n",
      "[265]\tvalidation_0-merror:0.23963\n",
      "[266]\tvalidation_0-merror:0.23963\n",
      "[267]\tvalidation_0-merror:0.23963\n",
      "[268]\tvalidation_0-merror:0.23878\n",
      "[269]\tvalidation_0-merror:0.23793\n",
      "[270]\tvalidation_0-merror:0.23793\n",
      "[271]\tvalidation_0-merror:0.23878\n",
      "[272]\tvalidation_0-merror:0.23878\n",
      "[273]\tvalidation_0-merror:0.23878\n",
      "[274]\tvalidation_0-merror:0.23793\n",
      "[275]\tvalidation_0-merror:0.23709\n",
      "[276]\tvalidation_0-merror:0.23709\n",
      "[277]\tvalidation_0-merror:0.23793\n",
      "[278]\tvalidation_0-merror:0.23709\n",
      "[279]\tvalidation_0-merror:0.23709\n",
      "[280]\tvalidation_0-merror:0.23624\n",
      "[281]\tvalidation_0-merror:0.23624\n",
      "[282]\tvalidation_0-merror:0.23709\n",
      "[283]\tvalidation_0-merror:0.23709\n",
      "[284]\tvalidation_0-merror:0.23793\n",
      "[285]\tvalidation_0-merror:0.23624\n",
      "[286]\tvalidation_0-merror:0.23709\n",
      "[287]\tvalidation_0-merror:0.23793\n",
      "[288]\tvalidation_0-merror:0.23793\n",
      "[289]\tvalidation_0-merror:0.23793\n",
      "[290]\tvalidation_0-merror:0.23793\n",
      "[291]\tvalidation_0-merror:0.23709\n",
      "[292]\tvalidation_0-merror:0.23624\n",
      "[293]\tvalidation_0-merror:0.23624\n",
      "[294]\tvalidation_0-merror:0.23709\n",
      "[295]\tvalidation_0-merror:0.23624\n",
      "[296]\tvalidation_0-merror:0.23709\n",
      "[297]\tvalidation_0-merror:0.23709\n",
      "[298]\tvalidation_0-merror:0.23878\n",
      "[299]\tvalidation_0-merror:0.23709\n",
      "[300]\tvalidation_0-merror:0.23455\n",
      "[301]\tvalidation_0-merror:0.23455\n",
      "[302]\tvalidation_0-merror:0.23793\n",
      "[303]\tvalidation_0-merror:0.23963\n",
      "[304]\tvalidation_0-merror:0.23963\n",
      "[305]\tvalidation_0-merror:0.24047\n",
      "[306]\tvalidation_0-merror:0.23963\n",
      "[307]\tvalidation_0-merror:0.24047\n",
      "[308]\tvalidation_0-merror:0.23878\n",
      "[309]\tvalidation_0-merror:0.23878\n",
      "[310]\tvalidation_0-merror:0.23793\n",
      "[311]\tvalidation_0-merror:0.23878\n",
      "[312]\tvalidation_0-merror:0.23878\n",
      "[313]\tvalidation_0-merror:0.23878\n",
      "[314]\tvalidation_0-merror:0.23878\n",
      "[315]\tvalidation_0-merror:0.23878\n",
      "[316]\tvalidation_0-merror:0.23963\n",
      "[317]\tvalidation_0-merror:0.24132\n",
      "[318]\tvalidation_0-merror:0.23963\n",
      "[319]\tvalidation_0-merror:0.23963\n",
      "[320]\tvalidation_0-merror:0.23963\n",
      "[321]\tvalidation_0-merror:0.23963\n",
      "[322]\tvalidation_0-merror:0.24047\n",
      "[323]\tvalidation_0-merror:0.23963\n",
      "[324]\tvalidation_0-merror:0.23793\n",
      "[325]\tvalidation_0-merror:0.23793\n",
      "[326]\tvalidation_0-merror:0.23793\n",
      "[327]\tvalidation_0-merror:0.23709\n",
      "[328]\tvalidation_0-merror:0.23709\n",
      "[329]\tvalidation_0-merror:0.23793\n",
      "[330]\tvalidation_0-merror:0.23793\n",
      "[331]\tvalidation_0-merror:0.23878\n",
      "[332]\tvalidation_0-merror:0.23963\n",
      "[333]\tvalidation_0-merror:0.23793\n",
      "[334]\tvalidation_0-merror:0.23793\n",
      "[335]\tvalidation_0-merror:0.23709\n",
      "[336]\tvalidation_0-merror:0.23709\n",
      "[337]\tvalidation_0-merror:0.23624\n",
      "[338]\tvalidation_0-merror:0.23878\n",
      "[339]\tvalidation_0-merror:0.23793\n",
      "[340]\tvalidation_0-merror:0.23709\n",
      "[341]\tvalidation_0-merror:0.23709\n",
      "[342]\tvalidation_0-merror:0.23624\n",
      "[343]\tvalidation_0-merror:0.23624\n",
      "[344]\tvalidation_0-merror:0.23455\n",
      "[345]\tvalidation_0-merror:0.23539\n",
      "[346]\tvalidation_0-merror:0.23539\n",
      "[347]\tvalidation_0-merror:0.23539\n",
      "[348]\tvalidation_0-merror:0.23539\n",
      "[349]\tvalidation_0-merror:0.23539\n",
      "[350]\tvalidation_0-merror:0.23709\n",
      "[351]\tvalidation_0-merror:0.23709\n",
      "[352]\tvalidation_0-merror:0.23793\n",
      "[353]\tvalidation_0-merror:0.23709\n",
      "[354]\tvalidation_0-merror:0.23539\n",
      "[355]\tvalidation_0-merror:0.23539\n",
      "[356]\tvalidation_0-merror:0.23624\n",
      "[357]\tvalidation_0-merror:0.23963\n",
      "[358]\tvalidation_0-merror:0.23878\n",
      "[359]\tvalidation_0-merror:0.23878\n",
      "[360]\tvalidation_0-merror:0.23793\n",
      "[361]\tvalidation_0-merror:0.23793\n",
      "[362]\tvalidation_0-merror:0.23793\n",
      "[363]\tvalidation_0-merror:0.23624\n",
      "[364]\tvalidation_0-merror:0.23624\n",
      "[365]\tvalidation_0-merror:0.23709\n",
      "[366]\tvalidation_0-merror:0.23793\n",
      "[367]\tvalidation_0-merror:0.23878\n",
      "[368]\tvalidation_0-merror:0.23878\n",
      "[369]\tvalidation_0-merror:0.23878\n",
      "[370]\tvalidation_0-merror:0.23878\n",
      "[371]\tvalidation_0-merror:0.23793\n",
      "[372]\tvalidation_0-merror:0.23793\n",
      "[373]\tvalidation_0-merror:0.23963\n",
      "[374]\tvalidation_0-merror:0.23793\n",
      "[375]\tvalidation_0-merror:0.23709\n",
      "[376]\tvalidation_0-merror:0.23878\n",
      "[377]\tvalidation_0-merror:0.24047\n",
      "[378]\tvalidation_0-merror:0.24047\n",
      "[379]\tvalidation_0-merror:0.24132\n",
      "[380]\tvalidation_0-merror:0.24132\n",
      "[381]\tvalidation_0-merror:0.24047\n",
      "[382]\tvalidation_0-merror:0.23963\n",
      "[383]\tvalidation_0-merror:0.23963\n",
      "[384]\tvalidation_0-merror:0.23878\n",
      "[385]\tvalidation_0-merror:0.23963\n",
      "[386]\tvalidation_0-merror:0.23963\n",
      "[387]\tvalidation_0-merror:0.23878\n",
      "[388]\tvalidation_0-merror:0.23963\n",
      "[389]\tvalidation_0-merror:0.24132\n",
      "[390]\tvalidation_0-merror:0.24047\n",
      "[391]\tvalidation_0-merror:0.24047\n",
      "[392]\tvalidation_0-merror:0.23963\n",
      "[393]\tvalidation_0-merror:0.24047\n",
      "[394]\tvalidation_0-merror:0.24047\n",
      "[395]\tvalidation_0-merror:0.24047\n",
      "[396]\tvalidation_0-merror:0.24047\n",
      "[397]\tvalidation_0-merror:0.24132\n",
      "[398]\tvalidation_0-merror:0.24132\n",
      "[399]\tvalidation_0-merror:0.24132\n",
      "[400]\tvalidation_0-merror:0.24132\n",
      "[401]\tvalidation_0-merror:0.23963\n",
      "[402]\tvalidation_0-merror:0.23963\n",
      "[403]\tvalidation_0-merror:0.23963\n",
      "[404]\tvalidation_0-merror:0.23963\n",
      "[405]\tvalidation_0-merror:0.23963\n",
      "[406]\tvalidation_0-merror:0.23963\n",
      "[407]\tvalidation_0-merror:0.24047\n",
      "[408]\tvalidation_0-merror:0.24132\n",
      "[409]\tvalidation_0-merror:0.24217\n",
      "[410]\tvalidation_0-merror:0.24047\n",
      "[411]\tvalidation_0-merror:0.24047\n",
      "[412]\tvalidation_0-merror:0.24132\n",
      "[413]\tvalidation_0-merror:0.24132\n",
      "[414]\tvalidation_0-merror:0.24132\n",
      "[415]\tvalidation_0-merror:0.24132\n",
      "[416]\tvalidation_0-merror:0.24132\n",
      "[417]\tvalidation_0-merror:0.24132\n",
      "[418]\tvalidation_0-merror:0.24047\n",
      "[419]\tvalidation_0-merror:0.24047\n",
      "[420]\tvalidation_0-merror:0.24132\n",
      "[421]\tvalidation_0-merror:0.24132\n",
      "[422]\tvalidation_0-merror:0.24047\n",
      "[423]\tvalidation_0-merror:0.24132\n",
      "[424]\tvalidation_0-merror:0.24217\n",
      "[425]\tvalidation_0-merror:0.24132\n",
      "[426]\tvalidation_0-merror:0.24047\n",
      "[427]\tvalidation_0-merror:0.24132\n",
      "[428]\tvalidation_0-merror:0.23963\n",
      "[429]\tvalidation_0-merror:0.24047\n",
      "[430]\tvalidation_0-merror:0.24047\n",
      "[431]\tvalidation_0-merror:0.24047\n",
      "[432]\tvalidation_0-merror:0.24132\n",
      "[433]\tvalidation_0-merror:0.24217\n",
      "[434]\tvalidation_0-merror:0.24217\n",
      "[435]\tvalidation_0-merror:0.24217\n",
      "[436]\tvalidation_0-merror:0.24301\n",
      "[437]\tvalidation_0-merror:0.24386\n",
      "[438]\tvalidation_0-merror:0.24386\n",
      "[439]\tvalidation_0-merror:0.24386\n",
      "[440]\tvalidation_0-merror:0.24471\n",
      "[441]\tvalidation_0-merror:0.24471\n",
      "[442]\tvalidation_0-merror:0.24471\n",
      "[443]\tvalidation_0-merror:0.24386\n",
      "[444]\tvalidation_0-merror:0.24217\n",
      "[445]\tvalidation_0-merror:0.24217\n",
      "[446]\tvalidation_0-merror:0.24132\n",
      "[447]\tvalidation_0-merror:0.24217\n",
      "[448]\tvalidation_0-merror:0.24132\n",
      "[449]\tvalidation_0-merror:0.24132\n",
      "[450]\tvalidation_0-merror:0.24047\n",
      "[451]\tvalidation_0-merror:0.24047\n",
      "[452]\tvalidation_0-merror:0.23878\n",
      "[453]\tvalidation_0-merror:0.23963\n",
      "[454]\tvalidation_0-merror:0.23878\n",
      "[455]\tvalidation_0-merror:0.23963\n",
      "[456]\tvalidation_0-merror:0.24132\n",
      "[457]\tvalidation_0-merror:0.24301\n",
      "[458]\tvalidation_0-merror:0.24301\n",
      "[459]\tvalidation_0-merror:0.24301\n",
      "[460]\tvalidation_0-merror:0.24386\n",
      "[461]\tvalidation_0-merror:0.24386\n",
      "[462]\tvalidation_0-merror:0.24471\n",
      "[463]\tvalidation_0-merror:0.24555\n",
      "[464]\tvalidation_0-merror:0.24471\n",
      "[465]\tvalidation_0-merror:0.24471\n",
      "[466]\tvalidation_0-merror:0.24471\n",
      "[467]\tvalidation_0-merror:0.24386\n",
      "[468]\tvalidation_0-merror:0.24386\n",
      "[469]\tvalidation_0-merror:0.24301\n",
      "[470]\tvalidation_0-merror:0.24386\n",
      "[471]\tvalidation_0-merror:0.24301\n",
      "[472]\tvalidation_0-merror:0.24047\n",
      "[473]\tvalidation_0-merror:0.24047\n",
      "[474]\tvalidation_0-merror:0.24047\n",
      "[475]\tvalidation_0-merror:0.24047\n",
      "[476]\tvalidation_0-merror:0.24047\n",
      "[477]\tvalidation_0-merror:0.23963\n",
      "[478]\tvalidation_0-merror:0.23963\n",
      "[479]\tvalidation_0-merror:0.24047\n",
      "[480]\tvalidation_0-merror:0.24047\n",
      "[481]\tvalidation_0-merror:0.24047\n",
      "[482]\tvalidation_0-merror:0.24217\n",
      "[483]\tvalidation_0-merror:0.24217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484]\tvalidation_0-merror:0.24301\n",
      "[485]\tvalidation_0-merror:0.24217\n",
      "[486]\tvalidation_0-merror:0.24132\n",
      "[487]\tvalidation_0-merror:0.24217\n",
      "[488]\tvalidation_0-merror:0.24301\n",
      "[489]\tvalidation_0-merror:0.24132\n",
      "[490]\tvalidation_0-merror:0.24301\n",
      "[491]\tvalidation_0-merror:0.24386\n",
      "[492]\tvalidation_0-merror:0.24301\n",
      "[493]\tvalidation_0-merror:0.24301\n",
      "[494]\tvalidation_0-merror:0.24301\n",
      "[495]\tvalidation_0-merror:0.24386\n",
      "[496]\tvalidation_0-merror:0.24386\n",
      "[497]\tvalidation_0-merror:0.24471\n",
      "[498]\tvalidation_0-merror:0.24386\n",
      "[499]\tvalidation_0-merror:0.24555\n",
      "[500]\tvalidation_0-merror:0.24725\n",
      "Stopping. Best iteration:\n",
      "[300]\tvalidation_0-merror:0.23455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_set = [(preprocessor.transform(X_val), y_val)]\n",
    "model = model.fit((preprocessor.transform(X_train)),y_train,eval_set=eval_set,early_stopping_rounds=200,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbswrBxWokO-",
    "outputId": "b2d89975-24b7-4146-9980-03c2c72e4dfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9057802244336227, 0.7654530059271804)"
      ]
     },
     "execution_count": 399,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(preprocessor.transform(X_train),y_train),model.score(preprocessor.transform(X_val),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kowvG5_7okPA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_xgb = model.predict(preprocessor.transform(X_test))\n",
    "output = pd.DataFrame({'id':test_df.id,'outage_duration':y_test_xgb})\n",
    "output.to_csv('xgb/output1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pC3PdwEGokPE"
   },
   "source": [
    "The above model gave an accuracy of 65% on test, hence proceeding with the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iL4Qq2_2okPE"
   },
   "source": [
    "## 5. Summary\n",
    "\n",
    "* We described the problem statement\n",
    "* Imported necessary libraries to perform EDA\n",
    "* Read all the datasets in the raw format\n",
    "* Data Wrangling to get the necessary format for EDA and model building\n",
    "* Explored each dataset with description and EDA\n",
    "* Answers to the questions by the operations team\n",
    "* Recommendations to improve the detection of outage durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYMCmZGkokPF"
   },
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "* Splitting the train dataset into train and validation\n",
    "* Trying models in the order of the increasing complexity\n",
    "* Analyzing the performance(bias/variance) of different models\n",
    "* Tweaking the models and analyzing the performance\n",
    "* Choosing the best model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PHD_Syed_Shariff.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
